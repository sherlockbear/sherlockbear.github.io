<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    
    
    
    <title>Reinforcement Learning Reading List（持续更新） | Hamish的科研blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content>
    <meta name="description" content="*标注的为值得精读论文 ICLR2020Oral Contrastive Learning of Structured World Models. Kipf, T., van der Pol, E., &amp;amp; Welling, M. (2019). [原文链接]*  从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning Reading List（持续更新）">
<meta property="og:url" content="http://yoursite.com/2020/03/06/Reinforcement-Learning-Reading-List/index.html">
<meta property="og:site_name" content="Hamish的科研blog">
<meta property="og:description" content="*标注的为值得精读论文 ICLR2020Oral Contrastive Learning of Structured World Models. Kipf, T., van der Pol, E., &amp;amp; Welling, M. (2019). [原文链接]*  从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-03-06T06:57:59.056Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning Reading List（持续更新）">
<meta name="twitter:description" content="*标注的为值得精读论文 ICLR2020Oral Contrastive Learning of Structured World Models. Kipf, T., van der Pol, E., &amp;amp; Welling, M. (2019). [原文链接]*  从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一">
    
        <link rel="alternate" type="application/atom+xml" title="Hamish的科研blog" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Kang Yachen</h5>
          <a href="mailto:kangyachen@westlake.edu.cn" title="kangyachen@westlake.edu.cn" class="mail">kangyachen@westlake.edu.cn</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/sherlockbear" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Reinforcement Learning Reading List（持续更新）</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Reinforcement Learning Reading List（持续更新）</h1>
        <h5 class="subtitle">
            
                <time datetime="2020-03-06T01:44:37.000Z" itemprop="datePublished" class="page-time">
  2020-03-06
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#ICLR2020"><span class="post-toc-number">1.</span> <span class="post-toc-text">ICLR2020</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Oral"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">Oral</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-Reinforcement-Learning-Reading-List"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Reinforcement Learning Reading List（持续更新）</h1>
        <div class="post-meta">
            <time class="post-time" title="2020-03-06 09:44:37" datetime="2020-03-06T01:44:37.000Z"  itemprop="datePublished">2020-03-06</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>*标注的为值得精读论文</p>
<h2 id="ICLR2020"><a href="#ICLR2020" class="headerlink" title="ICLR2020"></a>ICLR2020</h2><h3 id="Oral"><a href="#Oral" class="headerlink" title="Oral"></a>Oral</h3><ul>
<li><p><strong>Contrastive Learning of Structured World Models.</strong> Kipf, T., van der Pol, E., &amp; Welling, M. (2019). [<a href="http://arxiv.org/abs/1911.12247" target="_blank" rel="noopener">原文链接</a>]*</p>
<ul>
<li>从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一个挑战。为了朝这个目标迈进，我们引入了Contrastively-trained Structured World Models（C-SWMs）。C-SWMs利用对比方法在具有合成结构的环境中进行表示学习。我们通过图神经网络建模将每个状态嵌入构造为一组对象表示及其关系。这允许模型从原始像素观察中发现对象，而无需把直接监督作为学习过程的一部分。我们在包含多个交互对象的合成环境中评估C-SWMs，这些交互对象可以由智能体独立操作，包括简单的Atari游戏和多对象物理模拟器。我们的实验表明，C-SWMs可以在学习到可解释的基于对象的表示形式的基础上，克服基于像素重构的模型的局限性，并在高度结构化的环境中胜过此类模型的典型代表。</li>
</ul>
</li>
<li><p><strong>IMPROVING GENERALIZATION IN META REINFORCE-MENT LEARNING USING LEARNED OBJECTIVES.</strong> Kirsch, L., Van Steenkiste, S., &amp; Urgen Schmidhuber, J. ¨. (2019). [<a href="http://louiskirsch.com/code/metagenrl" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>生物进化将许多学习者的经验提炼为人类的通用学习算法。我们新颖的元强化学习算法<em>MetaGenRL</em>受此过程启发。<em>MetaGenRL</em>提取了许多复杂智能体的经验，元学习一种低复杂度的神经目标函数，该函数决定了个体在未来将如何学习。与最近的元强化算法不同，<em>MetaGenRL</em>可以推广到与元训练阶段完全不同的新环境。在某些情况下，它甚至优于人工设计的RL算法。<em>MetaGenRL</em>在元训练期间使用off-policy二阶梯度，可大大提高其样本效率。</li>
</ul>
</li>
<li><p><strong>Graph Convolutional Reinforcement Learning.</strong> Jiang, J., Dun, C., Huang, T., &amp; Lu, Z. (2018). [<a href="http://arxiv.org/abs/1810.09202" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>在多智能体环境中，学习如何合作至关重要。关键是要了解智能体之间的相互影响。但是，多智能体环境是高度动态的，智能体不断移动，其邻居不断改变。这使得学习智能体之间相互作用的抽象表示变得困难。为了解决这些困难，我们提出了图卷积增强学习，其中图卷积适应于多智能体环境潜在图的动态，并且关系内核通过它们的关系表示来捕获智能体之间的相互作用。利用卷积层从逐渐增加的感受野产生的潜在特征来学习协作，并且通过时间关系正则化来进一步改善协作以保持一致性。从经验上讲，我们证明了我们的方法在各种协作方案中都大大优于现有方法。</li>
</ul>
</li>
<li><p><strong>IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO.</strong> Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., &amp; Madry, A. (2019). [<a href="https://github.com/openai/baselines" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>我们通过对两个流行算法（近端策略优化PPO和信任区域策略优化TRPO）的案例，研究了深度策略梯度算法中算法进步的根源。我们调研了“代码级优化”（仅在实现中发现或被描述为核心算法的辅助细节的算法增强）的结果。看起来是次要的，然而这种优化对智能体行为有重大影响。我们的结果表明，这些优化（a）影响了PPO优于TRPO的累积奖励中的大部分收益，以及（b）从根本上改变RL方法的功能。这些观察表明了在强化学习中对效果提升的归因是困难和重要的。</li>
</ul>
</li>
<li><p><strong>A CLOSER LOOK AT DEEP POLICY GRADIENTS.</strong> Ilyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., &amp; Madry, A. (2019). [<a href="https://openreview.net/forum?id=ryxdEkHtPS" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>我们研究了深度策略梯度算法的行为如何反映激励其发展的概念框架。 为此，我们基于该框架的关键要素（梯度估计，值预测和优化环境）提出了一种最新技术的细粒度分析。 我们的结果表明，深层策略梯度算法的行为通常会偏离其激励框架的预测：替代目标与真实奖励格局不匹配，学习的价值估算器无法满足真实价值函数，并且梯度估算之间的相关性很差 具有“真实”的渐变。 我们发现的预测行为和经验行为之间的不匹配，凸显了我们对当前方法的理解不足，并表明需要超越当前以基准为中心的评估方法。</li>
</ul>
</li>
<li><p><strong>Meta-Q-Learning.</strong> Fakoor, R., Chaudhari, P., Soatto, S., &amp; Smola, A. J. (2019). [<a href="http://arxiv.org/abs/1910.00125" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>本文介绍了Meta-Q-Learning（MQL），这是一种用于元强化学习（meta-RL）的新的非策略算法。  MQL基于三个简单的想法。 首先，我们表明，如果可以访问表示过去轨迹的上下文变量，Q学习将与最新的元RL算法竞争。 其次，使用多任务目标来最大化训练任务的平均奖励是对RL策略进行元训练的有效方法。 第三，可以使用元策略重播缓冲区中的过去数据进行回收，以适应新任务的策略。  MQL借鉴了倾向性评估中的想法，从而扩大了可用于适应的可用数据量。 在标准连续控制基准上进行的实验表明，MQL与最新的meta-RL算法相比具有优势。</li>
</ul>
</li>
<li><p><strong>POSTERIOR SAMPLING FOR MULTI-AGENT REINFORCE-MENT LEARNING: SOLVING EXTENSIVE GAMES WITH IMPERFECT INFORMATION.</strong> Zhou, Y., Li, J., &amp; Zhu, J. (2019). [<a href="https://openreview.net/forum?id=Syg-ET4FPS" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>强化学习的后验采样（PSRL）是在未知环境中进行决策的有用框架。  PSRL维护环境的后验分布，然后对从后验分布采样的环境进行规划。 尽管PSRL在单人强化学习问题上表现良好，但如何将PSRL应用于多人强化学习问题仍未探索。 在这项工作中，我们将PSRL扩展到具有不完善信息（TEGI）的两人零和博弈，这是一类多代理系统。 从技术上讲，我们将PSRL与反事实后悔最小化（CFR）相结合，这是已知环境下TEGI的领先算法。 我们的主要贡献是互动策略的新颖设计。 通过我们的交互策略，我们的算法可证明以O（）的速度收敛至Nash均衡。</li>
</ul>
</li>
<li><p><strong>A Generalized Training Approach for Multiagent Learning.</strong> Muller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat, J., Liu, S., … Munos, R. (2019). [<a href="http://arxiv.org/abs/1909.12823" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>本文研究了一种基于博弈论的，基于博弈论原理的培训制度，该制度被称为“策略间隔响应Oracle”（PSRO）。  PSRO具有一般意义，因为它（1）包含了众所周知的算法，例如虚拟游戏和特殊情况下的双重预言，并且（2）原则上适用于一般和多玩家游戏。 尽管如此，以前对PSRO的研究都集中在两人零和游戏上，这种系统中纳什均衡是可计算的。 从两人零和游戏转移到更一般的设置时，纳什均衡的计算很快变得不可行。 在这里，我们通过考虑替代解决方案概念α-Rank来扩展PSRO的理论基础，α-Rank是唯一的（因此与Nash不同，因此不存在均衡选择问题），并且易于应用于一般和，多玩家设置。 我们在几个游戏类中建立收敛性保证，并确定纳什均衡与α-Rank之间的联系。 我们证明了基于α-Rank的PSRO与2人游戏Kuhn和Leduc Poker中基于Nash求解器的PSRO的竞争性能。 然后，通过考虑3至5人扑克游戏，我们超越了先前的PSRO应用程序，产生了α-Rank比近似Nash解算器更快的收敛速度的实例，因此将其确立为有利的一般游戏解算器。 我们还对MuJoCo足球进行了初步的经验验证，说明了该方法在另一个复杂领域中的可行性。</li>
</ul>
</li>
<li><p><strong>Harnessing Structures for Value-Based Planning and Reinforcement Learning.</strong> Yang, Y., Zhang, G., Xu, Z., &amp; Katabi, D. (2019). [<a href="http://arxiv.org/abs/1909.12255" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>基于价值的方法构成了计划和深度强化学习（RL）的基本方法。 在本文中，我们建议为计划和深度RL开发状态作用值函数（即Q函数）的基础结构。 特别是，如果潜在的系统动力学导致Q函数的某些全局结构，则应该能够通过利用这种结构更好地推断该函数。 具体来说，我们研究了低秩结构，它在大数据矩阵中广泛存在。 我们在控制和深度RL任务的上下文中凭经验验证了低秩Q函数的存在。 作为我们的主要贡献，通过利用矩阵估计（ME）技术，我们提出了一个通用框架来利用Q函数中的底层低秩结构。 这导致经典控制的计划程序效率更高，此外，可以将简单方案应用于基于值的RL技术，以在“低级”任务上始终获得更好的性能。 关于控制任务和Atari游戏的大量实验证实了我们方法的有效性。</li>
</ul>
</li>
<li><p><strong>Fast Task Inference with Variational Intrinsic Successor Features.</strong> Hansen DeepMind, S., Dabney DeepMind, W., Barreto DeepMind, A., Warde-Farley DeepMind, D., Van de Wiele, T., &amp; Mnih DeepMind, V. (2019). [<a href="https://openreview.net/forum?id=BJeAHkrYDS" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>已经确定，可以通过奖励与其他政策有区别的政策来训练跨越马尔可夫决策过程可控子空间的各种行为（Gregor等人，2016; Eysenbach等人，2018; Warde-Farley等人。  ，2018）。 但是，这种表述的一个局限性是难以推广到超出明确学习的有限行为集的范围，这在后续任务中可能是必需的。 后继特征（Dayan，1993; Barreto et al。，2017）提供了一个吸引人的解决方案，适用于此泛化问题，但需要在某些基础特征空间中将奖励函数定义为线性。 在本文中，我们展示了可以将这两种技术结合使用，并且每种方法都可以解决另一个主要限制。 为此，我们引入了变种内在后继特征（VISR），这是一种新颖的算法，可学习可控特征，可通过后继特征框架利用可控特征来提供增强的泛化能力和快速的任务推断。 我们以新颖的设置对整个Atari套件上的VISR进行了经验验证，其中的奖励仅是在漫长的无人监督阶段之后才短暂暴露出来的。 我们相信，在12场比赛中达到人类水平的表现并超过所有基准时，我们认为VISR代表了朝着能够从有限的反馈中快速学习的代理商迈出的一步。</li>
</ul>
</li>
<li><p><strong>Observational Overfitting in Reinforcement Learning.</strong> Song, X., Jiang, Y., Tu, S., Du, Y., &amp; Neyshabur, B. (2019). [<a href="http://arxiv.org/abs/1912.02975" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>在无模型强化学习（RL）中过度拟合的一个主要组成部分涉及以下情况：代理人可能会根据马尔可夫决策过程（MDP）产生的观察结果错误地将奖励与某些虚假特征相关联。 我们提供了一个用于分析这种情况的通用框架，该框架用于仅通过修改MDP的观察空间来设计多个综合基准。 即使基本MDP动态是固定的，当特工过度拟合到不同的观察空间时，我们也称这种观察过度拟合。 我们的实验揭示了有趣的属性，尤其是在隐式正则化方面，还证实了RL泛化和监督学习（SL）先前工作的结果。</li>
</ul>
</li>
<li><p><strong>Dynamics-Aware Unsupervised Discovery of Skills.</strong> Sharma, A., Gu, S., Levine, S., Kumar, V., &amp; Hausman, K. (2019). [<a href="http://arxiv.org/abs/1907.01657" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>传统上，基于模型的强化学习（MBRL）旨在学习环境动力学的全局模型。 一个好的模型可以潜在地使计划算法生成各种各样的行为并解决各种各样的任务。 但是，要为复杂的动力学系统学习准确的模型仍然很困难，即使那样，该模型也可能无法很好地推广训练状态的分布。 在这项工作中，我们将基于模型的学习与对原语的无模型学习结合在一起，从而使基于模型的计划变得容易。 为此，我们旨在回答这个问题：我们如何发现其结果易于预测的技能？ 我们提出了一种无监督的学习算法，即“动态感知技能发现（DADS）”，它可以同时发现可预测的行为并学习其动态。 从理论上讲，我们的方法可以利用连续的技能空间，即使对于高维状态空间，我们也可以无限学习许多行为。 我们证明，在学习的潜在空间中的零散规划明显优于标准MBRL和无模型的目标条件RL，可以处理稀疏奖励任务，并且在无监督技能发现方面大大优于现有的分层RL方法。 我们在以下网址公开了我们的实现：<a href="https://github.com/google-research/dads" target="_blank" rel="noopener">https://github.com/google-research/dads</a></li>
</ul>
</li>
<li><p><strong>Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives.</strong> Goyal, A., Sodhani, S., Binas, J., Peng, X. Bin, Levine, S., &amp; Bengio, Y. (2019). [<a href="http://arxiv.org/abs/1906.10667" target="_blank" rel="noopener">原文链接</a>] [<a href="https://sherlockbear.github.io/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/" target="_blank" rel="noopener">阅读笔记</a>]</p>
<ul>
<li>在各种复杂环境中运行的强化学习代理可以从其行为的结构分解中受益。通常，这是在分层强化学习的语境下解决的，往往目标是将策略分解为较低级别的原语或选项，同时较高级别的元策略针对给定情况触发适当的行为。但是，元策略仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原语，类似于分层强化学习，但没有高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用信息理论机制来实现此分散决策：每个原语都会选择需要多少有关当前状态的信息以做出决定，请求有关当前状态最多信息的原语被选择与环境交互。对原语进行规范化以使用尽可能少的信息，从而导致自然竞争和特异化。我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。</li>
</ul>
</li>
</ul>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2020-03-06T06:57:59.056Z" itemprop="dateUpdated">2020-03-06 14:57:59</time>
</span><br>


        
        这里可以写作者留言，标签和 hexo 中所有变量及辅助函数等均可调用，示例：<a href="/2020/03/06/Reinforcement-Learning-Reading-List/" target="_blank" rel="external">http://yoursite.com/2020/03/06/Reinforcement-Learning-Reading-List/</a>
        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="Kang Yachen">
            Kang Yachen
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            

            


        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives</h4>
      </a>
    </div>
  
</nav>



    




















</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Kang Yachen &copy; 2015 - 2020</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>


    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: false, REWARD: false };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





</body>
</html>
