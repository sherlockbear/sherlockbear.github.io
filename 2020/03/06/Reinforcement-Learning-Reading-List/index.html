<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- <link rel="stylesheet" type="text/css" href="/css/matery.css"> -->
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>Reinforcement Learning Reading List（持续更新） | Hamish的科研blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="Reinforcement Learning">
    <meta name="description" content="2020年初顶会论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。">
<meta name="keywords" content="Reinforcement Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning Reading List（持续更新）">
<meta property="og:url" content="https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/index.html">
<meta property="og:site_name" content="Hamish的科研blog">
<meta property="og:description" content="2020年初顶会论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_25.png">
<meta property="og:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_27.png">
<meta property="og:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_28.png">
<meta property="og:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_29.png">
<meta property="og:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_31.png">
<meta property="og:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_30.png">
<meta property="og:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/AcroRd32_CCYGAesAxx.png">
<meta property="og:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_32.png">
<meta property="og:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_34.png">
<meta property="og:updated_time" content="2020-03-13T06:59:51.387Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning Reading List（持续更新）">
<meta name="twitter:description" content="2020年初顶会论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。">
<meta name="twitter:image" content="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_25.png">
    
        <link rel="alternate" type="application/atom+xml" title="Hamish的科研blog" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/avatar.jpg">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

    
</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Kang Yachen</h5>
          <a href="mailto:kangyachen@westlake.edu.cn" title="kangyachen@westlake.edu.cn" class="mail">kangyachen@westlake.edu.cn</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/sherlockbear" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Reinforcement Learning Reading List（持续更新）</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Reinforcement Learning Reading List（持续更新）</h1>
        <h5 class="subtitle">
            
                <time datetime="2020-03-06T01:44:37.000Z" itemprop="datePublished" class="page-time">
  2020-03-06
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Reading-List/">Reading List</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#ICLR2020"><span class="post-toc-number">1.</span> <span class="post-toc-text">ICLR2020</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Oral（10篇）"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">Oral（10篇）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#spotlight（15篇）"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">spotlight（15篇）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#poster"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">poster</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-Reinforcement-Learning-Reading-List"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Reinforcement Learning Reading List（持续更新）</h1>
        <div class="post-meta">
            <time class="post-time" title="2020-03-06 09:44:37" datetime="2020-03-06T01:44:37.000Z"  itemprop="datePublished">2020-03-06</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Reading-List/">Reading List</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>2020年初顶会论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。</p>
<a id="more"></a>
<p>*标注的为值得精读论文</p>
<h2 id="ICLR2020"><a href="#ICLR2020" class="headerlink" title="ICLR2020"></a>ICLR2020</h2><h3 id="Oral（10篇）"><a href="#Oral（10篇）" class="headerlink" title="Oral（10篇）"></a>Oral（10篇）</h3><ul>
<li><p><strong>Contrastive Learning of Structured World Models.</strong> Kipf, T., van der Pol, E., &amp; Welling, M. (2019). [<a href="http://arxiv.org/abs/1911.12247" target="_blank" rel="noopener">原文链接</a>]*</p>
<ul>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_25.png" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>表示学习，图神经网络，自监督</li>
<li>从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一个挑战。为了朝这个目标迈进，我们引入了Contrastively-trained Structured World Models（C-SWMs）。C-SWMs利用对比方法在具有合成结构的环境中进行表示学习。<strong>我们通过图神经网络建模将每个state嵌入构造为一组对象表示及其关系。这允许模型从原始像素观察中发现对象，而无需把直接监督作为学习过程的一部分</strong>。我们在包含多个交互对象的合成环境中评估C-SWMs，这些交互对象均可以被智能体相互独立操作，包括简单的Atari游戏和多对象物理模拟器。我们的实验表明，C-SWMs可以在学习到可解释的基于对象的表示形式的基础上，克服基于像素重构的模型的局限性，并在高度结构化的环境中胜过此类模型的典型代表。</li>
</ul>
</li>
<li><p><strong>IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO.</strong> Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., &amp; Madry, A. (2019). [<a href="https://github.com/openai/baselines" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>OpenAI</li>
<li>强化学习实现</li>
<li>我们通过对两个流行算法（近似策略优化PPO和信任区域策略优化TRPO）的案例，研究了深度策略梯度算法中算法进步的根源。我们调研了“代码级优化”（仅在实现中发现或被描述为核心算法的辅助细节的算法增强）的结果。看起来是次要的，然而<strong>这种优化对智能体行为有重大影响</strong>。我们的结果表明，这些优化（a）贡献了PPO优于TRPO累积奖励中的大部分收益，以及（b）从根本上改变RL方法的功能。这些观察表明了在强化学习中对效果提升的归因是困难和重要的。</li>
</ul>
</li>
<li><p><strong>A CLOSER LOOK AT DEEP POLICY GRADIENTS.</strong> Ilyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., &amp; Madry, A. (2019). [<a href="https://openreview.net/forum?id=ryxdEkHtPS" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>MIT</li>
<li>强化学习评估方法</li>
<li>我们研究了深度策略梯度算法的行为如何反映激励其发展的概念框架。为此，<strong>我们基于该框架的关键要素（gradient estimation, value prediction, 和optimization landscapes）提出了一种对SOTA的细粒度分析方法</strong>。我们的结果表明，深度策略梯度算法的行为通常会偏离其激励框架的预测：替代目标与真实奖励机制不匹配，学到的value estimators无法匹配真实的value function，并且gradient estimates与“真正”的梯度之间的相关性很差。我们发现的预测行为和实验行为之间的不匹配，凸显了我们对当前方法的理解不足，并表明需要超越当前以benchmark为中心的评估方法。</li>
</ul>
</li>
<li><p><strong>Meta-Q-Learning.</strong> Fakoor, R., Chaudhari, P., Soatto, S., &amp; Smola, A. J. (2019). [<a href="http://arxiv.org/abs/1910.00125" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>元强化学习，Q-learning</li>
<li>本文介绍了Meta-Q-Learning（MQL），这是<strong>一种用于元强化学习（meta-RL）的新的off-policy算法</strong>。MQL基于三个简单的想法。首先，我们展示了如果可以访问表示过去轨迹的上下文变量，Q-learning可以匹敌最新的meta-RL算法。其次，使用多任务目标来最大化训练任务的平均奖励是对RL策略进行元训练的有效方法。第三，元训练replay buffer中的过去数据可以被循环，用于在新任务上使用off-policy更新来改变策略。MQL借鉴了propensity estimation中的想法，从而扩大了可用于更新的数据量。在标准连续控制benchmark上进行的实验表明，MQL与最新的meta-RL算法相比具有优势。</li>
</ul>
</li>
<li><p><strong>POSTERIOR SAMPLING FOR MULTI-AGENT REINFORCE-MENT LEARNING: SOLVING EXTENSIVE GAMES WITH IMPERFECT INFORMATION.</strong> Zhou, Y., Li, J., &amp; Zhu, J. (2019). [<a href="https://openreview.net/forum?id=Syg-ET4FPS" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>清华</li>
<li>强化学习后验采样，multi-agent，CFR</li>
<li>Posterior sampling for reinforcement learning（PSRL）是在未知环境中进行决策的有用框架。PSRL维护环境的后验分布，然后在后验分布上采样的环境中进行规划。尽管PSRL在单智能体强化学习问题上表现良好，但如何将PSRL应用于多智能体强化学习问题仍待探索。在这项工作中，<strong>我们将PSRL扩展到具有不完全信息的双人零和博弈（TEGI）</strong>，这是一类多智能体系统。从技术上讲，我们将PSRL与counterfactual regret minimization（CFR，这是对环境已知的TEGI上的领先算法）相结合。我们的主要贡献是互动策略的新设计。通过我们的交互策略，我们的算法可证明以$O(\sqrt{\log T / T})$的速度收敛至Nash均衡。实验结果表明，我们的算法效果很好。</li>
</ul>
</li>
</ul>
<!-- * **A Generalized Training Approach for Multiagent Learning.** Muller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat, J., Liu, S., … Munos, R. (2019). [[原文链接](http://arxiv.org/abs/1909.12823)]
  * 博弈论，Policy-Spaced Response Oracles，$\alpha$-Rank
  * 本文研究了一种基于博弈论原理的种群训练范式，该范式被称为“Policy-Spaced Response Oracles”（PSRO）。PSRO具有一般意义，因为它（1）包含了众所周知的算法，例如fictitious play和double oracle as special cases，并且（2）原则上适用于general-sum多玩家游戏。尽管如此，以前对PSRO的研究都集中在双人零和博弈上，这种范式中纳什均衡是可计算的。从双人零和博弈转移到更一般的设置时，纳什均衡的计算很快变得不可行。在这里，我们通过考虑替代的解决方案即$\alpha$-Rank概念来扩展PSRO的理论基础，$\alpha$-Rank是唯一的（因此与Nash不同，不存在均衡选择问题），并且易于应用于general-sum多玩家设置。我们在几个游戏类中建立收敛性保证，并确定纳什均衡与$\alpha$-Rank之间的联系。我们证明了在双人游戏Kuhn and Leduc Poker中基于$\alpha$-Rank的PSRO与基于Nash求解器的PSRO相比具有相似性能。然后，通过考虑3至5人扑克游戏，我们超越了先前的PSRO应用，产生了$\alpha$-Rank比近似Nash解算器具有更快收敛速度的实例，因此证明其为良好的一般游戏解算器。我们还对MuJoCo足球进行了初步的实验验证，说明了该方法在另一个复杂领域中的可行性。 -->
<ul>
<li><p><strong>Harnessing Structures for Value-Based Planning and Reinforcement Learning.</strong> Yang, Y., Zhang, G., Xu, Z., &amp; Katabi, D. (2019). [<a href="http://arxiv.org/abs/1909.12255" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_27.png" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>MIT</li>
<li>Q函数，低秩结构</li>
<li>Value-based方法是规划与深度强化学习（RL）的基本方法之一。本文中，我们建议在规划和深度强化学习中利用state-action value函数（即Q函数）的潜在结构。特别是，如果潜在的系统动态导致了Q函数的某些全局结构，则应该能够通过利用这种结构更好地推断该函数。具体来说，我们研究了低秩结构，它在大数据矩阵中广泛存在。我们在控制和深度强化学习任务的环境中通过实验验证了低秩Q函数的存在。作为我们的主要贡献，<strong>通过利用矩阵估计（ME）技术，我们提出了一个通用框架来利用Q函数中的底层低秩结构</strong>。这使得对经典控制任务的规划程序效率更高，此外，可以将简单方案应用于value-based强化学习技术，以在“低秩”任务上始终获得更好的性能。在控制任务和Atari游戏的大量实验证实了我们方法的有效性。</li>
</ul>
</li>
<li><p><strong>Fast Task Inference with Variational Intrinsic Successor Features.</strong> Hansen DeepMind, S., Dabney DeepMind, W., Barreto DeepMind, A., Warde-Farley DeepMind, D., Van de Wiele, T., &amp; Mnih DeepMind, V. (2019). [<a href="https://openreview.net/forum?id=BJeAHkrYDS" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_28.png" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>DeepMind</li>
<li>diverse behavior，Successor features</li>
<li>已经确定，张成马尔可夫决策过程可控子空间的多样性行为可以通过奖励与其他policy有区别的policy来训练(Gregor et al., 2016; Eysenbach et al., 2018; Warde-Farley et al., 2018)。但是，这种方法的一个局限性是难以推广到超出可明确学习的有限行为集的范围，而这在后续任务中可能是必需的。Successor features(Dayan, 1993; Barreto et al., 2017)提供了一个吸引人的解决方案，适用于此泛化问题，但需要在某些基础特征空间中将奖励函数定义为线性。在本文中，我们展示了可以将这两种技术结合使用，并且相互可以解决彼此的主要局限。为此，<strong>我们引入了Variational Intrinsic Successor FeatuRes（VISR），这是一种新的算法，能够学习可控特征，可通过Successor features框架利用可控特征来提供增强的泛化能力和快速的任务推断能力</strong>。我们在全部Atari套件上对VISR进行了实验验证，我们使用了新的设置，其中奖励仅是在漫长的无监督阶段之后才短暂显示。在12场比赛中达到人类水平的表现并超过所有baselines，使我们认为VISR代表了朝着能够从有限的反馈中快速学习的智能体迈出的一步。</li>
</ul>
</li>
<li><p><strong>Observational Overfitting in Reinforcement Learning.</strong> Song, X., Jiang, Y., Tu, S., Du, Y., &amp; Neyshabur, B. (2019). [<a href="http://arxiv.org/abs/1912.02975" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_29.png" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>Google，MIT</li>
<li>过拟合，分析框架</li>
<li>在无模型强化学习（RL）中过拟合的一个主要组成部分涉及以下情况：<strong>智能体可能会根据马尔可夫决策过程（MDP）产生的观察结果错误地将奖励与某些虚假特征相关联</strong>。我们提供了一个用于分析这种情况的通用框架，我们使用该框架通过仅修改MDP的观察空间来设计了多个综合benchmarks。当智能体过拟合到不同的观察空间（即使潜在的MDP动态是固定的）时，我们称之为<em>observational overfitting</em>。我们的实验揭示了有趣的属性，尤其是在<em>implicit regularization</em>方面，还证实了先前工作中RL泛化和监督学习（SL）的结果。</li>
</ul>
</li>
<li><p><strong>Dynamics-Aware Unsupervised Discovery of Skills.</strong> Sharma, A., Gu, S., Levine, S., Kumar, V., &amp; Hausman, K. (2019). [<a href="http://arxiv.org/abs/1907.01657" target="_blank" rel="noopener">原文链接</a>]*</p>
<ul>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_31.png" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_30.png" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>Google Brain</li>
<li>传统上，基于模型的强化学习（MBRL）旨在学习环境动态的全局模型。一个好的模型可以潜在地使规划算法生成多样化的行为并解决各种不同的任务。但是，要为复杂的动态系统学习准确的模型仍然很困难，即使成功，该模型也可能无法很好地推广到训练时的状态分布之外。在这项工作中，<strong>我们将基于模型的学习与针对原语的无模型学习结合在一起，从而使基于模型的规划变得容易</strong>。为此，我们旨在回答这个问题：我们如何发现结果易于预测的技能？我们提出了一种无监督的学习算法，即“Dynamics-Aware Discovery of Skills（DADS）”，它可以同时发现<em>可预测</em>的行为并学习其动态。从理论上讲，我们的方法可以利用连续的技能空间，使我们即使面对高维状态空间也可以不停学习许多行为。我们证明，在学习到的潜在空间中进行<em>zero-shot planning</em>明显优于标准MBRL和model-free goal-conditioned RL，可以处理稀疏奖励任务，并且在无监督技能发现方面大大优于现有的分层RL方法。 我们在以下网址公开了我们的实现：<a href="https://github.com/google-research/dads" target="_blank" rel="noopener">https://github.com/google-research/dads</a></li>
</ul>
</li>
<li><p><strong>Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives.</strong> Goyal, A., Sodhani, S., Binas, J., Peng, X. Bin, Levine, S., &amp; Bengio, Y. (2019). [<a href="http://arxiv.org/abs/1906.10667" target="_blank" rel="noopener">原文链接</a>] [<a href="https://sherlockbear.github.io/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/">阅读笔记</a>]*</p>
<ul>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/AcroRd32_CCYGAesAxx.png" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>UCB</li>
<li>原语生成，信息论，去中心化原语决策</li>
<li>在各种复杂环境中运行的强化学习智能体可以从其行为的结构分解中受益。通常，这是在分层强化学习的语境下解决的，往往目标是将策略分解为较低级别的原语或选项，同时较高级别的元策略针对给定情况触发适当的行为。但是，元策略仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原语，类似于分层强化学习，但没有高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用信息论机制来实现此分散决策：<strong>每个原语都会选择需要多少有关当前状态的信息以做出决定，请求有关当前状态最多信息的原语被选择与环境交互</strong>。对原语进行正则化以使用尽可能少的信息，从而导致自然竞争和特异化。我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。</li>
</ul>
</li>
</ul>
<h3 id="spotlight（15篇）"><a href="#spotlight（15篇）" class="headerlink" title="spotlight（15篇）"></a>spotlight（15篇）</h3><ul>
<li><p><strong>DOUBLY ROBUST BIAS REDUCTION IN INFINITE HORIZON OFF-POLICY ESTIMATION.</strong> Tang, Z., Feng, Y., Li, L., Research, G., Zhou, D., &amp; Liu, Q. (2019). [<a href="https://arxiv.org/abs/1910.07186" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>Austin，Google Research</li>
<li>off-policy</li>
<li>由于典型importance sampling（IS）估计量的方差过大，因此<em>Infinite horizon</em> off-policy policy的评估是一项极具挑战性的任务。最近，Liu et al. (2018a)提出了一种方法，该方法通过估算固定密度比来显着减少infinite horizon off-policy的评估的方差，但这是以引入密度比估计误差引起的biases为代价的。在本文中，<strong>我们开发了一种对他们方法减少bias的改进，可以利用学到的value function来提高精度</strong>。我们的方法具有<em>双重鲁棒性</em>，因为当密度比或value function估计完美时，bias消失。通常，当它们中的任何一个准确时，也可以减小bias。理论和实验结果均表明，我们的方法比以前的方法具有明显的优势。</li>
</ul>
</li>
<li><p><strong>INFLUENCE-BASED MULTI-AGENT EXPLORATION.</strong> Wang, T., Wang, J., Wu, Y., &amp; Zhang, C. (2019). [<a href="https://arxiv.org/abs/1910.05512" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>清华</li>
<li>transition-dependent multi-agent settings</li>
<li>内在驱动的强化学习旨在解决稀疏奖励任务的探索挑战。但是，文献中基本上没有研究transition-dependent的多主体环境中的探索方法。我们旨在朝着解决这个问题迈出一步。<strong>我们介绍了两种探索方法：exploration via information-theoretic influence（EITI）和exploration via decision-theoretic influence（EDTI），利用智能体在协作行为中的交互</strong>。EITI使用互信息来获取智能体transition dynamics之间的相互依存关系。EDTI使用一种称为Value of Interaction（VoI）的新的内在奖励来表征和量化一个智能体的行为对其他智能体的return期望的影响。通过优化EITI或EDTI目标作为正则项，鼓励智能体协调其探索和学习策略以优化集体效果。我们展示了如何优化这些正则项，以便它们可以轻松地与策略梯度强化学习集成。由此产生的更新规则在协同探索和内在reward分布之间建立了联系。最后，我们通过实验证明了我们的方法在多种多智能体场景中的强大优势。</li>
</ul>
</li>
<li><p><strong>MODEL BASED REINFORCEMENT LEARNING FOR ATARI.</strong> Kaiser, Ł., Babaeizadeh, M., Miłos, P., Zej Osí Nski, B., Campbell, R. H., Czechowski, K., … Ai, D. (2019). [<a href="https://goo.gl/itykP8" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_32.png" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>Google Brain</li>
<li>model-based，视频预测模型</li>
<li>无模型强化学习（RL）可以用于学习复杂任务（例如Atari游戏）的有效策略，甚至可以从图像观察中学习。但是，这通常需要非常大量的交互——实际上，比人类学习相同游戏所需的交互要多得多。人是如何能如此迅速地学习？答案的部分原因可能是人们可以了解游戏的运作方式并预测哪些动作将导致理想的结果。在本文中，我们探索视频预测模型如何类似地使智能体能够以比无模型方法用更少的交互来完成Atari游戏。<strong>我们描述了Simulated Policy Learning（SimPLe），这是一种完全model-based的基于视频预测模型的深度RL算法</strong>，并提供了几种模型结构的比较，其中包括一种在我们的环境中产生最佳效果的新结构。我们的实验在智能体与环境之间进行100k次交互的低数据状态下的一系列Atari游戏中评估SimPLe，这相当于两个小时的实时播放。在大多数游戏中，SimPLe的性能优于最新的无模型算法，在某些游戏中，SimPLe的性能甚至超越一个数量级。</li>
</ul>
</li>
<li><p><strong>Behaviour Suite for Reinforcement Learning.</strong> Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A., … Deepmind, H. (2019). [<a href="https://arxiv.org/abs/1908.03568" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>DeepMind</li>
<li>benchmark</li>
<li>本文介绍了<em>Behaviour Suite for Reinforcement Learning</em>，简称bsuite。<strong>bsuite是经过精心设计的实验的集合，这些实验通过两个目标研究了强化学习（RL）智能体的核心功能</strong>。首先，要收集清晰，信息丰富和可扩展的问题，以捕捉通用和高效的学习算法设计中的关键问题。第二，通过智能体在这些共享benchmark上的表现来研究他们的行为。为了补充这项工作，我们开源了<a href="https://github.com/deepmind/bsuite" target="_blank" rel="noopener">https://github.com/deepmind/bsuite</a>，可以自动评估和分析bsuite上的任何智能体。该库有助于对RL中的核心问题进行可重复且易于访问的研究，并最终设计出卓越的学习算法。我们的代码是Python，易于在现有项目中使用。我们包含了OpenAI Baselines，多巴胺以及新的参考实现的示例。未来，我们希望纳入学界的更多出色实验，并承诺定期由著名研究人员委员会审查bsuite。</li>
</ul>
</li>
<li><p><strong>EMERGENT TOOL USE FROM MULTI-AGENT AUTOCURRICULA.</strong> Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., … Brain, G. (2019). [<a href="https://arxiv.org/abs/1909.07528" target="_blank" rel="noopener">原文链接</a>]*</p>
<ul>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2020-03-13_Mendeley_Desktop_34.png" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>OpenAI</li>
<li>multi-agent</li>
<li>通过多智能体竞争，捉迷藏的简单目标以及大规模的标准强化学习算法，我们发现主体创建了自监督的自动课程，包含多回合不同的策略，其中许多回合需要复杂的工具使用和协作。我们发现在我们的环境中智能体策略的六个紧急阶段是显而易见的，每个阶段都会给对方团队带来新的压力。例如，智能体学会使用可移动的盒子来建造multi-object掩体，这反过来又导致智能体发现他们可以使用坡道克服障碍。<strong>我们进一步提供的证据表明，与其他自监督的强化学习方法（例如内在驱动）相比，多智能体竞争可能会随着环境复杂性的提高而更好地拓展，并导致以人类相关技能为中心的行为</strong>。最后，我们提出迁移和fine-tuning作为定量评估目标能力的一种方法，并且我们在一组特定领域的智力测验中将捉迷藏智能体和内在驱动与随机初始化baseline进行了比较。</li>
</ul>
</li>
<li><p><strong>Dream to Control: Learning Behaviors by Latent Imagination.</strong> Hafner, D., Lillicrap, T., Ba, J., &amp; Norouzi, M. (2019). [<a href="http://arxiv.org/abs/1912.01603" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>Google Brain，Deepmind</li>
<li>学习得到的世界模型总结了智能体的经验，以促进学习复杂行为。虽然通过深度学习从高维sensory输入中学习世界模型变得可行，但是仍有许多潜在的方法可以从中推导行为。我们研究了Dreamer，这是一种强化学习智能体，可以只通过潜在的想象力解决图像中的长时程任务。<strong>我们通过在学到的世界模型紧凑状态空间中想象轨迹传播学到的state values的解析梯度来有效地学习行为</strong>。（We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model.） 在完成20项具有挑战性的视觉控制任务后，Dreamer在数据效率，计算时间和最终性能方面都超过了现有方法。</li>
</ul>
</li>
<li><p><strong>SIMPLIFIED ACTION DECODER FOR DEEP MULTI-AGENT REINFORCEMENT LEARNING.</strong> Hu, H., &amp; Foerster, J. N. (2019). [<a href="https://github.com/facebookresearch/Hanabi_SAD." target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>近年来，我们在AI的许多benchmark问题上看到了快速的进步，现代方法在Go，Poker和Dota中达到了近乎或超乎人类的表现。所有这些挑战的一个共同方面是，它们在设计上是<em>对抗</em>的，或者更技术地说是零和的。与这些设置相反，在现实世界中，成功通常需要人类在至少部分合作的设置下与他人合作和交流。去年，纸牌游戏<em>Hanabi</em>被确立为AI的新benchmark环境，以填补这一空白。特别是，<em>Hanabi</em>对人类很有趣，因为它完全专注于思想理论，即在观察其他玩家的行为时能够有效地推理其他玩家的意图，信念和观点的能力。强化学习（RL）面临着一个有趣的挑战，即在被他人观察时学习如何提供信息是一个有趣的挑战：强化学习从根本上要求智能体进行探索，以便发现良好的policy。 但是，如果仅仅简单地做到这一点，这种随机性将固有地使他们的动作在训练过程中给他人提供的信息少。我们提出了一种新的深度多智能体RL方法，即<em>Simplified Action Decoder</em>（SAD），该方法通过集中训练阶段解决了这一矛盾。在训练过程中，SAD允许其他智能体不仅能观察所选择的（<em>exploratory</em>）行为，而且智能体还观察其队友的<em>greedy</em>行为。通过将这种简单的intuition与用于多智能体学习的最佳实现相结合，SAD在Hanabi挑战的独立游戏部分为2-5名智能体提供达到了新SOTA的学习方法。与最佳实现组件相比，我们的ablations显示了SAD的贡献。 我们所有的代码和训练好的智能体都可以在<a href="https://github.com/facebookresearch/Hanabi_SAD" target="_blank" rel="noopener">https://github.com/facebookresearch/Hanabi_SAD</a>上找到。</li>
</ul>
</li>
<li><p><strong>IS A GOOD REPRESENTATION SUFFICIENT FOR SAM-PLE EFFICIENT REINFORCEMENT LEARNING?</strong> Du, S. S., Kakade, S. M., Wang, R., &amp; Yang, L. F. (2019). [<a href="https://arxiv.org/abs/1910.03016" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li><p>现代深度学习方法提供了学习良好表示的有效手段。但是，良好的表示形式本身是否足以进行样本有效的强化学习？仅在更经典的近似动态规划文献中，针对（最坏情况）近似误差研究了该问题。从统计学角度看，这个问题在很大程度上尚待探讨，并且现有文献主要集中在<em>允许</em>样本进行有效强化学习而几乎不了解有效强化学习的<em>必要</em>条件的情况下。</p>
<p>这项工作表明，从统计学的角度来看，情况比传统的近似观点所建议的要微妙得多，在传统的近似观点中，对满足样本有效RL的表示要求更加严格。我们的主要结果为强化学习方法提供了清晰的门槛，表明在构成良好的函数逼近（就表示的维数而言）方面存在严格的限制，我们专注于与value-based, model-based, 以及policy-based的学习相关自然表示条件。这些下限突显出，除非其近似值的质量超过某些严格的阈值，否则本身具有良好的（value- based, model-based, 或policy-based）表示不足以进行有效的强化学习。此外，这一下限还意味着样本复杂性在以下几点对比间指数级的分离：1）具有完美表示的value-based learning与具有良好但不完美表示的value-based learning；2）value-based learning与policy-based learning，3）policy-based learning和监督学习，以及4）强化学习和模仿学习。</p>
</li>
</ul>
</li>
<li><p><strong>Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees.</strong> Chen, B., Dai, B., Lin, Q., Ye, G., Liu, H., &amp; Song, L. (2019). [<a href="http://arxiv.org/abs/1903.00070" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>我们提出一种名为Neural Exploration-Exploitation Trees（NEXT）的元路径规划算法，以从先前的经验中学习，用来解决高维连续状态和动作空间中的新路径规划问题。与更经典的基于采样的方法（如RRT）相比，我们的方法在高维度上可获得更高的采样效率，并且可以从类似环境的规划经验中受益。更具体地说，NEXT利用一种新的神经体系结构，可以从问题结构中学习有前景的搜索方向。然后，将学习到的先验知识整合到UCB类型的算法中，以在解决新问题时实现<em>exploration</em>和<em>exploitation</em>之间的在线平衡。我们进行了彻底的实验，以表明NEXT通过更紧凑的搜索树解决了新的计划问题，并在某些benchmark上明显优于SOTA。</li>
</ul>
</li>
<li><p><strong>Making Sense of Reinforcement Learning and Probabilistic Inference.</strong> O’Donoghue, B., Osband, I., &amp; Ionescu, C. (2020). [<a href="http://arxiv.org/abs/2001.00805" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>强化学习（RL）将控制问题与统计估计结合在一起：智能体不知道系统动态，但可以通过经验来学习。最近的研究工作阐述了“RL作为推理”，并提出了一个特殊的框架将RL问题推广为概率推论。我们的论文揭示了该方法的主要缺点，并阐明了将RL连贯地转换为推理问题的意义。具体来说，RL智能体必须考虑其行为对未来reward和观察的影响：exploration-exploitation的权衡。在除最简单的设置之外的所有条件下，得出的推论在计算上都是棘手的，因此实际的RL算法必须重新近似。我们证明了流行的“RL作为推论”近似方法即使在非常基本的问题中也可能表现不佳。但是，我们展示了只需稍加修改，该框架就可以产生可证明具有良好性能的算法，并且我们表明，所得算法等同于最近提出的K学习，我们还进一步将其与Thompson采样结合在一起。</li>
</ul>
</li>
<li><p><strong>IMPROVING GENERALIZATION IN META REINFORCE-MENT LEARNING USING LEARNED OBJECTIVES.</strong> Kirsch, L., Van Steenkiste, S., &amp; Urgen Schmidhuber, J. ¨. (2019). [<a href="http://louiskirsch.com/code/metagenrl" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>生物进化将许多学习者的经验提炼为人类的通用学习算法。我们新颖的元强化学习算法<em>MetaGenRL</em>受此过程启发。<em>MetaGenRL</em>提取了许多复杂智能体的经验，元学习一种低复杂度的神经目标函数，该函数决定了个体在未来将如何学习。与最近的元强化算法不同，<em>MetaGenRL</em>可以推广到与元训练阶段完全不同的新环境。在某些情况下，它甚至优于人工设计的RL算法。<em>MetaGenRL</em>在元训练期间使用off-policy二阶梯度，可大大提高其样本效率。</li>
</ul>
</li>
<li><p><strong>Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning.</strong> Scobee, D. R. R., &amp; Sastry, S. S. (2019, September 25). [<a href="https://arxiv.org/abs/1909.05477" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>尽管逆强化学习（IRL）问题的大多数方法都集中在估计可以最好地解释专家的policy或在控制任务上的演示行为的奖励函数，但通常情况下，这种行为可以通过简单奖励结合一系列严格的约束更简洁地表示。在这种情况下，智能体正试图在这些给定的行为约束框架下最大化累积奖励。我们对马尔可夫决策过程（MDP）上的IRL问题进行了重新表述，以便在给定环境的nominal模型和nominal奖励函数的情况下，我们寻求在激励了智能体行为的环境中估计状态，动作和特征约束条件。 我们的方法基于最大熵IRL框架，这使我们能够根据我们对MDP的了解来推断专家演示的似然。使用我们的方法，我们可以推断能将哪些约束添加到MDP，以最大程度地增加观察这些演示得到的似然。我们提出了一种算法，该算法可迭代地推断最大似然约束以最好地解释观察到的行为，并且我们将使用模拟行为和在障碍物附近行走的人类记录数据来评估其效果。</li>
</ul>
</li>
<li><p><strong>THE INGREDIENTS OF REAL-WORLD ROBOTIC REINFORCEMENT LEARNING.</strong> Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., … Levine, S. (2019). [<a href="https://openreview.net/forum?id=rJe2syrtvS" target="_blank" rel="noopener">原文链接</a>]*</p>
<ul>
<li>在现实世界中，强化学习的成功仅限于仪器化的实验室场景，通常需要艰苦的人工和监督才能实现持续学习。在这项工作中，我们讨论了可以不断地，自主地通过现实世界中收集的数据进行改善的机器人学习系统所需的要素。我们使用dexterous manipulation作为案例研究，提出了这样一个系统的特定实例。随后，我们研究了在没有仪器的情况下学习时会遇到的许多挑战。在这种情况下，学习必须在无需人工设计的复位，仅使用板载感知器并且没有手工设计的奖励函数条件下仍然是可行的。我们提出了针对这些挑战的简单且可扩展的解决方案，然后证明了我们提出的系统在一组机器人dexterous manipulation任务上的有效性，从而提供了与该学习范式相关的挑战的深入分析。我们证明，我们的完整系统可以在没有任何人工干预的情况下进行学习，并通过真实的三爪机器人获得各种基于视觉的技能。</li>
</ul>
</li>
<li><p><strong>Measuring the Reliability of Reinforcement Learning Algorithms.</strong> Chan, S. C. Y., Fishman, S., Canny, J., Korattikara, A., &amp; Guadarrama, S. (2019). [<a href="http://arxiv.org/abs/1912.05663" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>缺乏可靠性是强化学习（RL）算法的一个众所周知的问题。近年来，这个问题已引起越来越多的关注，并且为改善它而进行的努力已大大增加。为了帮助RL研究人员和生产用户评估和提高可靠性，我们提出了一套可定量测量可靠性各个方面的指标。在这项工作中，我们专注于训练期间和学习后（固定policy）的变异性和风险。我们将这些指标设计为通用的，还设计了补充统计测试以对这些指标进行严格的比较。在本文中，我们首先描述度量标准及其设计的期望属性，度量标准的可靠性方面以及它们在不同情况下的适用性。然后，我们描述统计测试并为报告结果提出其他实用建议。度量标准和随附的统计工具已作为开源代码库<a href="https://github.com/google-research/rl-reliability-metrics" target="_blank" rel="noopener">https://github.com/google-research/rl-reliability-metrics</a>提供。我们将度量标准应用于一组通用RL算法和环境，进行比较并分析结果。</li>
</ul>
</li>
<li><p><strong>DISAGREEMENT-REGULARIZED IMITATION LEARNING.</strong> Brantley, K., Sun, W., &amp; Henaff, M. (2019). [<a href="https://openreview.net/forum?id=rkgbYyHtwB" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>我们提出了一种简单有效的算法，旨在解决模仿学习中的协变量偏移问题。它进行如下操作：通过在专家演示数据上训练一组集成policy，然后将其预测的方差作为cost（通过RL最小化）以及有监督行为克隆的cost。与对抗式模仿方法不同，它使用易于优化的固定奖励函数。我们证明了该算法的regret界，该算法在时域内是线性的，乘以一个对于行为克隆失败的某些问题显示为低的系数。我们在多个基于像素的Atari环境和连续控制任务上对算法进行了实验评估，结果表明该算法与行为克隆和生成对抗模仿学习相近或明显胜过。</li>
</ul>
</li>
</ul>
<h3 id="poster"><a href="#poster" class="headerlink" title="poster"></a>poster</h3><ul>
<li><p><strong>Graph Convolutional Reinforcement Learning.</strong> Jiang, J., Dun, C., Huang, T., &amp; Lu, Z. (2018). [<a href="http://arxiv.org/abs/1810.09202" target="_blank" rel="noopener">原文链接</a>]</p>
<ul>
<li>在多智能体环境中，学习如何合作至关重要。关键是要了解智能体之间的相互影响。但是，多智能体环境是高度动态的，智能体不断移动，其邻居不断改变。这使得学习智能体之间相互作用的抽象表示变得困难。为了解决这些困难，我们提出了图卷积增强学习，其中图卷积适应于多智能体环境潜在图的动态，并且关系内核通过它们的关系表示来捕获智能体之间的相互作用。利用卷积层从逐渐增加的感受野产生的潜在特征来学习协作，并且通过时间关系正则化来进一步改善协作以保持一致性。从实验结果来看，我们证明了我们的方法在各种协作方案中都大大优于现有方法。</li>
</ul>
</li>
<li><p>Eramo, C. D., Tateo, D., Bonarini, A., Restelli, M., Milano, P., &amp; Peters, J. (2020). Sharing Knowledge in Multi-Task Deep Reinforcement Learning. 1–18. Retrieved from <a href="https://openreview.net/forum?id=rkgpv2VFvr" target="_blank" rel="noopener">https://openreview.net/forum?id=rkgpv2VFvr</a></p>
</li>
<li><p>Reddy, S., Dragan, A. D., &amp; Levine, S. (2019). SQIL: IMITATION LEARNING VIA REINFORCEMENT LEARNING WITH SPARSE REWARDS.</p>
</li>
<li><p>Lan, Q., Pan, Y., Fyshe, A., &amp; White, M. (2019). MAXMIN Q-LEARNING: CONTROLLING THE ESTIMATION BIAS OF Q-LEARNING. Retrieved from <a href="https://github.com/qlan3/Explorer" target="_blank" rel="noopener">https://github.com/qlan3/Explorer</a></p>
</li>
<li><p>Yu, R., Wang, X., Wang, R., Zhang, Y., An, B., Shi, Z., &amp; Lai, H. (2019). LEARNING EXPENSIVE COORDINATION: AN EVENT-BASED DEEP RL APPROACH.</p>
</li>
<li><p>Huang, S., Su, H., Zhu, J., &amp; Chen, T. (2019). SVQN: SEQUENTIAL VARIATIONAL SOFT Q-LEARNING NETWORKS.</p>
</li>
<li><p>Lin, K., &amp; Zhou, J. (2019). RANKING POLICY GRADIENT.</p>
</li>
<li><p>Zhang, C., Vinyals, O., Munos, R., &amp; Bengio, S. (2018). A Study on Overfitting in Deep Reinforcement Learning. Retrieved from <a href="http://arxiv.org/abs/1804.06893" target="_blank" rel="noopener">http://arxiv.org/abs/1804.06893</a></p>
</li>
<li><p>Nair, S., &amp; Finn, C. (2019). Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation. Retrieved from <a href="http://arxiv.org/abs/1909.05829" target="_blank" rel="noopener">http://arxiv.org/abs/1909.05829</a></p>
</li>
<li><p>Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., Abbeel, P., &amp; Science, C. (2017). MULTI-AGENT REINFORCEMENT LEARNING FOR NETWORKED SYSTEM CONTROL. (1), 1–14. Retrieved from <a href="https://openreview.net/forum?id=Syx7A3NFvH" target="_blank" rel="noopener">https://openreview.net/forum?id=Syx7A3NFvH</a></p>
</li>
<li><p>Goyal, A., Bengio, Y., Botvinick, M., &amp; Levine, S. (n.d.). THE VARIATIONAL BANDWIDTH BOTTLENECK: STOCHASTIC EVALUATION ON AN INFORMATION BUDGET. Retrieved from <a href="https://openreview.net/forum?id=Hye1kTVFDS" target="_blank" rel="noopener">https://openreview.net/forum?id=Hye1kTVFDS</a></p>
</li>
<li><p>Nasim Rahaman, Steffen Wolf, Anirudh Goyal, Roman Remme, Y. B. (n.d.). LEARNING THE ARROW OF TIME FOR PROBLEMS IN REINFORCEMENT LEARNING. Retrieved from <a href="https://openreview.net/forum?id=rylJkpEtwS" target="_blank" rel="noopener">https://openreview.net/forum?id=rylJkpEtwS</a></p>
</li>
<li><p>Goyal, A., Sodhani, S., Binas, J., Peng, X. Bin, Levine, S., &amp; Bengio, Y. (2019). REINFORCEMENT LEARNING WITH COMPETITIVE ENSEMBLES OF INFORMATION-CONSTRAINED PRIMITIVES.</p>
</li>
<li><p>Jinnai, Y., Park, J. W., Machado, M. C., Brain, G., &amp; Konidaris, G. (2019). EXPLORATION IN REINFORCEMENT LEARNING WITH DEEP COVERING OPTIONS.</p>
</li>
<li><p>Zhou, A., Jang Google Brain, E., Kappler, D., Herzog, A. X., Khansari, M., Wohlart, P., … Finn Google Brain, C. (2019). WATCH, TRY, LEARN: META-LEARNING FROM DEMONSTRATIONS AND REWARDS. Retrieved from <a href="https://sites.google.com/view/watch-try-learn-project" target="_blank" rel="noopener">https://sites.google.com/view/watch-try-learn-project</a></p>
</li>
<li><p>Song, H. F., Abdolmaleki, A., Springenberg, T., Clark, A., Soyer, H., Rae, J. W., … Botvinick, M. M. (2019). V-MPO: ON-POLICY MAXIMUM A POSTERIORI POLICY OPTIMIZATION FOR DISCRETE AND CONTINUOUS CONTROL.</p>
</li>
<li><p>Racanì, S., Lampinen, A. K., Santoro, A., Reichert, D. P., Firoiu, V., &amp; Lillicrap Deepmind, T. P. (2019). AUTOMATED CURRICULA THROUGH SETTER-SOLVER INTERACTIONS.</p>
</li>
<li><p>Luo, Y., Xu, H., &amp; Ma, T. (2019). LEARNING SELF-CORRECTABLE POLICIES AND VALUE FUNCTIONS FROM DEMONSTRATIONS WITH NEGATIVE SAMPLING.</p>
</li>
<li><p>Wang, T., &amp; Ba, J. (2019). EXPLORING MODEL-BASED PLANNING WITH POLICY NETWORKS. Retrieved from <a href="https://github.com/WilsonWangTHU/POPLIN" target="_blank" rel="noopener">https://github.com/WilsonWangTHU/POPLIN</a>.</p>
</li>
<li><p>Gleave, A., Dennis, M., Wild, C., Kant, N., Levine, S., &amp; Russell, S. (2019). ADVERSARIAL POLICIES: ATTACKING DEEP REINFORCEMENT LEARNING. Retrieved from <a href="https://adversarialpolicies.github.io/" target="_blank" rel="noopener">https://adversarialpolicies.github.io/</a>.</p>
</li>
<li><p>Kumar, M., Babaeizadeh, M., Erhan, D., Finn, C., Levine, S., Dinh, L., &amp; Kingma, D. (2019). VIDEOFLOW: A CONDITIONAL FLOW-BASED MODEL FOR STOCHASTIC VIDEO GENERATION.</p>
</li>
<li><p>Han, D., Doya, K., &amp; Tani, J. (2019). Variational Recurrent Models for Solving Partially Observable Control Tasks. Retrieved from <a href="http://arxiv.org/abs/1912.10703" target="_blank" rel="noopener">http://arxiv.org/abs/1912.10703</a></p>
</li>
<li><p>Jung, W., Park, G., &amp; Sung, Y. (2019). POPULATION-GUIDED PARALLEL POLICY SEARCH FOR REINFORCEMENT LEARNING.</p>
</li>
<li><p>Dong, K., Wang, Y., Chen, X., &amp; Wang, L. (2019). Q-LEARNING WITH UCB EXPLORATION IS SAMPLE EFFICIENT FOR INFINITE-HORIZON MDP.</p>
</li>
<li><p>Wang, W., Yang, T., Liu, Y., Hao, J., Hao, X., Hu, Y., … Gao, Y. (2019). ACTION SEMANTICS NETWORK: CONSIDERING THE EFFECTS OF ACTIONS IN MULTIAGENT SYSTEMS. Retrieved from <a href="https://sites.google.com/view/iclrasn" target="_blank" rel="noopener">https://sites.google.com/view/iclrasn</a>,</p>
</li>
<li><p>Gafni, O., Wolf, L., &amp; Taigman, Y. (2019). VID2GAME: CONTROLLABLE CHARACTERS EX-TRACTED FROM REAL-WORLD VIDEOS.</p>
</li>
<li><p>Rashid, T., Peng, B., Böhmer, W., &amp; Whiteson, S. (2020). Optimistic Exploration even with a Pessimistic Initialisation. Retrieved from <a href="http://arxiv.org/abs/2002.12174" target="_blank" rel="noopener">http://arxiv.org/abs/2002.12174</a></p>
</li>
<li><p>Rupprecht, C., Ibrahim, C., &amp; Pal, C. J. (2019). FINDING AND VISUALIZING WEAKNESSES OF DEEP REINFORCEMENT LEARNING AGENTS.</p>
</li>
<li><p>Fu, Z., Yang, Z., Chen, Y., &amp; Wang, Z. (2019). ACTOR-CRITIC PROVABLY FINDS NASH EQUILIBRIA OF LINEAR-QUADRATIC MEAN-FIELD GAMES.</p>
</li>
<li><p>Bagaria, A., &amp; Konidaris, G. (2020). Option Discovery using Deep Skill Chaining. ICLR. Retrieved from <a href="https://openreview.net/forum?id=B1gqipNYwH" target="_blank" rel="noopener">https://openreview.net/forum?id=B1gqipNYwH</a></p>
</li>
<li><p>Hartikainen, K., Geng, X., Haarnoja, T., &amp; Levine, S. (2019). Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery. Retrieved from <a href="http://arxiv.org/abs/1907.08225" target="_blank" rel="noopener">http://arxiv.org/abs/1907.08225</a></p>
</li>
<li><p>Kostrikov, I., Nachum, O., Tompson, J., &amp; Research, G. (2019). IMITATION LEARNING VIA OFF-POLICY DISTRIBUTION MATCHING. Retrieved from <a href="https://github.com/google-research/" target="_blank" rel="noopener">https://github.com/google-research/</a></p>
</li>
<li><p>Ryu, M., Chow, Y., Anderson, R., Tjandraatmadja, C., Boutilier, C., &amp; Research, G. (2019). CAQL: CONTINUOUS ACTION Q-LEARNING.</p>
</li>
<li><p>Beck, J., Ciosek, K., Devlin, S., Tschiatschek, S., Zhang, C., &amp; Hofmann, K. (2019). AMRL: AGGREGATED MEMORY FOR REINFORCEMENT LEARNING.</p>
</li>
<li><p>Rhinehart, N., Mcallister, R., &amp; Levine, S. (2019). DEEP IMITATIVE MODELS FOR FLEXIBLE INFERENCE, PLANNING, AND CONTROL. Retrieved from <a href="https://sites.google.com/view/imitative-models" target="_blank" rel="noopener">https://sites.google.com/view/imitative-models</a>.</p>
</li>
<li><p>Yang, J., Nakhaei, A., Isele, D., Fujimura, K., &amp; Zha, H. (2018). CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning. Retrieved from <a href="http://arxiv.org/abs/1809.05188" target="_blank" rel="noopener">http://arxiv.org/abs/1809.05188</a></p>
</li>
<li><p>Chitnis, R., Tulsiani, S., Gupta, S., &amp; Gupta, A. (2020). Intrinsic Motivation for Encouraging Synergistic Behavior. Retrieved from <a href="http://arxiv.org/abs/2002.05189" target="_blank" rel="noopener">http://arxiv.org/abs/2002.05189</a></p>
</li>
<li><p>Ammanabrolu, P., &amp; Hausknecht, M. (2019). GRAPH CONSTRAINED REINFORCEMENT LEARNING FOR NATURAL LANGUAGE ACTION SPACES. Retrieved from <a href="https://github.com/microsoft/jericho" target="_blank" rel="noopener">https://github.com/microsoft/jericho</a></p>
</li>
<li><p>Qureshi, A. H., Johnson, J. J., Qin, Y., Henderson, T., Boots, B., &amp; Yip, M. C. (2019). COMPOSING TASK-AGNOSTIC POLICIES WITH DEEP REINFORCEMENT LEARNING. Retrieved from <a href="https://sites.google.com/view/compositional-rl" target="_blank" rel="noopener">https://sites.google.com/view/compositional-rl</a></p>
</li>
<li><p>Excellence, P. D. (2020). Discovering Motor Programs By Recomposing Demonstrations. 1–21. Retrieved from <a href="https://openreview.net/forum?id=rkgHY0NYwr" target="_blank" rel="noopener">https://openreview.net/forum?id=rkgHY0NYwr</a></p>
</li>
<li><p>Platanios, E. A., Saparov, A., &amp; Mitchell, T. (2019). JELLY BEAN WORLD: A TESTBED FOR NEVER-ENDING LEARNING.</p>
</li>
<li><p>Yang, J., Petersen, B., Zha, H., &amp; Faissol, D. (2019). Single Episode Policy Transfer in Reinforcement Learning. Retrieved from <a href="http://arxiv.org/abs/1910.07719" target="_blank" rel="noopener">http://arxiv.org/abs/1910.07719</a></p>
</li>
<li><p>Clavera, I., Fu, Y., &amp; Abbeel, P. (2019). MODEL-AUGMENTED ACTOR-CRITIC: BACKPROPAGATING THROUGH PATHS.</p>
</li>
<li><p>Jeevana Priya Inala, Osbert Bastani, Zenna Tavares, A. S.-L. (2019). Synthesizing Programmatic Policies that Inductively Generalize. Retrieved March 12, 2020, from <a href="https://openreview.net/forum?id=S1l8oANFDH" target="_blank" rel="noopener">https://openreview.net/forum?id=S1l8oANFDH</a></p>
</li>
<li><p>Mankowitz, D. J., Levine, N., Jeong, R., Shi, Y., Kay, J., Abdolmaleki, A., … Riedmiller, M. (2019). Robust Reinforcement Learning for Continuous Control with Model Misspecification. Retrieved from <a href="http://arxiv.org/abs/1906.07516" target="_blank" rel="noopener">http://arxiv.org/abs/1906.07516</a></p>
</li>
<li><p>Pan, Y., Mei, J., &amp; Farahmand, A.-M. (2019). FREQUENCY-BASED SEARCH-CONTROL IN DYNA.</p>
</li>
<li><p>Ali Mousavi, Lihong Li, Qiang Liu, D. Z. (n.d.). Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning | OpenReview. Retrieved March 12, 2020, from <a href="https://openreview.net/forum?id=S1ltg1rFDS" target="_blank" rel="noopener">https://openreview.net/forum?id=S1ltg1rFDS</a></p>
</li>
<li><p>Liu, M., Zhou, M., Zhang, W., Zhuang, Y., Wang, J., Liu, W., &amp; Yu, Y. (2019). MULTI-AGENT INTERACTIONS MODELING WITH COR-RELATED POLICIES. Retrieved from <a href="https://github.com/apexrl/CoDAIL" target="_blank" rel="noopener">https://github.com/apexrl/CoDAIL</a>.</p>
</li>
<li><p>Xiao, T., Jang, E., Kalashnikov, D., Levine, S., Ibarz, J., Hausman, K., … Berkeley, U. C. (2019). THINKING WHILE MOVING: DEEP REINFORCEMENT LEARNING WITH CONCURRENT CONTROL. Retrieved from <a href="https://sites.google.com/view/thinkingwhilemoving" target="_blank" rel="noopener">https://sites.google.com/view/thinkingwhilemoving</a>.</p>
</li>
<li><p>Long, Q., Zhou, Z., Gupta, A., Fang, F., Wu, Y., &amp; Wang, X. (2019). EVOLUTIONARY POPULATION CURRICULUM FOR SCALING MULTI-AGENT REINFORCEMENT LEARNING. Retrieved from <a href="https://sites.google.com/view/epciclr2020/" target="_blank" rel="noopener">https://sites.google.com/view/epciclr2020/</a>.</p>
</li>
<li><p>Xu, P., Gao, F., &amp; Gu, Q. (2019). SAMPLE EFFICIENT POLICY GRADIENT METHODS WITH RECURSIVE VARIANCE REDUCTION.</p>
</li>
<li><p>Gangwani, T., &amp; Peng, J. (2019). STATE-ONLY IMITATION WITH TRANSITION DYNAM-ICS MISMATCH. Retrieved from <a href="https://github.com/tgangwani/RL-Indirect-imitation" target="_blank" rel="noopener">https://github.com/tgangwani/RL-Indirect-imitation</a></p>
</li>
<li><p>Gupta, P., Puri, N., Verma, S., Kayastha, D., Deshmukh, S., Krishnamurthy, B., &amp; Singh, S. (2019). Explain Your Move: Understanding Agent Actions Using Focused Feature Saliency. Retrieved from <a href="http://arxiv.org/abs/1912.12191" target="_blank" rel="noopener">http://arxiv.org/abs/1912.12191</a></p>
</li>
<li><p>Lee, K., Lee, K., Shin, J., &amp; Lee, H. (2019). Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning. Retrieved from <a href="http://arxiv.org/abs/1910.05396" target="_blank" rel="noopener">http://arxiv.org/abs/1910.05396</a></p>
</li>
<li><p>Raileanu, R., &amp; Rocktäschel, T. (2019). RIDE: REWARDING IMPACT-DRIVEN EXPLORATION FOR PROCEDURALLY-GENERATED ENVIRONMENTS.</p>
</li>
<li><p>Yang, T.-Y., Rosca, J., Narasimhan, K., &amp; Ramadge, P. J. (2019). PROJECTION-BASED CONSTRAINED POLICY OPTIMIZATION. Retrieved from <a href="https://sites.google.com/view/iclr2020-pcpo" target="_blank" rel="noopener">https://sites.google.com/view/iclr2020-pcpo</a></p>
</li>
<li><p>Hamrick DeepMind, J. B., Bapst DeepMind, V., Sanchez-Gonzalez DeepMind, A., Pfaff DeepMind, T., Weber DeepMind, T., Buesing DeepMind, L., &amp; Battaglia DeepMind, P. W. (2019). COMBINING Q-LEARNING AND SEARCH WITH AMORTIZED VALUE ESTIMATES.</p>
</li>
<li><p>Weng, T.-W., Dvijotham, K., Uesato, J., Xiao, K., Gowal, S., Stanforth, R., &amp; Kohli, P. (2019). TOWARD EVALUATING ROBUSTNESS OF DEEP REIN-FORCEMENT LEARNING WITH CONTINUOUS CONTROL.</p>
</li>
<li><p>Kossen, J., Stelzner, K., Hussing, M., Voelcker, C., &amp; Kersting, K. (2019). STRUCTURED OBJECT-AWARE PHYSICS PREDICTION FOR VIDEO MODELING AND PLANNING.</p>
</li>
<li><p>Chen, X., Wang, L., Hang, Y., Ge, H., &amp; Zha, H. (2019). INFINITE-HORIZON OFF-POLICY POLICY EVALUATION WITH MULTIPLE BEHAVIOR POLICIES.</p>
</li>
<li><p>Gulcehre, C., Le Paine, T., Shahriari, B., Denil, M., Hoffman, M., Soyer, H., … Wang, Z. (2019). MAKING EFFICIENT USE OF DEMONSTRATIONS TO SOLVE HARD EXPLORATION PROBLEMS.</p>
</li>
<li><p>Zintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., &amp; Whiteson, S. (2019). VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning. Retrieved from <a href="http://arxiv.org/abs/1910.08348" target="_blank" rel="noopener">http://arxiv.org/abs/1910.08348</a></p>
</li>
<li><p>Siegel, N. Y., Springenberg, T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., … Deepmind, M. R. (2019). KEEP DOING WHAT WORKED: BEHAVIOR MODELLING PRIORS FOR OFFLINE REIN-FORCEMENT LEARNING.</p>
</li>
<li><p>Sohn, S., Woo, H., Choi, J., &amp; Lee, H. (2019). META REINFORCEMENT LEARNING WITH AUTONOMOUS INFERENCE OF SUBTASK DEPENDENCIES. Retrieved from <a href="https://bit.ly/msgi-videos" target="_blank" rel="noopener">https://bit.ly/msgi-videos</a>.</p>
</li>
<li><p>Whitney, W., Agarwal, R., Cho, K., &amp; Gupta, A. (2019). Dynamics-aware Embeddings. Retrieved from <a href="http://arxiv.org/abs/1908.09357" target="_blank" rel="noopener">http://arxiv.org/abs/1908.09357</a></p>
</li>
<li><p>Puigdomènech Badia, A., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski, S., … Blundell, C. (2019). NEVER GIVE UP: LEARNING DIRECTED EXPLORATION STRATEGIES.</p>
</li>
<li><p>Lowe, R., Gupta, A., Foerster, J., Kiela, D., &amp; Pineau, J. (2019). ON THE INTERACTION BETWEEN SUPERVISION AND SELF-PLAY IN EMERGENT COMMUNICATION. Retrieved from <a href="https://github.com/backpropper/s2p" target="_blank" rel="noopener">https://github.com/backpropper/s2p</a>.</p>
</li>
<li><p>Liu, F., Ling, Z., Mu, T., &amp; Su, H. (2019). STATE ALIGNMENT-BASED IMITATION LEARNING.</p>
</li>
<li><p>Ma, X., Karkus, P., Hsu, D., Lee, W. S., &amp; Ye, N. (2020). Discriminative Particle Filter Reinforcement Learning for Complex Partial Observations. Retrieved from <a href="http://arxiv.org/abs/2002.09884" target="_blank" rel="noopener">http://arxiv.org/abs/2002.09884</a></p>
</li>
<li><p>Zhu, G., Lin, Z., Yang, G., &amp; Zhang, C. (2020). Episodic Reinforcement Learning With Associiative Memory. ICLR. Retrieved from <a href="https://openreview.net/forum?id=HkxjqxBYDB" target="_blank" rel="noopener">https://openreview.net/forum?id=HkxjqxBYDB</a></p>
</li>
<li><p>Li, A. C., Florensa, C., Clavera, I., &amp; Abbeel, P. (2019). SUB-POLICY ADAPTATION FOR HIERARCHICAL REINFORCEMENT LEARNING.</p>
</li>
<li><p>Wang, T., Wang, J., Zheng, C., &amp; Zhang, C. (2019). LEARNING NEARLY DECOMPOSABLE VALUE FUNC-TIONS VIA COMMUNICATION MINIMIZATION. Retrieved from <a href="https://sites.google.com/view/ndq" target="_blank" rel="noopener">https://sites.google.com/view/ndq</a>.</p>
</li>
<li><p>Lee, Y., Yang, J., &amp; Lim, J. J. (2019). LEARNING TO COORDINATE MANIPULATION SKILLS VIA SKILL BEHAVIOR DIVERSIFICATION. Retrieved from <a href="https://clvrai.com/coordination" target="_blank" rel="noopener">https://clvrai.com/coordination</a>.</p>
</li>
</ul>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2020-03-13T06:59:51.387Z" itemprop="dateUpdated">2020-03-13 14:59:51</time>
</span><br>


        
        本文作者： Kang Yachen 本文链接： <a href="/2020/03/06/Reinforcement-Learning-Reading-List/" target="_blank" rel="external">https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/</a> 转载请注明出处
        
    </div>
    
    <footer>
        <a href="https://sherlockbear.github.io">
            <img src="/img/avatar.jpg" alt="Kang Yachen">
            Kang Yachen
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/&title=《Reinforcement Learning Reading List（持续更新）》 — Hamish的科研blog&pic=https://sherlockbear.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/&title=《Reinforcement Learning Reading List（持续更新）》 — Hamish的科研blog&source=2020年初顶会论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Reinforcement Learning Reading List（持续更新）》 — Hamish的科研blog&url=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/&via=https://sherlockbear.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives</h4>
      </a>
    </div>
  
</nav>



    

















<section class="comments" id="comments">
    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>
        var id = location.pathname
        if (location.pathname.length > 50) {
          id = location.pathname.replace(/\/\d+\/\d+\/\d+\//, '').replace('/', '').substring(0, 50)
        }
        const gitalk = new Gitalk({
          clientID: 'acc13bb9287721dc399c',
          clientSecret: '6b291913ac95fa96b3d3d1b23efd53d4bb162c08',
          repo: 'blogcomment',
          owner: 'sherlockbear',
          admin: ['sherlockbear'],
          id: id,      // Ensure uniqueness and length less than 50
          title: document.title.split('|')[0],
          distractionFreeMode: false  // Facebook-like distraction free mode
        })

        gitalk.render('gitalk-container')
    </script>
</section>




</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Kang Yachen &copy; 2015 - 2020</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/&title=《Reinforcement Learning Reading List（持续更新）》 — Hamish的科研blog&pic=https://sherlockbear.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/&title=《Reinforcement Learning Reading List（持续更新）》 — Hamish的科研blog&source=2020年初顶会论文中关于强化学习的论文列表，大部分应都有收录，如有缺漏，感谢指正。" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Reinforcement Learning Reading List（持续更新）》 — Hamish的科研blog&url=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/&via=https://sherlockbear.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://sherlockbear.github.io/2020/03/06/Reinforcement-Learning-Reading-List/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACsUlEQVR42u3aQY7jMAwEwPn/p3fPi0WcblIa+FA+DSZ2zPLBYpr6+YmPPx+O9trn////96dPDx94eHh4h0r/dLOk3Odznq96PjOvGQ8PD+8279Mbtb3Z5trnJSdZJPDw8PDez0tKTz59bpfz9hoPDw/vzbznhnhTXNJk4+Hh4b2Bl391e4M85miXnMNZCx4eHl7My6dI7/n7ynwPDw8Pbz1Vb0dZ+aNpx2CzAw8PD+8GLy9032qfjXGLM/Hw8PCO8tpoNYpNy7LaLQhFmIuHh4d3gde+uGdffeqqvBEfZip4eHh4Ma8NAvLS8+HWbCkazvfw8PDwDvFmC8PZYCJ5QBdnd3h4eHglbNYQz8pKHmK7PevjfA8PDw/vGi+PGzZv3fxxtBFt1Ezj4eHhrXn5K75ti/djrdVuCDw8PLxrvJy02dLaLg9thdHoCw8PD+8ybxYNRK1tfJfNOO2fMAIPDw/vKC/foVS8jkeRa9JGH5v14eHh4a15+fi/HVa1Ae5+u1Wx6QoPDw9vxMuj1TZgPZUut03zl60DeHh4eId47QaCWXOcj8rapvzjd+Lh4eFd4LW/4tuQ4tRiky8zq5ACDw8PLzgnKXQWtp4ahm1GaHh4eHj3eKdCgc3Ivx16JUsRHh4e3g3e7Ed+vg2rjRs2d/wyBsPDw8P7FV5SSj6UOrURoW7W8fDw8K7xkna2vU37DXkMUQQleHh4eL/CS4LaPNidsTcHHh4e3j3eZvvUBp+3zvl4rAhw8fDw8Ba8G0P92bCqxR9eVPDw8PBKXv4q34+pkpC3Zdf7I/Dw8PAO8faDq2SU1ca1swkXHh4e3tt4s3Pyprlt3L/UgIeHh/cCXhKh5mFr2yLXgzc8PDy8a7xZoDBru9tKZssMHh4e3j3egR/8QRs9a5pnETMeHh7eNd5fAiTFOjoteYgAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





</body>
</html>
