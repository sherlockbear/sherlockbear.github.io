{"meta":{"title":"Hamish的科研blog","subtitle":null,"description":null,"author":"Kang Yachen","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2019-10-10T12:53:04.391Z","updated":"2019-10-10T12:53:04.391Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2019-10-10T12:51:19.703Z","updated":"2019-10-10T12:51:19.703Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Meta-Learning: A Survey","slug":"Meta-Learning-A-Survey","date":"2019-10-09T12:37:57.000Z","updated":"2019-10-16T13:51:53.624Z","comments":true,"path":"2019/10/09/Meta-Learning-A-Survey/","link":"","permalink":"http://yoursite.com/2019/10/09/Meta-Learning-A-Survey/","excerpt":"摘要元学习或学会学习是系统地观察不同机器学习方法如何在广泛的学习任务中执行的科学，然后从这种经验或元数据中学习，以比其他方式更快地学习新任务 。这不仅极大地加速和改进了机器学习流程或神经网络架构的设计，还使我们能够用数据驱动方式学习的新方法取代手工设计算法。在本章中，我们将概述这个迷人且不断发展的领域的最新技术。","text":"摘要元学习或学会学习是系统地观察不同机器学习方法如何在广泛的学习任务中执行的科学，然后从这种经验或元数据中学习，以比其他方式更快地学习新任务 。这不仅极大地加速和改进了机器学习流程或神经网络架构的设计，还使我们能够用数据驱动方式学习的新方法取代手工设计算法。在本章中，我们将概述这个迷人且不断发展的领域的最新技术。 Introduction 当我们学习新技能时，我们很少 - 如果有的话 - 从头开始。我们从之前在相关任务中学到的技能开始，重用以前运作良好的方法，并根据经验关注可能值得尝试的内容（Lake et al。，2017）。通过学到的所有技能，学习新技能变得更容易，需要更少的示例和更少的试错。简而言之，我们跨任务学习如何学习。同样，在为特定任务构建机器学习模型时，我们通常会建立相关任务的经验，或者使用我们（通常是隐含的）对机器学习技术行为的理解来帮助做出正确的选择。 元学习的挑战是以系统的，数据驱动的方式从先前的经验中学习。首先，我们需要收集描述先前学习任务和先前学习模型的元数据。它们包括用于训练模型的精确算法配置，包括超参数设置，流程组合和/或网络架构，所得到的模型评估，例如准确性和训练时间，学到的模型参数，例如训练到的神经网络的权重，以及任务本身的可测量的适当关系，也称为元特征。接着，我们需要从这个先前的元数据开始学习，以提取和传递知识用于指导搜索新任务的最佳模型。本章简要概述了有效实现这一目标的不同元学习方法。 元学习一词涵盖基于其他任务的先前经验的任何类型的学习。这些先前的任务越相似，我们可以利用的元数据类型就越多，并且定义任务相似性将是一个关键的总体挑战。不用多说，天下没有免费的午餐（Wolpert和Macready，1996； Giraud-Carrier和Provost，2005）。当一项新任务代表完全不相关的现象或随机噪音时，利用先前的经验将无效。幸运的是，在现实世界中的任务中，有很多机会可以学习以前的经验。 在本章的其余部分，我们根据元数据学习所利用的元数据类型对元学习技术进行分类，从最通用的到最特定于任务的。首先，在第2节中，我们讨论如何纯粹从模型评估中学习。这些技术可用于推荐通常有用的配置和配置搜索空间，以及从经验相似的任务中转移知识。在第3节中，我们讨论如何表征任务以更明确地表达任务相似性并建立元模型来学习数据特征与学习性能之间的关系。最后，第4节介绍了如何在固有相似的任务之间传递训练后的模型参数，例如共享相同的输入功能，从而可以进行迁移学习（Pan和Yang，2010）和少样本学习（Ravi和Larochelle，2017）。 请注意，尽管多任务学习（Caruana，1997）（同时学习多个相关任务）和集成学习（Dietterich，2000）（在同一任务上构建多个模型）通常可以与元学习系统有意义地结合，本身并不涉及在其他任务上的先前经验中学习。 Learning from Model Evaluations 考虑到我们可以访问先前任务$t_j \\in T$，所有已知任务的集合以及一组学习算法，这些算法完全由其配置$\\theta_i\\in \\Theta$定义； 在此，$\\Theta$表示离散的，连续的或混合的配置空间，其可以包括超参数设置，流程组件和/或网络架构组件。$P$是任务$t_j$上所有配置$\\theta_i$的所有先前标量评估$P_{i, j}=P(\\theta_i,t_j)$的集合，根据预先定义的评估方法，例如 准确性和模型评估技术，例如 交叉验证。$P_{new}$是新任务$t_{new}$上一组已知评估$P_{i,new}$的集合。现在，我们想训练一个元学习器$L$，它预测针对新任务$t_{new}$的推荐配置$\\Theta^\\ast _{new}$。元学习器接受元数据$P\\cup P_{new}$的训练。$P$通常是事先收集的，或者是从元数据存储库中提取的（Vanschoren等，2014，2012）。通过元学习技术本身以迭代方式学习$P_{new}$，有时以另一种方法生成的初始$P’_ {new}$进行热启动。 Task-Independent Recommendations首先，假设无法获得有关$t_{new}$的任何评估，因此$P_{new}=\\emptyset$。然后，我们仍然可以学习函数$f: \\Theta \\times T \\to \\{\\theta^\\ast _ k\\},k = 1..K$，产生了一组独立于$t_{new}$的推荐配置。然后可以重新评估这些$\\theta ^ \\ast _ k$，以选择最佳的$\\theta _ k$，或热启动进一步的优化方法，例如第2.3节中讨论的方法。 这种方法通常会产生排序，即有序集合$\\theta ^ \\ast _ k$。这通常是通过将$\\theta$离散化为一组候选配置$\\theta_ i$，该候选配置也称为portfolio，在大量任务$t_j$上进行评估来完成的。然后，我们可以根据成功率，AUC或significant wins来建立每个任务上的排名（Brazdil等，2003a； Demˇsar，2006； Leite等，2012）。但是，通常希望将同样好的但速度更快的算法排在更高的位置，并且已经提出了多种方法来权衡准确性和训练时间（Brazdil等人，2003a; van Rijn等人，2015）。接下来，我们可以将这些单任务排名汇总为全局排名，例如通过计算在所有任务上的平均排名（Lin，2010; Abdulrahman et al。，2018）。当没有足够的数据来建立全局排名时，可以根据每个先验任务的最佳已知配置来推荐配置子集（Todorovski和Dzeroski，1999; Kalousis，2002），或者返回准线性排名（Cook等。（1996）。 为了找到任务$t_{new}$的最佳$\\theta^\\ast$，这是从未见过的任务，一种随时随地的简单方法是选择前$K$个配置（Brazdil等人，2003a），从列表中查找并依次评估每个配置在$t_{new}$上的表现。在固定值$K$个，时间预算约束或找到足够准确的模型之后，可以停止此评估。在时间受限的环境中，已表明多目标排名（包括训练时间）更快地收敛到接近最优的模型（Abdulrahman等，2018； van Rijn等，2015），并提供了强有力的基线用于算法比较（Abdulrahman等，2018; Leite等，2012）。 与上述方法非常不同的方法是，首先对特定任务$t_j$的所有先前评估拟合微分函数$f_j(\\theta_i)= P_{i,j}$，然后使用梯度下降找到每个先前任务的优化配置$\\theta^\\ast _ j$（Wistuba等人，2015a）。假设某些任务$t_j$与$t_{new}$相似，则这些$\\theta ^ \\ast _ j$对于热启动贝叶斯优化方法很有用。 Confifiguration Space DesignConfifiguration TransferRelative LandmarksSurrogate ModelsWarm-Started Multi-task LearningOther TechniquesLearning CurvesLearning from Task Properties Task-Independent RecommendationsConfifiguration Space DesignConfifiguration TransferRelative LandmarksSurrogate ModelsWarm-Started Multi-task LearningOther TechniquesLearning CurvesLearning from Task Properties Meta-FeaturesLearning Meta-FeaturesWarm-Starting Optimization from Similar TasksMeta-ModelsRankingPerformance PredictionPipeline SynthesisTo Tune or Not to Tune?Learning from Prior Models Transfer LearningMeta-Learning in Neural NetworksFew-Shot Learning一项特别具有挑战性的元学习问题是，鉴于有大量可用训练集的非常相似任务的先前经验，我们仅使用几个训练示例就可以训练出准确的深度学习模型。这称为少样本学习。人类对此具有与生俱来的能力，我们希望构建可以做到这一点的机器学习agent（Lake等人，2017）。一个具体的例子是“ K-shot N-way”分类，其中给了我们某些类别（例如物体）的许多例子（例如图像），并且想要学习一个能够仅使用$K$个示例对$N$个新类别进行分类的分类器$l_{new}$。 例如，利用先前的经验，我们可以学习所有任务的通用特征表示，通过更好的模型参数初始化$W_{init}$开始训练$l_{new}$，以及获得归纳偏差以帮助指导模型参数的优化，从而使$l_{new}$能比其他方法训练快得多。 单样本学习的早期工作主要基于手工设计的特征 (Fei-Fei et al., 2006; Fei-Fei, 2006; Fink, 2005; Bart and Ullman, 2005)。但是，通过元学习，我们希望以端到端的方式学习所有任务的通用特征表示。 Vinyals et al. (2016) 指出，要从很少的数据中学习，就应该关注非参数模型（例如k近邻），该模型使用记忆组件而不是学习许多模型参数。他们的元学习器是一个匹配网络，它应用了神经网络中记忆组件的概念。它为标记的样本学习通用表示，并使用余弦相似度将每个新的测试样本与存储的样本进行匹配。该网络在小批次上进行了训练，每个批次仅包含几个特定任务的样本。 Snell et al. (2017) 提出了原型网络，将样本映射到p维向量空间，以使给定输出类的样本彼此接近。然后，它计算每个类的原型（均值向量）。新的测试样本被映射到相同的向量空间，并且距离度量用于在所有可能的类上创建softmax。Ren et al. (2018) 将这种方法扩展到半监督学习。 Ravi and Larochelle (2017) 使用基于LSTM的元学习器来学习用于训练神经网络学习器的更新规则。对于每个新样本，学习器将当前的梯度和损失返回给LSTM元学习器，然后LSTM元学习器更新学习器的模型参数$\\{w_k\\}$。元学习器在所有先前任务上训练。 另一方面，模型无关的元学习（MAML） (Finn et al., 2017)不尝试学习更新规则，而是学习模型参数初始化$W_{init}$，该模型可以更好地推广到类似任务。从随机$\\{w_k\\}$开始，迭代选择一批先前的任务，并针对每个任务对学习器进行$K$个样本的训练，以计算梯度和损失（在测试集上）。然后，它会将元梯度反向传播，以沿权重$\\{w_k\\}$更容易更新的方向进行更新。换句话说，在每次迭代之后，权重$\\{w_k\\}$成为更好的$W_{init}$，可以开始对任何任务进行微调。Finn and Levine (2017) 表明，在使用足够深的ReLU网络和某些损失的情况下，MAML能够逼近任何学习算法。他们还得出结论，与基于LSTM的元学习方法相比，MAML初始化对于小样本的过拟合更具弹性，并且泛化得更广泛。 Grant et al. (2018) 提出了MAML的新派生和扩展，说明了该算法可以理解为推理贝叶斯模型中先验分布的参数。 REPTILE (Nichol et al., 2018) 是MAML的近似值，它对给定任务执行$K$次迭代的随机梯度下降，然后朝$K$次迭代后获得的权重方向逐渐移动初始化权重。Intuition在于每个任务可能具有一组以上的最佳权重$\\{w_i^\\ast\\}$，目标是找到一个与每个任务至少接近$\\{w_i^\\ast\\}$的$W_{init}$。 最后，我们还可以从黑盒神经网络派生一个元学习器。 Santoro et al. (2016a)提出了记忆增强神经网络（MANNs），它可以训练神经图灵机（NTM） (Graves et al., 2014)，这是一种具有增强记忆功能的神经网络，是一种元学习器。然后，该元学习者可以记住有关先前任务的信息，并利用这些信息学习学习器$l_{new}$。 SNAIL (Mishra et al., 2018)是一种通用的元学习器架构，由交错的时间卷积和因果attention层组成。卷积网络为训练样本（图像）学习一个公共特征向量，以汇总过去的经验信息。因果关注层可从收集的经验中学习选择哪些信息，以适应新的任务。 总体而言，深度学习和元学习的交集被证明是开创性新思想的特别沃土，我们希望随着时间的推移，这一领域将变得越来越重要。 Beyond Supervised LearningConclusion 元学习机会以许多不同的方式展现出来，并且可以使用多种学习技术加以体现。每当我们尝试学习某个任务时，无论成功与否，我们都会获得有益的经验，可以利用这些经验来学习新任务。我们永远不必完全从头开始。相反，我们应该系统地收集我们的“学习资源”，并从中学习以构建随着时间的推移不断改进的AutoML系统，从而帮助我们更加有效地解决新的学习问题。我们遇到的新任务越多，这些新任务越相似，我们就越可以利用先前的经验，以至于大多数必需的学习已经事先完成。计算机系统能够存储几乎无限量的以前的学习经验（以元数据的形式）的能力为以全新的方式使用该经验提供了广泛的机会，而我们才刚刚开始学习如何从中学习事先有效的经验。然而，这是一个值得实现的目标：学习如何学习任何任务，不仅使我们了解如何学习特定的任务，还使我们拥有了更多的能力。","categories":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://yoursite.com/categories/论文笔记/"}],"tags":[{"name":"meta-learning","slug":"meta-learning","permalink":"http://yoursite.com/tags/meta-learning/"},{"name":"综述","slug":"综述","permalink":"http://yoursite.com/tags/综述/"}]},{"title":"Learning to Learn via Self-Critique","slug":"Learning-to-Learn-via-Self-Critique","date":"2019-10-09T12:33:12.000Z","updated":"2019-10-09T12:36:06.903Z","comments":true,"path":"2019/10/09/Learning-to-Learn-via-Self-Critique/","link":"","permalink":"http://yoursite.com/2019/10/09/Learning-to-Learn-via-Self-Critique/","excerpt":"","text":"摘要在少样本学习中，机器学习系统从一小组与特定任务有关的有标签样本中学习，从而可以推广到同一任务的新示例。鉴于此类任务中有标签样本的数量有限，我们希望充分利用所有可能的信息。通常，模型从小型训练集（support-set）中学习任务特定的信息，以对无标签验证集（target-set也叫query-set）进行预测。target-set包含其他特定于任务的信息，而现有的少样本学习方法并未利用这些信息。通过transductive learning来使用target-set样本需要更先进的方法；at inference time, the target-set contains only unlabelled input data-points, and so discriminative learning cannot be used。在本文中，我们提出了一个名为“Self-Critique and Adapt”或SCA的框架，该框架可以学习无标签损失函数，该函数被参数化为神经网络。基本模型使用现有方法（例如，随机梯度下降与交叉熵损失相结合）在支持集上学习，然后使用学习到的损失函数针对传入的target-task进行更新。学习无标签损失函数，以便target-set-updated模型实现更高的泛化性能。实验表明，与仅适用于支持集的基准相比，SCA可以显着降低错误率，并可以在Mini-ImageNet和Caltech-UCSD Birds 200上获得最先进的基准性能。 3 Self-Critique and Adapt为了让模型学习和适应仅输入数据点可用的设置（例如，在给定任务的少样本target-set上），就需要一种无标签损失函数。例如，许多无监督的学习方法试图使生成概率最大化，因此使用负对数似然度（或其bound）作为损失函数。通常，大多数生成模型都与任务无关。在一组特定的任务中，针对损失函数可能会有更恰当和专业的选择。 手动设计这样的损失函数具有挑战性，通常只能产生可能在一种设置下起作用而在另一种情况下不起作用的损失函数。了解损失函数选择的全部影响并不容易。相反，我们提出了一种Self-Critique and Adapt方法，该方法元学习特定任务集的损失函数。它是通过使用set-to-set少样本学习框架并使用端到端基于梯度的可微元学习作为我们的学习框架来解决问题的。 SCA与模型无关，可以应用在任何使用内环优化过程来获取特定于任务的信息的端到端可微且基于梯度的元学习方法之上。许多这样的方法（Ravi and Larochelle, 2016; Finn et al., 2017; Li et al., 2017; Antoniou et al., 2019; Finn et al., 2018; Qiao et al., 2018; Rusu et al., 2018; Grant et al., 2018）目前正在争夺少样本学习领域中的SOTA。 在上图中总结的Self-Critique and Adapt，采用一个基本模型并使用现有的基于梯度的元学习方法(e.g. MAML (Finn et al., 2017), MAML++ (Antoniou et al., 2019) or LEO (Rusu et al., 2018))，根据support-set更新基本模型，然后推断出对target-set的预测。推断出预测后，将它们与其他基本模型相关的信息（例如模型参数，任务嵌入等）串联在一起，然后传递到可学习的critic loss network，其输出应解释为给定输入的loss值。该critic network计算并返回关于target-set的损失。然后针对该critic loss，使用任何随机梯度优化方法(如SGD)更新基本模型；如有必要，可以多次进行更新。这种inner-loop优化可生成特定于support/target set信息的预测模型。 内部循环过程在推理时直接用于手头的任务。但是，与其他元学习设置一样，我们使用一系列训练任务来优化inner-loop（这些任务与第7节中所述的测试任务不同）。使用训练任务中的ground truth标签评估inner-loop学习的预测模型的质量。然后，outer loop优化初始参数和critic loss，以最大化inner loop预测的质量。与其他元学习方法一样，整个inner loop的可微性确保可以使用基于梯度的方法来学习此outer loop。 在本文中，我们使用MAML ++作为基本方法。我们用$f(\\cdot,\\theta)$表示参数化为神经网络的模型，参数为$\\theta$，critic loss $C(\\cdot,W)$也是参数为$W$的神经网络。我们想学习好的参数$\\theta$和$W$，当优化模型$f$时，针对support set $S_b=\\{x_S, y_S\\}$上的loss $L$，需要执行$N$步优化，然后再针对critic loss $C$ 另外向target-set $T_b = \\{x_T\\}$优化$I$步，可以在target-set上实现良好的泛化性能。这里，$b$是一个具体任务在一批任务中的索引。完整算法在下面进行了描述。 算法框图中的等式2定义了潜在的条件特征集合$F$，这些特征概括了基本模型及其行为。这些特征可以被无监督的critic loss $C$ 用来调整target set 更新。在这些可能的特征中，$f (\\theta_N,x_T)$是基本模型$f$的预测，使用参数$\\theta_N$（即针对support-set loss的N步更新后的参数），而$g(x_S,x_n)$是任务嵌入，参数化为神经网络，该神经网络以support和target输入数据点为条件。","categories":[],"tags":[]},{"title":"HOW TO TRAIN YOUR MAML","slug":"HOW-TO-TRAIN-YOUR-MAML","date":"2019-10-09T12:31:44.000Z","updated":"2019-10-09T13:56:40.058Z","comments":true,"path":"2019/10/09/HOW-TO-TRAIN-YOUR-MAML/","link":"","permalink":"http://yoursite.com/2019/10/09/HOW-TO-TRAIN-YOUR-MAML/","excerpt":"","text":"摘要少样本学习领域最近有了长足的进步。这些进步中的大多数来自将少样本学习构建为元学习问题。目前，Model Agnostic Meta Learning或MAML是通过元学习进行少样本学习的最佳方法之一。MAML简单，优雅且功能强大，但是它具有许多问题，例如对神经网络结构非常敏感，通常会导致训练过程中的不稳定，需要艰巨的超参数搜索来稳定训练并实现高泛化，在训练和推理时都非常耗费算力。在本文中，我们提出了对MAML的各种修改，这些修改不仅可以稳定系统，而且可以大大提高MAML的泛化性能，收敛速度和计算开销，我们称之为MAML++。 3 MAML \\theta_{0}=\\theta_{0}-\\beta \\nabla_{\\theta} \\sum_{b=1}^{B} \\mathcal{L}_{T_{b}}\\left(f_{\\theta_{N}^{b}\\left(\\theta_{0}\\right)}\\right)3.1 MAML的问题MAML的简单，优雅和高性能使其成为元学习的非常强大的框架。但是，MAML也有许多问题，使其难以使用。 梯度不稳定性：如上图所示，受到神经网络结构和全局超参数设置的影响，MAML在训练过程中可能非常不稳定。优化outer loop涉及多次穿过由同一网络组成的未展开的inner loop进行导数的反向传播。仅此一项就可能导致梯度问题。但是，模型架构进一步加剧了梯度问题，标准4层卷积网络但没有skip-connections。缺少任何skip-connections意味着每个梯度必须多次通过每个卷积层。实际上，梯度将被多次乘以相同的参数集。经过多次反向传播后，展开网络的深度结构和skip-connections的缺失会分别引起梯度爆炸和梯度消失问题。 二阶导数成本：通过梯度更新步骤进行优化需要计算二阶梯度，而二阶梯度的计算成本非常高昂。MAML的作者建议使用一阶近似将处理速度提高三倍，但是使用这些近似可能会对最终的泛化误差产生负面影响。已经在Reptile(Nichol et al., 2018)中尝试了进一步使用一阶方法的尝试，作者在基本模型上应用标准SGD，然后更新其初始化参数向N步更新后的参数方向迈出一步。Reptile的结果变化较大，在某些情况下超过MAML，而在另一些情况下则不如MAML。尚未提出减少计算时间而不牺牲泛化性能的方法。 缺少Batch Normalization统计量累积：影响生成性能的另一个问题是原始MAML论文中在实验中使用Batch Normalization的方式。不是累积运行统计信息，而是将当前batch的统计信息用于Batch Normalization。这导致Batch Normalization的效果较差，因为学习的偏差必须适应各种不同的均值和标准差，而不是单个均值和标准差。另一方面，如果Batch Normalization使用累积的运行统计信息，则最终将收敛到某些全局平均值和标准偏差。这样就只剩下一个均值和标准偏差来学习偏差了。使用running统计信息而不是batch统计信息，可以极大地提高收敛速度，稳定性和泛化性能，因为归一化的特征将导致更平滑的优化环境（Santurkar et al.，2018）。 共享（跨step）Batch Normalization偏差：MAML中的批处理规范化的另一个问题源于以下事实：Batch Normalization偏差未在inner loop中更新；相反，在基础模型的所有迭代中都使用相同的偏差。隐式地执行此操作将假定所有基本模型在整个inner loop更新中都是相同的，因此通过它们传递的特征具有相同的分布。这是一个错误的假设，因为在每次inner loop更新时，都会实例化一个新的基础模型，该基础模型与前一个基础模型的差异足以从偏差估计的角度将其视为新模型。因此，为基本模型的所有迭代学习单个偏差集会限制性能。 共享的inner loop（跨step和跨参数）学习率：影响泛化和收敛速度（就训练迭代而言）的一个问题是对所有参数和所有更新步骤使用共享学习率的问题。这样做会带来两个主要问题。具有固定的学习率要求进行多次超参数搜索，以找到特定数据集的正确学习率； 根据搜索的完成方式，此过程可能在计算上非常昂贵。（Li et al。，2017）中的作者建议为网络的每个参数学习学习率并更新方向。这样做解决了手动搜索正确学习率的问题，并且还允许各个参数具有较小或较大的学习率。然而，这种方法带来了自己的问题。由于网络包含40K到50K的参数（取决于数据点的维数），因此学习每个网络参数的学习率意味着要增加计算量并增加内存使用量。 固定outer loop学习率：在MAML中，作者使用具有固定学习率的Adam来优化元目标。事实证明，使用阶跃或余弦函数对学习率进行退火对于在多种情况下实现最新的泛化性能至关重要(Loshchilov &amp; Hutter, 2016; He et al., 2016; Larsson et al., 2016; Huang et al., 2017)。因此，我们认为使用静态学习率会降低MAML的泛化性能，这也可能是优化速度较慢的原因。此外，具有固定的学习速率可能意味着必须花费更多（计算）时间来调整学习速率。 4 稳定，自动和改进的MAML在本节中，我们提出了解决MAML框架问题的方法，如第3.1节所述。每个解决方案都有一个与要解决的问题相同的参考。 梯度不稳定性→多步损失优化（MSL）：MAML最小化完成对support set任务的所有inner-loop更新后的基础网络所计算出的在target set的loss。相反，我们建议最小化完成对support set任务的每一步更新的基础网络所计算出的在target set的loss。更具体地说，我们建议最小化的loss是每步support set loss更新后target set loss的加权总和。更正式地： \\theta=\\theta-\\beta \\nabla_{\\theta} \\sum_{b=1}^{B} \\sum_{i=0}^{N} v_{i} \\mathcal{L}_{T_{b}}\\left(f_{\\theta_{i}^{b}}\\right)其中$\\beta$是学习率，$L_{T_b}(f_{\\theta^b_i})$表示在$i$向最小化support set任务loss更新后的基本网络权重在任务$b$的target set loss，$v_i$表示步骤$i$中target set loss的重要性权重， 用于计算加权和。 通过使用上面提出的multi-step loss，我们改善了梯度传播，因为现在每一步的基础网络权重都直接（对于当前步loss）和间接（来自后续步的loss）接收梯度。使用第3节中描述的原始方法，由于反向传播，除最后一步外，每个步骤的基础网络权重都被隐式优化，这导致了MAML的许多不稳定问题。但是，如图1所示，使用multi-step loss可以缓解此问题。此外，我们对每步损耗采用了退火加权。最初，所有损失都对损失具有相同的贡献，但是随着迭代次数的增加，我们会减少早期步骤的权重，并逐渐增加后续步骤的权重。这样做是为了确保随着训练的进行，最终步数loss会受到优化器的更多关注，从而确保其达到可能的最低损失。如果不使用退火，我们发现最终损失可能会高于原始方法。 二阶导数成本→导数退火（DA）：使MAML具有更高的计算效率的一种方法是减少所需的inner-loop更新次数，这可以通过本报告后续部分中介绍的某些方法来实现。但是，在本段中，我们提出了一种直接减少per-step计算开销的方法。MAML的作者提出了梯度导数的一阶近似的用法。但是，他们在整个训练阶段都采用了一阶近似。相反，我们建议随着训练的进行对微分阶数进行退火。更具体地说，我们建议在训练阶段的前50个epochs使用一阶梯度，然后在训练阶段的其余时间使用二阶梯度。我们凭经验证明，这样做可以大大加快前50个epochs的速度，同时允许进行二阶训练，以实现二阶梯度提供给模型的强大泛化性能。另一个有趣的观察结果是，与更不稳定的仅二阶实验相反，微分阶数退火实验没有出现梯度爆炸或消失的事件。在开始使用二阶导数之前使用一阶可以用作一种强大的预训练方法，该方法可以学习不太可能产生梯度爆炸/减小问题的参数。 缺少Batch Normalization统计信息累积→Per-Step Batch Normalization运行统计信息（BNRS）：在MAML Finn et al. (2017)的原始实现中，作者仅使用当前batch统计信息作为Batch Normalization统计信息。我们认为，这导致了3.1节中描述的各种不良影响。为了缓解这些问题，我们建议使用running batch统计信息进行Batch Normalization。要在MAML上下文中简单地实现Batch Normalization，就需要在inner-loop fast-knowledge获取过程的所有更新步骤之间共享running batch统计信息。然而，这样做将导致不希望的结果，即所存储的统计信息将在网络的所有inner loop更新之间共享。由于能在跨网络参数的多个更新上工作的学习参数的复杂性不断提高，因此这将导致优化问题，并有可能减慢或完全停止优化。更好的替代方法是按步骤收集统计信息。要按步骤收集running统计信息，需要实例化网络中每个Batch Normalization层的N组running平均值和running标准偏差集（其中N是inner loop更新步骤的总数），并使用优化过程中采取的步骤分别更新running统计信息。per-step batch normalization方法应加快MAML的优化速度，同时潜在地提高泛化性能。 共享（跨步骤）Batch Normalization偏差→Per-Step Batch Normalization权重和偏差（BNWB）：在MAML论文中，作者训练他们的模型去学到对每一层的一组偏差。这样做是假设通过网络传递的特征的分布是相似的。但是，这是一个错误的假设，因为基本模型已更新了许多次，从而使特征分布彼此之间越来越不相似。为了解决这个问题，我们建议在inner-loop更新过程中每步学习一组偏差。这样做意味着Batch Normalization将学习特定于在每个集合处看到的特征分布的偏差，这将提高收敛速度，稳定性和泛化性能。 共享的inner loop学习率（跨步和跨参数）→学习每层每步学习率和梯度方向（LSLR）：Li et al. (2017)的先前工作，证明了学习基础网络中每个参数的学习率和梯度方向可以提高系统的泛化性能。然而，这导致参数数量增加和计算开销增加的结果。因此，我们建议改为学习网络中每一层的学习率和方向，以及随着基础网络的逐步适应而学习不同的学习率。学习每层而不是每个参数的学习率和方向应该减少所需的内存和计算，同时在更新步骤中提供更多的灵活性。此外，对于学习到的每个学习率，将有N个实例的学习率，每个步骤要采用一个实例。通过这样做，参数可以自由学习在每步降低学习率，这可以帮助减轻过拟合的情况。 固定outer loop学习率→元优化器学习率的余弦退火（CA）：在MAML中，作者在元模型的优化器上使用静态学习率。通过使用阶跃函数(He et al., 2016)或余弦函数(Loshchilov＆Hutter，2016)对学习率进行退火已被证明对于具有更高泛化能力的学习模型至关重要。余弦退火调度在产生最新技术结果方面特别有效，同时消除了对学习速率空间进行任何超参数搜索的需求。因此，我们建议将余弦退火调度应用于元模型的优化器（即元优化器）。退火学习率可使模型更有效地拟合训练集，结果可能会产生更高的泛化性能。","categories":[],"tags":[]},{"title":"META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION","slug":"META-LEARNING-WITH-LATENT-EMBEDDING-OPTIMIZATION","date":"2019-10-09T12:27:13.000Z","updated":"2019-10-09T12:29:28.270Z","comments":true,"path":"2019/10/09/META-LEARNING-WITH-LATENT-EMBEDDING-OPTIMIZATION/","link":"","permalink":"http://yoursite.com/2019/10/09/META-LEARNING-WITH-LATENT-EMBEDDING-OPTIMIZATION/","excerpt":"","text":"摘要基于梯度的元学习技术在解决具有挑战性的少样本学习和快速适应问题方面有着广泛的应用和实用价值。然而，它们在极端低数据状态下在高维参数空间上操作时存在实际困难。我们表明，有可能通过学习到一个模型参数的依赖数据的潜在生成表示，并在此低维潜在空间中执行基于梯度的元学习，从而绕过这些限制。最终的方法，latent embedding optimization（LEO），将基于梯度的自适应过程与模型参数的底层高维空间解耦。我们的评估表明，LEO可以在竞争激烈的miniImageNet和tieredImageNet少样本分类任务中达到最先进的性能。进一步的分析表明，LEO能够捕获数据中的不确定性，并能通过在潜在空间中进行优化，更有效地进行适应。 2.3 LATENT EMBEDDING OPTIMIZATION FOR META-LEARNING本文的主要贡献是表明，有可能并且确实有益的是，将基于优化的元学习技术与模型参数的高维空间解耦。我们通过学习具有信息瓶颈的随机潜在空间来实现这一目标，该瓶颈取决于输入数据，并从中生成高维参数。 我们没有像在MAML中那样显式实例化并维护一组唯一的模型参数θ，而是学习了具有相同目的的模型参数的生成分布。这是一个自然扩展：我们将找到单个最佳$\\theta^* \\in\\Theta$的要求放宽到近似于$\\Theta$的数据相关条件概率分布的要求，这可能更具表达性。由编码过程和解码（或参数生成）过程组成的结构的选择，使我们能够在学习到的参数生成模型的低维嵌入空间中执行基于MAML梯度的适应步骤（或”inner loop”）（图1）。 2.3.1 模型概述大致的操作如（算法1）所示。首先，给定任务实例$T_i$，将输入$\\{x^k_n\\}$通过随机编码器以产生潜在边码$z$，然后使用参数生成器将其解码为参数$\\theta_i$。给定这些实例化的模型参数，在潜在空间中应用一个或多个适应步骤，通过计算相对于$z$的loss的微分，梯度更新几步获得$z\\prime$，解码新的模型参数并获得新的loss。最后，对优化后的编码进行解码以生成最终的适应参数$\\theta\\prime_i$，该参数可用于执行任务或计算任务特定的meta-loss。通过这种方式，LEO结合了基于模型和基于优化的元学习的各个方面，产生的参数首先取决于输入数据，然后通过梯度下降进行调整。 图2显示了生成的网络的结构。直觉上，解码器类似于生成模型，从低维潜在编码映射到模型参数的分布。编码过程可确保基于梯度的适应之前的初始潜在编码和参数已经与数据相关。该编码过程还利用了一个关系网络，该关系网络允许潜在编码依赖于上下文，考虑到问题实例中所有类之间的成对关系。在以下各节中，我们将更正式地解释LEO的各个步骤。 2.3.2 INITIALIZATION: GENERATING PARAMETERS CONDITIONED ON A FEW EXAMPLES Encoding Decoding 2.3.3 ADAPTATION BY LATENT EMBEDDING OPTIMIZATION (LEO) (THE “INNER LOOP”)2.3.4 META-TRAINING STRATEGY (THE “OUTER LOOP”)2.3.5 BEYOND CLASSIFICATION AND LINEAR OUTPUT LAYERS","categories":[],"tags":[]},{"title":"Meta-Learning with Implicit Gradients","slug":"Meta-Learning-with-Implicit-Gradients","date":"2019-10-09T12:26:02.000Z","updated":"2019-10-15T11:12:35.192Z","comments":true,"path":"2019/10/09/Meta-Learning-with-Implicit-Gradients/","link":"","permalink":"http://yoursite.com/2019/10/09/Meta-Learning-with-Implicit-Gradients/","excerpt":"摘要智能系统的一个核心功能是能够通过借鉴先前的经验来快速学习新任务的能力。最近，基于梯度（或优化）的元学习已成为一种有效的少样本学习方法。在此形式中，仅使用当前任务中的少量数据，即可在outer loop中学习元参数，而在inner-loop中学习特定于任务的模型。扩展这些方法的关键挑战是需要通过inner-loop学习过程计算微分，这可能会带来相当大的计算和内存负担。借助隐式微分，我们开发了隐式MAML算法，该算法仅取决于inner level优化的解，而不取决于inner loop优化器采用的路径。这有效地将元梯度计算与inner loop优化器的选择解耦。因此，我们的方法与inner loop优化器的选择无关，并且可以优雅地处理许多梯度步骤而不会梯度消失或内存限制。从理论上讲，我们证明隐式MAML可以使用不超过计算单个内循环梯度所需的内存占用量来计算准确的元梯度，而不会增加总的计算成本。从实验上，我们证明了隐式MAML的这些好处可转化为在少样本的图像识别benchmarks上的经验收益。","text":"摘要智能系统的一个核心功能是能够通过借鉴先前的经验来快速学习新任务的能力。最近，基于梯度（或优化）的元学习已成为一种有效的少样本学习方法。在此形式中，仅使用当前任务中的少量数据，即可在outer loop中学习元参数，而在inner-loop中学习特定于任务的模型。扩展这些方法的关键挑战是需要通过inner-loop学习过程计算微分，这可能会带来相当大的计算和内存负担。借助隐式微分，我们开发了隐式MAML算法，该算法仅取决于inner level优化的解，而不取决于inner loop优化器采用的路径。这有效地将元梯度计算与inner loop优化器的选择解耦。因此，我们的方法与inner loop优化器的选择无关，并且可以优雅地处理许多梯度步骤而不会梯度消失或内存限制。从理论上讲，我们证明隐式MAML可以使用不超过计算单个内循环梯度所需的内存占用量来计算准确的元梯度，而不会增加总的计算成本。从实验上，我们证明了隐式MAML的这些好处可转化为在少样本的图像识别benchmarks上的经验收益。 Problem Formulation and Notations我们首先在少样本监督学习的背景下提出元学习问题，然后概括该概念以帮助本文的其余论述。 Review of Few-Shot Supervised Learning and MAML在这种设置下，我们从$P(T)$中提取了一系列元训练任务$\\{T_i\\} ^M_{i = 1}$。每个任务$T_i$与一个数据集$D_i$相关联，我们可以从中采样两个不相交的集合：$D^{tr}_i$和$D^{test}_i$。这些数据集都由$K$个输入输出对组成。令$x\\in X$和$y\\in Y$分别表示输入和输出。数据集采用$D^{tr}_i = \\{(x^k_i，y^k_i)\\}^K_{k = 1}$的形式，对于$D^{test}_i$同样。我们对学习 $h_\\phi(x): X \\to Y $形式的模型感兴趣，由$\\phi\\in\\Phi\\equiv\\mathbb {R}^{d}$参数化。任务的性能由损失函数指定，例如交叉熵或平方误差损失。我们将损失函数以$L(\\phi，D)$的形式写成参数向量和数据集的函数。任务$T_i$的目标是使用$D^{tr}_ i$学习特定于任务的参数$\\phi_i$，以便我们可以使任务的总体或测试损失$L(\\phi_i，D^{test}_ i)$最小化。 在一般的双层元学习设置中，我们考虑使用一组元参数$\\theta\\in\\Theta\\equiv\\mathbb{R}^{d}$和来自任务任务的训练数据集用于计算任务特定参数的算法空间，形式化如$\\phi_i= Alg(\\theta,D^{tr}_ i)$针对任务$T_i$。元学习的目标是学习适应后产生良好任务特定参数的元参数，如下所示： \\overbrace{\\boldsymbol{\\theta}_{\\mathrm{ML}}^{*}:=\\underset{\\boldsymbol{\\theta} \\in \\Theta}{\\operatorname{argmin}} F(\\boldsymbol{\\theta})}^{\\text{outer-level}}, \\text { where } F(\\boldsymbol{\\theta})=\\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{L}\\left(\\overbrace{Alg\\left(\\boldsymbol{\\theta}, \\mathcal{D}_{i}^{\\mathrm{tr}}\\right)}^{\\text{inner-level}}, \\mathcal{D}_{i}^{\\mathrm{test}}\\right)由于我们通常将$Alg(\\theta,D^{tr}_i)$解释为显式或隐式解决潜在优化问题，因此我们将其视为双层优化问题。在元测试（部署）时，当提供与新任务$T_j\\sim P(T)$相对应的数据集$D^{tr}_j$时，我们可以通过对学到的元参数使用适应过程来获得良好的泛化性能（即低test error），即得到$\\phi_j= Alg(\\theta^\\ast_{ML},D^{tr}_j)$。 在MAML中，$Alg(\\theta,D)$对应于以$\\theta$初始化的一个或多个梯度下降步骤。例如，如果使用一个梯度下降步骤，我们得到： \\phi_{i} \\equiv \\mathcal{A} l g\\left(\\boldsymbol{\\theta}, \\mathcal{D}_{i}^{\\mathrm{tr}}\\right)=\\boldsymbol{\\theta}-\\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}\\left(\\boldsymbol{\\theta}, \\mathcal{D}_{i}^{\\mathrm{tr}}\\right) . \\quad \\text { (inner-level of MAML) }通常，$\\alpha$是标量超参数，但也可以是可学习的向量。因此，对于MAML，元学习参数（$\\theta^\\ast_{ML}$）具有学习归纳偏差，该偏差特别适合于使用$K$个样本对$P(T)$中的任务进行微调。为了使用基于梯度的方法解决外层问题，我们需要一种贯穿$Alg$计算微分的方法。在MAML中，这对应于通过梯度下降的动力学进行反向传播。 Proximal Regularization in the Inner Level为了在内层水平上获得足够的学习，同时又避免过拟合，$Alg$需要纳入某种形式的正则化。由于MAML使用少量的梯度步骤，这对应于早停，可以解释为正则化和贝叶斯先验的一种形式。在病态条件数的优化情形和中等样本量学习的情况下，我们可能需要采用多个梯度下降步，这给MAML带来了两个挑战。首先，我们需要通过漫长的$Alg$优化路径进行存储和计算微分，这会带来相当大的计算和内存负担。其次，随着$Alg$中梯度步数的增加，模型参数$\\{\\phi_i\\}$对元参数（$\\theta $）的依赖关系会缩小和消失，从而使得元学习变得困难。为了克服这些限制，我们考虑使用更明确的正则化算法： \\mathcal{Alg}^{\\star}\\left(\\boldsymbol{\\theta}, \\mathcal{D}_{i}^{\\mathrm{tr}}\\right)=\\underset{\\boldsymbol{\\phi}^{\\prime} \\in \\Phi}{\\operatorname{argmin}} \\mathcal{L}\\left(\\boldsymbol{\\phi}^{\\prime}, \\mathcal{D}_{i}^{\\mathrm{tr}}\\right)+\\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\phi}^{\\prime}-\\boldsymbol{\\theta}\\right\\|^{2}等式中的正则项促使$\\phi_i$保持接近$\\theta$，从而始终保持很强的依赖性。正则化强度（$\\lambda$）的作用类似于MAML中的学习率（$\\alpha$），控制先验（$\\theta$）相对于数据（$D^{tr}_T$）的强度。像$\\alpha$一样，正则强度$\\lambda$也可以被学习。此外，$\\alpha$和$\\lambda$都可以是标量，向量或完整矩阵。为简单起见，我们将$\\lambda$作为标量超参数。在等式中，我们用$\\star$表示优化问题已完全解决。在实践中，我们使用迭代算法（由$Alg$表示）进行有限迭代，其返回近似最小化解。我们在分析中明确考虑了近似解与精确解之间的差异。 The Bi-Level Optimization Problem为方便起见，有时我们会使用下标而不是参数来表达对任务$T_i$的依赖性，例如我们写： \\mathcal{L}_{i}(\\phi):=\\mathcal{L}\\left(\\phi, \\mathcal{D}_{i}^{\\text {test }}\\right), \\quad \\hat{\\mathcal{L}}_{i}(\\phi):=\\mathcal{L}\\left(\\phi, \\mathcal{D}_{i}^{\\text {tr }}\\right), \\quad \\mathcal{A} l g_{i}(\\boldsymbol{\\theta}):=\\mathcal{A} \\lg \\left(\\boldsymbol{\\theta}, \\mathcal{D}_{i}^{\\text {tr }}\\right)使用这种表示法，可以将二层元学习问题更一般地写为： \\begin{array}{l}{\\boldsymbol{\\theta}_{\\mathrm{ML}}^{*}:=\\underset{\\boldsymbol{\\theta} \\in \\Theta}{\\operatorname{argmin}} F(\\boldsymbol{\\theta}), \\text { where } F(\\boldsymbol{\\theta})=\\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{L}_{i}\\left(\\mathcal{A} l g_{i}^{\\star}(\\boldsymbol{\\theta})\\right), \\text { and }} \\\\ {\\mathcal{A} l g_{i}^{\\star}(\\boldsymbol{\\theta}):=\\underset{\\boldsymbol{\\phi}^{\\prime} \\in \\Phi}{\\operatorname{argmin}} G_{i}\\left(\\boldsymbol{\\phi}^{\\prime}, \\boldsymbol{\\theta}\\right), \\text { where } G_{i}\\left(\\boldsymbol{\\phi}^{\\prime}, \\boldsymbol{\\theta}\\right)=\\hat{\\mathcal{L}}_{i}\\left(\\boldsymbol{\\phi}^{\\prime}\\right)+\\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\phi}^{\\prime}-\\boldsymbol{\\theta}\\right\\|^{2}}\\end{array} \\tag{4}Total and Partial Derivatives我们用$d$表示全导数，用$\\nabla$表示偏导数。对于形式为$L_i(\\phi_i)$的嵌套函数，其中$\\phi_i= Alg_i(\\theta)$，我们有链式法则 \\boldsymbol{d}_{\\boldsymbol{\\theta}} \\mathcal{L}_{i}\\left(\\mathcal{A} l g_{i}(\\boldsymbol{\\theta})\\right)=\\frac{d \\mathcal{A} l g_{i}(\\boldsymbol{\\theta})}{d \\boldsymbol{\\theta}} \\nabla_{\\boldsymbol{\\phi}} \\mathcal{L}_{i}(\\boldsymbol{\\phi})|_{\\boldsymbol{\\phi}=\\mathcal{A} l g_{i}(\\boldsymbol{\\theta})}=\\frac{d \\mathcal{A} l g_{i}(\\boldsymbol{\\theta})}{d \\boldsymbol{\\theta}} \\nabla_{\\boldsymbol{\\phi}} \\mathcal{L}_{i}\\left(\\mathcal{A} l g_{i}(\\boldsymbol{\\theta})\\right)注意$d_{\\theta} \\mathcal{L}_{i}(Alg_{i}(\\theta))$和$\\nabla_{\\phi} \\mathcal{L}_{i}(Alg_{i}(\\theta))$之间的重要区别。前者传递导数穿过$Alg_i(\\theta)$，而后者则不穿过。$\\nabla_{\\phi} \\mathcal{L}_{i}(Alg_{i}(\\theta))$只是梯度函数，即$\\nabla_{\\phi} \\mathcal{L}_{i}(\\phi)$，以$\\phi=Alg_{i}(\\theta)$求值。还要注意，$d_{\\theta} \\mathcal{L}_{i}(Alg_{i}(\\theta))$和$\\nabla_{\\phi} \\mathcal{L}_{i}(Alg_{i}(\\theta))$是d维向量，而$\\frac{d Alg_{i}(\\theta)}{d \\theta}$是（d×d）大小的Jacobian矩阵。在本文中，我们还将无差别地使用$d_\\theta$和$\\frac{d}{d\\theta}$。 The Implicit MAML Algorithm我们的目的是使用形如$\\theta \\gets \\theta - \\eta d_\\theta F(\\theta)$的基于迭代梯度的算法解决公式4中的双层元学习问题。尽管为简单起见，我们基于标准梯度下降法导出了我们的方法，但也可以使用任何其他优化方法，例如准牛顿法或牛顿法，Adam或带动量的梯度下降法，而无需进行任何修改。使用链式法则扩展梯度下降更新为 \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}-\\eta \\frac{1}{M} \\sum_{i=1}^{M} \\frac{d \\mathcal{Alg}_{i}^{\\star}(\\boldsymbol{\\theta})}{d \\boldsymbol{\\theta}} \\nabla_{\\phi} \\mathcal{L}_{i}\\left(\\mathcal{A} l g_{i}^{\\star}(\\boldsymbol{\\theta})\\right)在此，$\\nabla_{\\phi} \\mathcal{L}_{i}(\\mathcal{Alg}_{i}^{\\star}(\\theta))$即$\\nabla_{\\phi} \\mathcal{L}_{i}(\\phi)|_{\\phi=\\mathcal{Alg}_{i}^{\\star}(\\theta)}$，操作上可以通过自动微分而容易地求出。对于此更新规则，我们必须计算$\\frac{d Alg^\\star_{i}(\\theta)}{d \\theta}$，其中$Alg^\\star_{i}$被隐式定义为优化问题(公式4)，这带来了主要挑战。现在，我们提出一种有效的算法（在计算和内存中）以计算元梯度。 Meta-Gradient Computation如果将$Alg^\\star_{i}(\\theta)$实现为迭代算法（例如梯度下降），则计算$\\frac{d Alg^\\star_{i}(\\theta)}{d \\theta}$的一种方法是通过迭代过程传播导数，无论是正向还是反向。但是，这样做的缺点是必须明确依赖于优化路径，而优化路径必须完全存储在内存中，而当所需的梯度下降步数较多时，该路径很快变得难以处理。此外，对于诸如牛顿法的二阶优化方法，需要难以获得的三阶导数。此外，当使用不可微分的操作（如行搜索）时，此方法变得不可能。但是，通过认识到$Alg^\\star_{i}$是隐式定义的优化问题的解决方案，我们可以采用一个不同的策略，该策略不需要考虑优化路径，而只需考虑最终结果。这源自以下引理。 引理 1. （隐式雅克比）考虑公式4中对于任务$T_i$定义的$Alg^\\star_{i}(\\theta)$。另$\\phi_i=\\mathcal{Alg}_{i}^{\\star}(\\theta)$为$Alg^\\star_{i}(\\theta)$的解。如果$\\left(\\boldsymbol{I}+\\frac{1}{\\lambda} \\nabla_{\\phi}^{2} \\hat{\\mathcal{L}}_{i}\\left(\\boldsymbol{\\phi}_{i}\\right)\\right)$是可逆的，则导数雅可比行列式为 \\frac{d \\mathcal{Alg}_{i}^{\\star}(\\boldsymbol{\\theta})}{d \\boldsymbol{\\theta}}=\\left(\\boldsymbol{I}+\\frac{1}{\\lambda} \\nabla_{\\phi}^{2} \\hat{\\mathcal{L}}_{i}\\left(\\boldsymbol{\\phi}_{i}\\right)\\right)^{-1} \\tag{6}请注意，导数（Jacobian）仅取决于算法的最终结果，而不取决于算法采用的路径。因此，原则上，任何算法方法都可用于计算$Alg^\\star_{i}(\\theta)$，从而将元梯度计算与内部级优化器的选择解耦。 具体算法：虽然引理1提供了一种理想的方法来计算$Alg^\\star_{i}$的雅可比行列式，因此通过扩展元梯度，在实践中可能很难直接使用它。有两个问题特别相关。首先，元梯度要求计算$Alg^\\star_{i}(\\theta)$，这是内部优化问题的精确解。实际上，我们可能只能获得近似解。第二，对于大型深度神经网络而言，显式地构造和求逆等式6中的矩阵，用于计算雅可比行列式，可能是棘手的。为了解决这些困难，我们考虑接近理想方法的可行算法。 首先，我们考虑内部优化问题的近似解，可以通过迭代优化算法（例如梯度下降）获得。 定义1. （$\\delta$-近似算法）另$Alg_i(\\theta)$为$Alg^\\star_{i}(\\theta)$的$\\delta$-近似，如： \\|Alg_{i}(\\theta)-Alg_{i}^{\\star}(\\theta)\\| \\leq \\delta其次，我们将执行部分或近似矩阵求逆： 定义2. （$\\delta’$-近似雅可比向量积）另$g_i$为一个向量满足 \\|g_{i}-\\left(I+\\frac{1}{\\lambda} \\nabla_{\\phi}^{2} \\hat{\\mathcal{L}}_{i}\\left(\\phi_{i}\\right)\\right)^{-1} \\nabla_{\\phi} \\mathcal{L}_{i}\\left(\\phi_{i}\\right)\\| \\leq \\delta^{\\prime}这里$\\phi_i=Alg_i(\\theta)$且$Alg_i$基于定义1. 注意，定义2中的$g_i$是任务$T_i$的元梯度的近似值。观察到可以将$g_i$作为优化问题的近似解： \\min _{\\boldsymbol{w}} \\boldsymbol{w}^{\\top}\\left(\\boldsymbol{I}+\\frac{1}{\\lambda} \\nabla_{\\boldsymbol{\\phi}}^{2} \\hat{\\mathcal{L}}_{i}\\left(\\boldsymbol{\\phi}_{i}\\right)\\right) \\boldsymbol{w}-\\boldsymbol{w}^{\\top} \\nabla_{\\phi} \\mathcal{L}_{i}\\left(\\boldsymbol{\\phi}_{i}\\right)共轭梯度（CG）算法由于其出色的迭代复杂性和仅要求$\\nabla_\\phi^2\\hat{\\mathcal{L}}_i(\\phi_i)$形式的Hessian矢量积的要求而特别适合于此问题。无需显式构造或存储Hessian矩阵即可容易地获得这样的hessian矢量积（如我们在附录C中所述）。这种基于CG的求逆已成功地应用在用于深度学习的Hessian-free或Newton-CG方法[36，44]和强化学习[52，47]中的信任区域方法。算法1提出了完整的实用算法。注意，这些近似值用于开发实用的算法，会在元梯度计算中引入误差。我们将在3.2节中分析这些误差的影响，并证明它们是可控制的。有关iMAML如何推广了基于先验梯度优化的元学习算法，请参阅附录A。","categories":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://yoursite.com/categories/论文笔记/"}],"tags":[{"name":"meta-learning","slug":"meta-learning","permalink":"http://yoursite.com/tags/meta-learning/"},{"name":"MAML","slug":"MAML","permalink":"http://yoursite.com/tags/MAML/"}]},{"title":"Meta-Learning with Differentiable Convex Optimization","slug":"Meta-Learning-with-Differentiable-Convex-Optimization","date":"2019-10-09T12:19:00.000Z","updated":"2019-10-09T12:20:29.568Z","comments":true,"path":"2019/10/09/Meta-Learning-with-Differentiable-Convex-Optimization/","link":"","permalink":"http://yoursite.com/2019/10/09/Meta-Learning-with-Differentiable-Convex-Optimization/","excerpt":"","text":"摘要少样本学习的许多元学习方法都依赖于简单的基础学习器，例如最近邻分类器。但是，即使在少样本情况下，经过判别训练的线性判别器可以提供更好的泛化能力。我们建议使用这些判别器作为基础学习器，以学习少样本学习的表示形式，并表明它们在一系列少样本识别benchmarks中提供了特征尺寸和性能之间的更好权衡。我们的目标是学习在线性分类规则下对新类别很好地泛化的特征嵌入。为了有效地解决该目标，我们利用线性分类器的两个属性：凸问题的最优性条件的隐式微分和优化问题的对偶表示。这使我们可以在计算开销适度增加的情况下使用具有更高泛化性的高维嵌入。我们的方法名为MetaOptNet，可在miniImageNet，tieredImageNet，CIFAR-FS和FC100一次性学习基准上获得最先进的性能。代码可以在这里找到","categories":[],"tags":[]}]}