<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hamish的科研blog</title>
  <icon>https://www.gravatar.com/avatar/3a8d3023f7286597c0df6b02acb9fa24</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-03-10T06:17:15.025Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Kang Yachen</name>
    <email>kangyachen@westlake.edu.cn</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Reinforcement Learning Reading List（持续更新）</title>
    <link href="http://yoursite.com/2020/03/06/Reinforcement-Learning-Reading-List/"/>
    <id>http://yoursite.com/2020/03/06/Reinforcement-Learning-Reading-List/</id>
    <published>2020-03-06T01:44:37.000Z</published>
    <updated>2020-03-10T06:17:15.025Z</updated>
    
    <content type="html"><![CDATA[<p>*标注的为值得精读论文</p><h2 id="ICLR2020"><a href="#ICLR2020" class="headerlink" title="ICLR2020"></a>ICLR2020</h2><h3 id="Oral"><a href="#Oral" class="headerlink" title="Oral"></a>Oral</h3><ul><li><p><strong>Contrastive Learning of Structured World Models.</strong> Kipf, T., van der Pol, E., &amp; Welling, M. (2019). [<a href="http://arxiv.org/abs/1911.12247" target="_blank" rel="noopener">原文链接</a>]*</p><ul><li>从对象，关系和层次结构上对我们的世界进行结构化的理解是人类认知的重要组成部分。从原始的感知数据中学习这种结构化的世界模型仍然是一个挑战。为了朝这个目标迈进，我们引入了Contrastively-trained Structured World Models（C-SWMs）。C-SWMs利用对比方法在具有合成结构的环境中进行表示学习。我们通过图神经网络建模将每个状态嵌入构造为一组对象表示及其关系。这允许模型从原始像素观察中发现对象，而无需把直接监督作为学习过程的一部分。我们在包含多个交互对象的合成环境中评估C-SWMs，这些交互对象可以由智能体独立操作，包括简单的Atari游戏和多对象物理模拟器。我们的实验表明，C-SWMs可以在学习到可解释的基于对象的表示形式的基础上，克服基于像素重构的模型的局限性，并在高度结构化的环境中胜过此类模型的典型代表。</li></ul></li><li><p><strong>IMPROVING GENERALIZATION IN META REINFORCE-MENT LEARNING USING LEARNED OBJECTIVES.</strong> Kirsch, L., Van Steenkiste, S., &amp; Urgen Schmidhuber, J. ¨. (2019). [<a href="http://louiskirsch.com/code/metagenrl" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>生物进化将许多学习者的经验提炼为人类的通用学习算法。我们新颖的元强化学习算法<em>MetaGenRL</em>受此过程启发。<em>MetaGenRL</em>提取了许多复杂智能体的经验，元学习一种低复杂度的神经目标函数，该函数决定了个体在未来将如何学习。与最近的元强化算法不同，<em>MetaGenRL</em>可以推广到与元训练阶段完全不同的新环境。在某些情况下，它甚至优于人工设计的RL算法。<em>MetaGenRL</em>在元训练期间使用off-policy二阶梯度，可大大提高其样本效率。</li></ul></li><li><p><strong>Graph Convolutional Reinforcement Learning.</strong> Jiang, J., Dun, C., Huang, T., &amp; Lu, Z. (2018). [<a href="http://arxiv.org/abs/1810.09202" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>在多智能体环境中，学习如何合作至关重要。关键是要了解智能体之间的相互影响。但是，多智能体环境是高度动态的，智能体不断移动，其邻居不断改变。这使得学习智能体之间相互作用的抽象表示变得困难。为了解决这些困难，我们提出了图卷积增强学习，其中图卷积适应于多智能体环境潜在图的动态，并且关系内核通过它们的关系表示来捕获智能体之间的相互作用。利用卷积层从逐渐增加的感受野产生的潜在特征来学习协作，并且通过时间关系正则化来进一步改善协作以保持一致性。从实验结果来看，我们证明了我们的方法在各种协作方案中都大大优于现有方法。</li></ul></li><li><p><strong>IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO.</strong> Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., &amp; Madry, A. (2019). [<a href="https://github.com/openai/baselines" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>我们通过对两个流行算法（近端策略优化PPO和信任区域策略优化TRPO）的案例，研究了深度策略梯度算法中算法进步的根源。我们调研了“代码级优化”（仅在实现中发现或被描述为核心算法的辅助细节的算法增强）的结果。看起来是次要的，然而这种优化对智能体行为有重大影响。我们的结果表明，这些优化（a）影响了PPO优于TRPO的累积奖励中的大部分收益，以及（b）从根本上改变RL方法的功能。这些观察表明了在强化学习中对效果提升的归因是困难和重要的。</li></ul></li><li><p><strong>A CLOSER LOOK AT DEEP POLICY GRADIENTS.</strong> Ilyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., &amp; Madry, A. (2019). [<a href="https://openreview.net/forum?id=ryxdEkHtPS" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>我们研究了深度策略梯度算法的行为如何反映激励其发展的概念框架。为此，我们基于该框架的关键要素（梯度估计，值预测和优化环境）提出了一种对SOTA的细粒度分析方法。我们的结果表明，深层策略梯度算法的行为通常会偏离其激励框架的预测：替代目标与真实奖励机制不匹配，学到的值预测器无法匹配真实价值函数，并且梯度预测与“真正”的梯度之间的相关性很差。我们发现的预测行为和经验行为之间的不匹配，凸显了我们对当前方法的理解不足，并表明需要超越当前以benchmark为中心的评估方法。</li></ul></li><li><p><strong>Meta-Q-Learning.</strong> Fakoor, R., Chaudhari, P., Soatto, S., &amp; Smola, A. J. (2019). [<a href="http://arxiv.org/abs/1910.00125" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>本文介绍了Meta-Q-Learning（MQL），这是一种用于元强化学习（meta-RL）的新的off-policy算法。MQL基于三个简单的想法。首先，我们展示了如果可以访问表示过去轨迹的上下文变量，Q-learning可以匹敌最新的元RL算法。其次，使用多任务目标来最大化训练任务的平均奖励是对RL策略进行元训练的有效方法。第三，元训练replay buffer中的过去数据可以被循环，用于在新任务上使用off-policy更新来适应策略。MQL借鉴了倾向性评估中的想法，从而扩大了可用于适应的数据量。在标准连续控制基准上进行的实验表明，MQL与最新的meta-RL算法相比具有优势。</li></ul></li><li><p><strong>POSTERIOR SAMPLING FOR MULTI-AGENT REINFORCE-MENT LEARNING: SOLVING EXTENSIVE GAMES WITH IMPERFECT INFORMATION.</strong> Zhou, Y., Li, J., &amp; Zhu, J. (2019). [<a href="https://openreview.net/forum?id=Syg-ET4FPS" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>强化学习的后验采样（PSRL）是在未知环境中进行决策的有用框架。PSRL维护环境的后验分布，然后在后验分布采样的环境上进行规划。尽管PSRL在单智能体强化学习问题上表现良好，但如何将PSRL应用于多智能体强化学习问题仍待探索。在这项工作中，我们将PSRL扩展到具有不完全信息（TEGI）的双人零和博弈，这是一类多智能体系统。从技术上讲，我们将PSRL与counterfactual regret minimization（CFR，这是对环境已知的TEGI上的领先算法）相结合。我们的主要贡献是互动策略的新设计。通过我们的交互策略，我们的算法可证明以$O(\sqrt{\log T / T})$的速度收敛至Nash均衡。实验结果表明，我们的算法效果很好。</li></ul></li><li><p><strong>A Generalized Training Approach for Multiagent Learning.</strong> Muller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat, J., Liu, S., … Munos, R. (2019). [<a href="http://arxiv.org/abs/1909.12823" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>本文研究了一种基于博弈论原理的训练范式，该范式被称为“Policy-Spaced Response Oracles”（PSRO）。PSRO具有一般意义，因为它（1）包含了众所周知的算法，例如虚拟游戏和特殊情况下的双重预言，并且（2）原则上适用于general-sum多玩家游戏。尽管如此，以前对PSRO的研究都集中在双人零和博弈上，这种范式中纳什均衡是可计算的。从双人零和博弈转移到更一般的设置时，纳什均衡的计算很快变得不可行。在这里，我们通过考虑替代的解决方案概念$\alpha$-Rank来扩展PSRO的理论基础，$\alpha$-Rank是唯一的（因此与Nash不同，不存在均衡选择问题），并且易于应用于general-sum多玩家设置。我们在几个游戏类中建立收敛性保证，并确定纳什均衡与$\alpha$-Rank之间的联系。我们证明了在双人游戏Kuhn and Leduc Poker中基于$\alpha$-Rank的PSRO与基于Nash求解器的PSRO相比具有相似性能。然后，通过考虑3至5人扑克游戏，我们超越了先前的PSRO应用，产生了$\alpha$-Rank比近似Nash解算器具有更快收敛速度的实例，因此证明其为良好的一般游戏解算器。我们还对MuJoCo足球进行了初步的实验验证，说明了该方法在另一个复杂领域中的可行性。</li></ul></li><li><p><strong>Harnessing Structures for Value-Based Planning and Reinforcement Learning.</strong> Yang, Y., Zhang, G., Xu, Z., &amp; Katabi, D. (2019). [<a href="http://arxiv.org/abs/1909.12255" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>Value-based方法规划与深度强化学习（RL）的基本方法之一。本文中，我们建议在规划和深度强化学习中利用state-action value函数（即Q函数）的潜在结构。特别是，如果潜在的系统动力学导致Q函数的某些全局结构，则应该能够通过利用这种结构更好地推断该函数。具体来说，我们研究了低秩结构，它在大数据矩阵中广泛存在。我们在控制和深度强化学习任务的环境中通过实验验证了低秩Q函数的存在。作为我们的主要贡献，通过利用矩阵估计（ME）技术，我们提出了一个通用框架来利用Q函数中的底层低秩结构。这使得对经典控制任务的规划程序效率更高，此外，可以将简单方案应用于value-based强化学习技术，以在“低秩”任务上始终获得更好的性能。在控制任务和Atari游戏的大量实验证实了我们方法的有效性。</li></ul></li><li><p><strong>Fast Task Inference with Variational Intrinsic Successor Features.</strong> Hansen DeepMind, S., Dabney DeepMind, W., Barreto DeepMind, A., Warde-Farley DeepMind, D., Van de Wiele, T., &amp; Mnih DeepMind, V. (2019). [<a href="https://openreview.net/forum?id=BJeAHkrYDS" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>已经确定，张成马尔可夫决策过程可控子空间的多样性行为可以通过奖励与其他policy有区别的policy来训练(Gregor et al., 2016; Eysenbach et al., 2018; Warde-Farley et al., 2018)。但是，这种方法的一个局限性是难以推广到超出可明确学习的有限行为集的范围，而这在后续任务中可能是必需的。后继特征(Dayan, 1993; Barreto et al., 2017)提供了一个吸引人的解决方案，适用于此泛化问题，但需要在某些基础特征空间中将奖励函数定义为线性。在本文中，我们展示了可以将这两种技术结合使用，并且相互可以解决彼此的主要局限。为此，我们引入了Variational Intrinsic Successor FeatuRes（VISR），这是一种新的算法，能够学习可控特征，可通过后继特征框架利用可控特征来提供增强的泛化能力和快速的任务推断。我们在全部Atari套件上对VISR进行了实验验证，我们使用了新的设置，其中奖励仅是在漫长的无监督阶段之后才短暂显示。在12场比赛中达到人类水平的表现并超过所有baselines，使我们认为VISR代表了朝着能够从有限的反馈中快速学习的智能体迈出的一步。</li></ul></li><li><p><strong>Observational Overfitting in Reinforcement Learning.</strong> Song, X., Jiang, Y., Tu, S., Du, Y., &amp; Neyshabur, B. (2019). [<a href="http://arxiv.org/abs/1912.02975" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>在无模型强化学习（RL）中过拟合的一个主要组成部分涉及以下情况：智能体可能会根据马尔可夫决策过程（MDP）产生的观察结果错误地将奖励与某些虚假特征相关联。我们提供了一个用于分析这种情况的通用框架，我们将该框架用于仅通过修改MDP的观察空间来设计多个综合benchmarks。当智能体过拟合到不同的观察空间（即使潜在的MDP动态是固定的）时，我们称之为<em>observational overfitting</em>。我们的实验揭示了有趣的属性，尤其是在<em>implicit regularization</em>方面，还证实了先前工作中RL泛化和监督学习（SL）的结果。</li></ul></li><li><p><strong>Dynamics-Aware Unsupervised Discovery of Skills.</strong> Sharma, A., Gu, S., Levine, S., Kumar, V., &amp; Hausman, K. (2019). [<a href="http://arxiv.org/abs/1907.01657" target="_blank" rel="noopener">原文链接</a>]*</p><ul><li>传统上，基于模型的强化学习（MBRL）旨在学习环境动态的全局模型。一个好的模型可以潜在地使规划算法生成多样化的行为并解决各种不同的任务。但是，要为复杂的动态系统学习准确的模型仍然很困难，即使成功，该模型也可能无法很好地推广到训练时的状态分布之外。在这项工作中，我们将基于模型的学习与针对原语的无模型学习结合在一起，从而使基于模型的规划变得容易。为此，我们旨在回答这个问题：我们如何发现结果易于预测的技能？我们提出了一种无监督的学习算法，即“Dynamics-Aware Discovery of Skills（DADS）”，它可以同时发现<em>可预测</em>的行为并学习其动态。从理论上讲，我们的方法可以利用连续的技能空间，使我们即使面对高维状态空间也可以不停学习许多行为。我们证明，在学习到的潜在空间中进行<em>zero-shot planning</em>明显优于标准MBRL和model-free goal-conditioned RL，可以处理稀疏奖励任务，并且在无监督技能发现方面大大优于现有的分层RL方法。 我们在以下网址公开了我们的实现：<a href="https://github.com/google-research/dads" target="_blank" rel="noopener">https://github.com/google-research/dads</a></li></ul></li><li><p><strong>Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives.</strong> Goyal, A., Sodhani, S., Binas, J., Peng, X. Bin, Levine, S., &amp; Bengio, Y. (2019). [<a href="http://arxiv.org/abs/1906.10667" target="_blank" rel="noopener">原文链接</a>] [<a href="https://sherlockbear.github.io/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/" target="_blank" rel="noopener">阅读笔记</a>]*</p><ul><li>在各种复杂环境中运行的强化学习代理可以从其行为的结构分解中受益。通常，这是在分层强化学习的语境下解决的，往往目标是将策略分解为较低级别的原语或选项，同时较高级别的元策略针对给定情况触发适当的行为。但是，元策略仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原语，类似于分层强化学习，但没有高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用信息理论机制来实现此分散决策：每个原语都会选择需要多少有关当前状态的信息以做出决定，请求有关当前状态最多信息的原语被选择与环境交互。对原语进行规范化以使用尽可能少的信息，从而导致自然竞争和特异化。我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。</li></ul></li></ul><h3 id="spotlight"><a href="#spotlight" class="headerlink" title="spotlight"></a>spotlight</h3><ul><li><p><strong>DOUBLY ROBUST BIAS REDUCTION IN INFINITE HORIZON OFF-POLICY ESTIMATION.</strong> Tang, Z., Feng, Y., Li, L., Research, G., Zhou, D., &amp; Liu, Q. (2019). [<a href="https://arxiv.org/abs/1910.07186" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>由于典型importance sampling（IS）估计量的方差过大，因此<em>Infinite horizon</em> off-policy policy的评估是一项极具挑战性的任务。最近，Liu et al. (2018a)提出了一种方法，该方法通过估算固定密度比来显着减少infinite horizon off-policy的评估的方差，但这是以引入密度比估计误差引起的biases为代价的。在本文中，我们开发了一种对他们方法减少bias的改进，可以利用学到的value function来提高精度。我们的方法具有<em>双重鲁棒性</em>，因为当密度比或value function估计完美时，bias消失。通常，当它们中的任何一个准确时，也可以减小bias。理论和实验结果均表明，我们的方法比以前的方法具有明显的优势。</li></ul></li><li><p><strong>INFLUENCE-BASED MULTI-AGENT EXPLORATION.</strong> Wang, T., Wang, J., Wu, Y., &amp; Zhang, C. (2019). [<a href="https://arxiv.org/abs/1910.05512" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>内在驱动的强化学习旨在解决稀疏奖励任务的探索挑战。但是，文献中基本上没有研究transition-dependent的多主体环境中的探索方法。我们旨在朝着解决这个问题迈出一步。我们介绍了两种探索方法：exploration via information-theoretic influenc（EITI）和exploration via decision-theoretic influenc（EDTI），通过利用智能体在协作行为中的交互。EITI使用互信息来获取智能体transition dynamics之间的相互依存关系。EDTI使用一种称为Value of Interaction（VoI）的新的内在奖励来表征和量化一个智能体的行为对其他智能体的return期望的影响。通过优化EITI或EDTI目标作为正则项，鼓励智能体协调其探索和学习策略以优化集体效果。 我们展示了如何优化这些正则器，以便它们可以轻松地与策略梯度强化学习集成。 由此产生的更新规则在协调的探索和内在的奖励分配之间建立了联系。 最后，我们通过经验证明了我们的方法在多种多主体场景中的强大优势。</li></ul></li><li><p><strong>MODEL BASED REINFORCEMENT LEARNING FOR ATARI.</strong> Kaiser, Ł., Babaeizadeh, M., Miłos, P., Zej Osí Nski, B., Campbell, R. H., Czechowski, K., … Ai, D. (2019). [<a href="https://goo.gl/itykP8" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>无模型强化学习（RL）可以用于学习复杂任务（例如Atari游戏）的有效策略，甚至可以从图像观察中学习。 但是，这通常需要非常大量的互动-实际上，比人类学习相同游戏所需的互动要多得多。 人们如何能如此迅速地学习？ 答案的部分原因可能是人们可以了解游戏的运作方式并预测哪些动作将导致理想的结果。 在本文中，我们探索视频预测模型如何类似地使代理能够以比无模型方法更少的交互来解决Atari游戏。 我们描述了模拟策略学习（SimPLe），这是一种基于完整模型的基于视频预测模型的深度RL算法，并提供了几种模型体系结构的比较，其中包括一种新颖的体系结构，可以在我们的环境中产生最佳效果。 我们的实验在代理与环境之间进行100k交互的低数据状态下的一系列Atari游戏中评估SimPLe，这相当于两个小时的实时播放。 在大多数游戏中，SimPLe的性能优于最新的无模型算法，在某些游戏中，SimPLe的性能超过一个数量级。</li></ul></li><li><p><strong>Behaviour Suite for Reinforcement Learning.</strong> Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A., … Deepmind, H. (2019). [<a href="https://arxiv.org/abs/1908.03568" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>本文介绍了用于强化学习的行为套件，简称bsuite。  bsuite是经过精心设计的实验的集合，这些实验研究了具有两个目标的强化学习（RL）代理的核心功能。 首先，要收集清晰，信息丰富和可扩展的问题，以捕捉通用和高效学习算法设计中的关键问题。 第二，通过代理在这些共享基准上的表现来研究他们的行为。 为了补充这项工作，我们开源了 github.com/deepmind/bsuite ，它可以自动评估和分析bsuite上的任何代理。 该库有助于对RL中的核心问题进行可重复且易于访问的研究，并最终设计出卓越的学习算法。 我们的代码是Python，易于在现有项目中使用。 我们包括OpenAI Baselines，多巴胺以及新的参考实现的示例。 展望未来，我们希望纳入研究界的更多出色实验，并承诺定期由著名研究人员委员会审查bsuite。</li></ul></li><li><p><strong>EMERGENT TOOL USE FROM MULTI-AGENT AUTOCURRICULA.</strong> Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., … Brain, G. (2019). [<a href="https://arxiv.org/abs/1909.07528" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>通过多主体竞争，捉迷藏的简单目标以及大规模的标准强化学习算法，我们发现主体创建了自我监督的自动课程，引发了多个不同的回合策略，其中许多回合需要复杂的工具 使用和协调。 我们发现在我们的环境中代理策略的六个紧急阶段是显而易见的，每个阶段都会给对立团队带来新的压力。 例如，特工学会使用可移动的盒子来建造多目标掩体，这反过来又导致特工发现他们可以使用坡道克服障碍。 我们进一步提供的证据表明，与其他自我监督的强化学习方法（例如内在动机）相比，多主体竞争可能会随着环境复杂性的提高而更好地扩展，并导致以人类相关技能为中心的行为。 最后，我们提出转移和微调作为定量评估目标能力的一种方法，并且我们在一组特定领域的智力测验中将捉迷藏的代理与内在动机和随机初始化基线进行了比较。</li></ul></li><li><p><strong>Dream to Control: Learning Behaviors by Latent Imagination.</strong> Hafner, D., Lillicrap, T., Ba, J., &amp; Norouzi, M. (2019). [<a href="http://arxiv.org/abs/1912.01603" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>经验丰富的世界模型总结了代理商的经验，以促进学习复杂行为。 虽然通过深度学习从高维感官输入中学习世界模型变得可行，但是有许多潜在的方法可以从中推导行为。 我们介绍了Dreamer，这是一种增强型学习代理，可以完全通过潜在的想象力解决图像中的长时间任务。 我们通过在学习的世界模型的紧凑状态空间中想象的轨迹传播学习状态值的解析梯度来有效地学习行为。 在完成20项具有挑战性的视觉控制任务后，Dreamer在数据效率，计算时间和最终性能方面都超过了现有方法。</li></ul></li><li><p><strong>SIMPLIFIED ACTION DECODER FOR DEEP MULTI-AGENT REINFORCEMENT LEARNING.</strong> Hu, H., &amp; Foerster, J. N. (2019). [<a href="https://github.com/facebookresearch/Hanabi_SAD." target="_blank" rel="noopener">原文链接</a>]</p><ul><li>近年来，我们在AI的许多基准问题上看到了快速的进步，现代方法在Go，Poker和Dota中达到了近乎或超乎人类的表现。 所有这些挑战的一个共同方面是，它们在设计上是对抗的，或者从技术上来说是零和。 与这些设置相反，在现实世界中，成功通常需要人类在至少部分合作的设置下与他人合作和交流。 去年，纸牌游戏Hanabi被确立为AI的新基准环境，以填补这一空白。 特别是，《哈纳比》对人类很有趣，因为它完全专注于思想理论，即在观察其他行为者时能够有效地推理其他行为者的意图，信念和观点的能力。 强化学习（RL）面临着一个有趣的挑战，即在他人观察时学习提供信息是一个有趣的挑战：强化学习从根本上要求代理商进行探索，以便发现良好的政策。 但是，如果天真地做到这一点，这种随机性将固有地使他们的动作在训练过程中对他人的信息少。 我们提出了一种新的深层多智能体RL方法，即简化动作解码器（SAD），该方法通过集中训练阶段解决了这一矛盾。 在训练过程中，SAD允许其他特工不仅观察所选择的（探索性）行为，而且特工还观察其队友的贪婪行为。 通过将这种简单的直觉与用于多主体学习的最佳实践相结合，SAD建立了一种新的SOTA，用于在Hanabi挑战的自我游戏部分为2-5名玩家提供学习方法。 与最佳实践组件相比，我们的摘录显示了SAD的贡献。 我们所有的代码和受过训练的代理都可以在 <a href="https://github.com/facebookresearch/Hanabi_SAD" target="_blank" rel="noopener">https://github.com/facebookresearch/Hanabi_SAD</a> 上找到。</li></ul></li><li><p><strong>IS A GOOD REPRESENTATION SUFFICIENT FOR SAM-PLE EFFICIENT REINFORCEMENT LEARNING?</strong> Du, S. S., Kakade, S. M., Wang, R., &amp; Yang, L. F. (2019). [<a href="https://arxiv.org/abs/1910.03016" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>现代深度学习方法提供了学习良好表达的有效手段。 但是，良好的表示形式本身是否足以进行样本有效的强化学习？ 在更经典的近似动态规划文献中，仅针对（最坏情况）近似误差研究了该问题。 关于统计观点，这个问题在很大程度上尚待探讨，并且现有文献主要集中在允许样本进行有效强化学习而几乎不了解有效强化学习的必要条件的条件。 这项工作表明，从统计学的角度来看，这种情况比传统的近似观点所暗示的要微妙得多，在传统的近似观点中，对满足样本有效RL的表示的要求更加严格。 我们的主要结果为强化学习方法提供了清晰的门槛，表明在构成良好的函数逼近（就表示的维数而言）方面存在严格的限制，我们专注于与基于价值，基于模型的自然表示条件 ，以及基于政策的学习。 这些下限突显出，除非其近似值的质量超过某些严格的阈值，否则本身具有良好的（基于值，基于模型或基于策略的）表示形式不足以进行有效的强化学习。 此外，我们的下限还意味着样本复杂性之间的指数分离：1）具有完美表示形式的基于价值的学习与具有良好但不是完美表示形式的基于价值的学习； 2）基于价值的学习和基于策略的学习 ，3）基于策略的学习和监督学习，以及4）强化学习和模仿学习。</li></ul></li><li><p><strong>Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees.</strong> Chen, B., Dai, B., Lin, Q., Ye, G., Liu, H., &amp; Song, L. (2019). [<a href="http://arxiv.org/abs/1903.00070" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>我们提出一种名为神经探索-开发树（NEXT）的元路径规划算法，以从先前的经验中学习，以解决高维连续状态和动作空间中的新路径规划问题。 与更经典的基于采样的方法（如RRT）相比，我们的方法在高维度上可获得更高的采样效率，并且可以从类似环境中的规划经验中受益。 更具体地说，NEXT利用一种新颖的神经体系结构，可以从问题结构中学习有前途的搜索方向。 然后，将学习到的先验知识整合到UCB类型的算法中，以在解决新问题时实现探索与开发之间的在线平衡。 我们进行了彻底的实验，以表明NEXT通过更紧凑的搜索树解决了新的计划问题，并在某些基准上明显优于最新方法。</li></ul></li><li><p><strong>Making Sense of Reinforcement Learning and Probabilistic Inference.</strong> O’Donoghue, B., Osband, I., &amp; Ionescu, C. (2020). [<a href="http://arxiv.org/abs/2001.00805" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>强化学习（RL）将控制问题与统计估计结合在一起：代理不知道系统动态，但可以通过经验来学习。 最近的研究将“ RL作为推论”，并提出了一个特殊的框架将RL问题概括为概率推论。 我们的论文揭示了该方法的主要缺点，并阐明了将RL连贯地转换为推理问题的意义。 特别是，RL代理商必须考虑其行为对未来回报和观察的影响：勘探与开发的权衡。 在除最简单的设置之外的所有条件下，得出的推论在计算上都是棘手的，因此实际的RL算法必须重新近似。 我们证明了流行的“ RL作为推论”近似方法即使在非常基本的问题中也可能表现不佳。 但是，我们表明，只需稍加修改，该框架就可以产生可证明具有良好性能的算法，并且我们表明，所得算法等同于最近提出的K学习，我们还将其与Thompson采样结合在一起。</li></ul></li><li><p><strong>IMPROVING GENERALIZATION IN META REINFORCE-MENT LEARNING USING LEARNED OBJECTIVES.</strong> Kirsch, L., Van Steenkiste, S., &amp; Urgen Schmidhuber, J. ¨. (2019). [<a href="http://louiskirsch.com/code/metagenrl" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>生物进化将许多学习者的经验提炼为人类的通用学习算法。 我们新颖的元强化学习算法MetaGenRL受此过程启发。  MetaGenRL提取了许多复杂代理的经验，以元学习一种低复杂度的神经目标功能，该功能决定了未来个人将如何学习。 与最近的元RL算法不同，MetaGenRL可以推广到与元训练完全不同的新环境。 在某些情况下，它甚至优于人工设计的RL算法。  MetaGenRL在元训练期间使用非策略性二阶梯度，可大大提高其采样效率。</li></ul></li><li><p><strong>Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning.</strong> Scobee, D. R. R., &amp; Sastry, S. S. (2019, September 25). [<a href="https://arxiv.org/abs/1909.05477" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>尽管逆向强化学习（IRL）问题的大多数方法都集中在估计可以最好地解释专家代理人的政策或在控制任务上表现出的行为的奖励功能，但通常情况下，这种行为更简单地由简单奖励来表示 结合一系列严格的约束。 在这种情况下，座席正试图根据这些给定的行为约束来最大化累积奖励。 我们对马尔可夫决策过程（MDP）上的IRL问题进行了重新表述，以便在给定环境的名义模型和名义奖励函数的情况下，我们寻求估计环境，行为和特征约束条件，以激发代理的行为。 我们的方法基于最大熵IRL框架，这使我们能够根据我们对MDP的了解来推断专家代理进行演示的可能性。 使用我们的方法，我们可以推断可以将哪些约束添加到MDP，以最大程度地增加观察这些演示的可能性。 我们提出了一种算法，该算法可迭代地推断最大似然约束以最好地解释观察到的行为，并且我们将使用模拟行为和在障碍物周围航行的人类的记录数据来评估其功效。</li></ul></li><li><p><strong>THE INGREDIENTS OF REAL-WORLD ROBOTIC REINFORCEMENT LEARNING.</strong> Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., … Levine, S. (2019). [<a href="https://openreview.net/forum?id=rJe2syrtvS" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>在现实世界中，强化学习的成功仅限于仪器化的实验室场景，通常需要艰苦的人工和监督才能实现持续学习。 在这项工作中，我们讨论了机器人学习系统所需的要素，该系统可以不断地，自主地改善现实世界中收集的数据。 我们使用灵巧操作作为案例研究，提出了这样一个系统的特定实例。 随后，我们研究了在没有仪器的情况下学习时会遇到的许多挑战。 在这种情况下，学习必须是可行的，而无需人工设计的复位，仅使用板载感知器并且没有手工设计的奖励功能。 我们提出了针对这些挑战的简单且可扩展的解决方案，然后证明了我们提出的系统在一组灵巧的机器人操纵任务上的功效，从而提供了与该学习范式相关的挑战的深入分析。 我们证明，我们的完整系统可以在没有任何人工干预的情况下进行学习，并通过真实的三指手获得各种基于视觉的技能。</li></ul></li><li><p><strong>Measuring the Reliability of Reinforcement Learning Algorithms.</strong> Chan, S. C. Y., Fishman, S., Canny, J., Korattikara, A., &amp; Guadarrama, S. (2019). [<a href="http://arxiv.org/abs/1912.05663" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>缺乏可靠性是强化学习（RL）算法的一个众所周知的问题。 近年来，这个问题已引起越来越多的关注，并且为改善它而进行的努力已大大增加。 为了帮助RL研究人员和生产用户评估和提高可靠性，我们提出了一套可定量测量可靠性各个方面的指标。 在这项工作中，我们专注于培训期间和学习后（根据固定政策）的变异性和风险。 我们将这些指标设计为通用的，还设计了补充统计测试以对这些指标进行严格的比较。 在本文中，我们首先描述度量标准及其设计的期望属性，度量标准的可靠性方面以及它们在不同情况下的适用性。 然后，我们描述统计测试并为报告结果提出其他实用建议。 度量标准和随附的统计工具已作为开放源代码库提供。1我们将度量标准应用于一组通用RL算法和环境，进行比较并分析结果。</li></ul></li><li><p><strong>DISAGREEMENT-REGULARIZED IMITATION LEARNING.</strong> Brantley, K., Sun, W., &amp; Henaff, M. (2019). [<a href="https://openreview.net/forum?id=rkgbYyHtwB" target="_blank" rel="noopener">原文链接</a>]</p><ul><li>我们提出了一种简单有效的算法，旨在解决模仿学习中的协变量偏移问题。 它通过在专家演示数据上训练一组策略，然后将其预测的差异用作成本（通过RL与监督的行为克隆成本最小化）来进行操作。 与对抗式模仿方法不同，它使用易于优化的固定奖励函数。 我们证明了该算法的遗憾界限，该算法在时间范围内是线性的，乘以一个系数，对于行为克隆失败的某些问题，该系数显示为低。 我们在多个基于像素的Atari环境和连续控制任务上对算法进行了经验评估，结果表明该算法与行为克隆和生成的对抗模仿学习相匹配或明显胜过。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;*标注的为值得精读论文&lt;/p&gt;
&lt;h2 id=&quot;ICLR2020&quot;&gt;&lt;a href=&quot;#ICLR2020&quot; class=&quot;headerlink&quot; title=&quot;ICLR2020&quot;&gt;&lt;/a&gt;ICLR2020&lt;/h2&gt;&lt;h3 id=&quot;Oral&quot;&gt;&lt;a href=&quot;#Oral&quot; 
      
    
    </summary>
    
    
      <category term="Reading List" scheme="http://yoursite.com/categories/Reading-List/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://yoursite.com/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives</title>
    <link href="http://yoursite.com/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/"/>
    <id>http://yoursite.com/2020/02/17/Reinforcement-Learning-with-Competitive-Ensembles-of-Information-Constrained-Primitives/</id>
    <published>2020-02-17T03:25:52.000Z</published>
    <updated>2020-03-03T05:51:51.885Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>在各种复杂环境中运行的强化学习代理可以从其行为的结构分解中受益。通常，这是在分层强化学习的语境下解决的，往往目标是将策略分解为较低级别的原语或选项，同时较高级别的元策略针对给定情况触发适当的行为。但是，元策略仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原语，类似于分层强化学习，但没有高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用信息理论机制来实现此分散决策：每个原语都会选择需要多少有关当前状态的信息以做出决定，请求有关当前状态最多信息的原语被选择与环境交互。对原语进行规范化以使用尽可能少的信息，从而导致自然竞争和特异化。我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/AcroRd32_CCYGAesAxx.png" alt title>                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><p>尽管分层强化学习通过分解可以使得底层原语可以特异化面对不同状态，并能够被元策略加以组合或选择。然而元策略本身仍然需要处理所有状态空间，从而使得元策略在不同环境中的迁移成为瓶颈。</p><p>该方法包括三个组件：</p><ol><li>一种将特定原语限制在状态空间子集的机制</li><li>原语之间的竞争机制，从而为给定状态选择最有效的原语</li><li>规范化机制，可提高策略整体的泛化性能。</li></ol><h4 id="包含信息瓶颈的原语"><a href="#包含信息瓶颈的原语" class="headerlink" title="包含信息瓶颈的原语"></a>包含信息瓶颈的原语</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/AcroRd32_qhtDGoA5UR.png" alt title>                </div>                <div class="image-caption"></div>            </figure><script type="math/tex; mode=display">\pi_{\theta}^{k}(A | S)=\int_{z} p_{\mathrm{enc}}\left(z_{k} | S\right) p_{\mathrm{dec}}\left(A | z_{k}\right) \mathrm{d} z_{k}</script><p>这里作者对每个原语设计一个信息瓶颈，以防止其利用状态中的全部信息（这里我认为是在迫使原语提取主要信息，以增强泛化性）</p><script type="math/tex; mode=display">\mathcal{L}_{k}=\mathrm{D}_{\mathrm{KL}}\left(p_{\mathrm{enc}}\left(Z_{k} | S\right) \| \mathcal{N}(0,1)\right)</script><p>通过Z（隐变量状态空间）的分布与正态分布之间的KL散度作为罚项以期另Z中包含尽可能少的关于S的信息，文中的表述是原语需要通过对$L_k$支付“信息损失”已获得关于当前状态的更多信息。<br>这里的正态分布可被替换为预先学习的先验。</p><p>尽管信息瓶颈使得原语能获得的关于状态的信息尽可能少，然而并没有约束不同原语去关注状态空间中的不同部分，所以进一步提出了竞争机制以鼓励原语的多样性。</p><h4 id="信息约束的竞争原语"><a href="#信息约束的竞争原语" class="headerlink" title="信息约束的竞争原语"></a>信息约束的竞争原语</h4><p>由于原语面对状态s时的$L_k$体现了其对于当前状态的有效性，所以最高$L_k$的原语应当被激活。这里采用softmax计算归一化权重的方法</p><script type="math/tex; mode=display">\alpha_{k}=\exp \left(\mathcal{L}_{k}\right) / \sum_{j} \exp \left(\mathcal{L}_{j}\right)</script><p>选择$\alpha_{k}$最高的或通过构造分布然后采样的方式，决定要激活的原语。</p><p>通过$r_{k}=\alpha_{k} r,$ 其中 $r=\sum_{k} r_{k}$的方式鼓励每个原语从状态中获取更多信息，与信息瓶颈相拮抗，使得每个原语特异化。</p><h4 id="组合表示的正则化"><a href="#组合表示的正则化" class="headerlink" title="组合表示的正则化"></a>组合表示的正则化</h4><p>采用如下的额外正则化项，鼓励多样化的原语设置，以及保证模型不会坍缩到单一原语</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{reg}}=\sum_{k} \alpha_{k} \mathcal{L}_{k}</script><p>可以重写为</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{reg}}=-H(\alpha)+\operatorname{LSE}\left(\mathcal{L}_{1}, \ldots, \mathcal{L}_{K}\right)</script><p>前项使得$\alpha$熵值增加，进而使得原语选择集合具有多样性。LSE则近似于参数的最大项，$\operatorname{LSE}(x) \approx \max _{j} x_{j}$，因此惩罚了占主导地位的$L_k$项，使他们趋向一致</p><h4 id="目标与算法总结"><a href="#目标与算法总结" class="headerlink" title="目标与算法总结"></a>目标与算法总结</h4><p>总目标函数包含3项：</p><ol><li>来自标准RL目标的期望奖励，$R(\pi)$，根据参与度将其分配给不同原语</li><li>单个瓶颈项导致单个原语专注于状态空间的特定部分，$L_k$，其中$k = 1,\dots,K$</li><li>归一化项应用于组合模型，$L_{reg}$</li></ol><p>对于第k个原语的总目标函数如下：</p><script type="math/tex; mode=display">J_{k}(\theta) \equiv \mathbb{E}_{\pi_{\theta}}\left[r_{k}\right]-\beta_{\mathrm{ind}} \mathcal{L}_{k}-\beta_{\mathrm{reg}} \mathcal{L}_{\mathrm{reg}}</script><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/AcroRd32_5OImexFX2U.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>如图，每个面板对应于不同的训练设置，其中不同的任务表示为A，B，C，…，具有n个圆圈的矩形对应于由在相应任务上训练的n个原语组成的代理。 第一行：为受过单个任务训练的代理原语的激活。 下面一行：重新训练：在A上训练了两个原语并迁移到B。结果（成功率）表明，多原语模型比baseline（transfer A2C）实质上具有更高的样本效率。 复制和合并：随着时间的推移，更多的原语以即插即用的方式添加到模型中（在A上训练了2个原语；模型通过自己的副本扩展自身；在B上训练了所得的四原语模型。）对比比其他强baseline更有效。零样本泛化：在C上训练一组原语，并评估对A和B的零样本泛化。这些原语学习一种空间分解的形式，这使它们可以在目标任务A和B中处于活动状态。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在各种复杂环境中运行的强化学习代理可以从其行为的结构分解中受益。通常，这是在分层强化学习的语境下解决的，往往目标是将策略分解为较低级别的原语或选项，同时较高级别的元策略针对给定情况触发适当的行为。但是，元策略仍必须在所有状态中做出适当的决定。在这项工作中，我们提出了一种策略设计，该策略设计可分解为原语，类似于分层强化学习，但没有高级元策略。相反，每个原语可以自己决定是否希望在当前状态下执行操作。我们使用信息理论机制来实现此分散决策：每个原语都会选择需要多少有关当前状态的信息以做出决定，请求有关当前状态最多信息的原语被选择与环境交互。对原语进行规范化以使用尽可能少的信息，从而导致自然竞争和特异化。我们通过实验证明，该策略体系结构在泛化方面比平面策略和分层策略都有所改进。&lt;/p&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/AcroRd32_CCYGAesAxx.png&quot; alt title&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://yoursite.com/tags/Reinforcement-Learning/"/>
    
      <category term="Skill Learning" scheme="http://yoursite.com/tags/Skill-Learning/"/>
    
  </entry>
  
  <entry>
    <title>NAS Reading List</title>
    <link href="http://yoursite.com/2019/12/05/NAS-Reading-List/"/>
    <id>http://yoursite.com/2019/12/05/NAS-Reading-List/</id>
    <published>2019-12-05T07:31:04.000Z</published>
    <updated>2020-03-10T06:17:49.377Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NAS"><a href="#NAS" class="headerlink" title="NAS"></a>NAS</h2><ul><li>[x] （ICLR2017, google brain）Neural architecture search with reinforcement learning</li><li>[ ] （2019.11）Meta-Learning of Neural Architectures for Few-Shot Learning，meta与NAS的结合：<a href="https://arxiv.org/abs/1911.11090v1" target="_blank" rel="noopener">https://arxiv.org/abs/1911.11090v1</a></li><li><p>[x] （2019.01）Designing neural networks through neuroevolution，NE方法综述</p></li><li><p>（2017.09）Evolution Strategies as a Scalable Alternative to Reinforcement Learning <a href="https://openai.com/blog/evolution-strategies/" target="_blank" rel="noopener">https://openai.com/blog/evolution-strategies/</a>， <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="noopener">https://arxiv.org/abs/1703.03864</a>：NES方法与DQN、A3C相匹敌（但未完全脱离梯度）</p></li><li><p>（2018.04）Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning <a href="https://arxiv.org/abs/1712.06567" target="_blank" rel="noopener">https://arxiv.org/abs/1712.06567</a>：gradient-free的NE方法与DQN、A3C相匹敌</p></li><li><p>（2018.04）Simple random search provides a competitive approach to reinforcement learning <a href="https://arxiv.org/abs/1803.07055" target="_blank" rel="noopener">https://arxiv.org/abs/1803.07055</a>：简化NE方法（RS方法）与RPO、PPO、DDPG相匹敌</p></li></ul><h3 id="结合基于梯度的方法和神经进化"><a href="#结合基于梯度的方法和神经进化" class="headerlink" title="结合基于梯度的方法和神经进化"></a>结合基于梯度的方法和神经进化</h3><ul><li><p>（2018.05）Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients <a href="https://arxiv.org/abs/1712.06563" target="_blank" rel="noopener">https://arxiv.org/abs/1712.06563</a>：保存状态与动作之间的关系库</p></li><li><p>（ICLR 2018.05）Policy Optimization by Genetic Distillation <a href="https://arxiv.org/abs/1711.01012" target="_blank" rel="noopener">https://arxiv.org/abs/1711.01012</a>：Genetic policy optimization</p></li><li><p>（ICLR 2018）Noisy Networks for Exploration <a href="https://arxiv.org/abs/1706.10295" target="_blank" rel="noopener">https://arxiv.org/abs/1706.10295</a></p></li><li><p>（ICLR 2018）Parameter space noise for exploration <a href="https://arxiv.org/abs/1706.01905" target="_blank" rel="noopener">https://arxiv.org/abs/1706.01905</a></p></li></ul><h3 id="新一代进化算法"><a href="#新一代进化算法" class="headerlink" title="新一代进化算法"></a>新一代进化算法</h3><ul><li><p>The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities <a href="https://arxiv.org/abs/1803.03453" target="_blank" rel="noopener">https://arxiv.org/abs/1803.03453</a> ：综述</p></li><li><p>（NIPS 2018）Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents <a href="https://arxiv.org/abs/1712.06560" target="_blank" rel="noopener">https://arxiv.org/abs/1712.06560</a></p></li><li><p>（NIPS workshop 2018）Deep Curiosity Search: Intra-Life Exploration Can Improve Performance on Challenging Deep Reinforcement Learning Problems <a href="https://arxiv.org/abs/1806.00553" target="_blank" rel="noopener">https://arxiv.org/abs/1806.00553</a></p></li></ul><h3 id="架构进化"><a href="#架构进化" class="headerlink" title="架构进化"></a>架构进化</h3><ul><li><p>From Nodes to Networks: Evolving Recurrent Neural Networks <a href="https://arxiv.org/abs/1803.04439" target="_blank" rel="noopener">https://arxiv.org/abs/1803.04439</a></p></li><li><p>（AAAI 2019）Regularized Evolution for Image Classifier Architecture Search <a href="https://arxiv.org/abs/1802.01548" target="_blank" rel="noopener">https://arxiv.org/abs/1802.01548</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;NAS&quot;&gt;&lt;a href=&quot;#NAS&quot; class=&quot;headerlink&quot; title=&quot;NAS&quot;&gt;&lt;/a&gt;NAS&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;[x] （ICLR2017, google brain）Neural architecture search with 
      
    
    </summary>
    
    
      <category term="Reading List" scheme="http://yoursite.com/categories/Reading-List/"/>
    
    
      <category term="NAS" scheme="http://yoursite.com/tags/NAS/"/>
    
  </entry>
  
  <entry>
    <title>Neural Architecture Search with Reinforcement Learning</title>
    <link href="http://yoursite.com/2019/12/03/Neural-Architecture-Search-with-Reinforcement-Learning/"/>
    <id>http://yoursite.com/2019/12/03/Neural-Architecture-Search-with-Reinforcement-Learning/</id>
    <published>2019-12-03T02:26:34.000Z</published>
    <updated>2020-03-10T06:15:22.489Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>神经网络是一种功能强大、灵活的模型，在图像、语音和自然语言理解等许多困难的学习任务中起着很好的作用。尽管取得了成功，神经网络仍然很难设计。在本文中，我们使用递归网络来生成神经网络的模型描述，并利用强化学习来训练该RNN，最大化所生成的架构在验证集上的期望精度。在CIFAR-10数据集上，我们的方法可以从头开始，设计一种新的网络体系结构，在测试集精度方面可以与人类发明的最佳体系结构相媲美。我们的CIFAR-10模型的测试错误率为3.65，比以前使用类似架构方案的最新模型高0.09%，快1.05倍。在Penn Treebank数据集上，我们的模型可以组成一个新的递归单元，其性能优于广泛使用的LSTM单元和其他SOTA baseline。我们的单元在Penn Treebank上达到了测试集62.4的困惑度，这比之前的SOTA模型在困惑度上好3.6。该单元还可以转移到PTB上的字符语言建模任务，并达到SOTA的1.214困惑度。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>在这一节中，我们将首先描述一种使用递归网络生成卷积结构的简单方法。我们将展示如何使用策略梯度方法来训练递归网络，以最大化采样架构的精度的期望。我们将提出几个基于我们核心方法的改进，如形成skip连接，以增加模型的复杂性，以及使用参数服务器方法来加快训练。这一节的最后，我们将着重于生成递归架构，这是本文的另一个重要贡献。</p><h4 id="Generate-Model-Descriptions-with-a-Controller-Recurrent-Neural-Network"><a href="#Generate-Model-Descriptions-with-a-Controller-Recurrent-Neural-Network" class="headerlink" title="Generate Model Descriptions with a Controller Recurrent Neural Network"></a>Generate Model Descriptions with a Controller Recurrent Neural Network</h4><p>在神经网络架构搜索（NAS）中，我们使用控制器生成神经网络的结构超参数。为了灵活，控制器被实现为RNN。假设我们想要预测只有卷积层的前馈神经网络，我们可以使用控制器将它们的超参数作为一系列标记生成：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2019-12-03_Zoph%2C_Le_-_2016_-_Neural_Architecture_Search_with__17.png" alt="图2：我们的控制器递归神经网络如何采样一个简单的卷积网络。它预测filter高度、filter宽度、stride高度、stride宽度、一层的filters数量并重复。每个预测都由softmax分类器进行，然后作为输入输入到下一个时间步。" title>                </div>                <div class="image-caption">图2：我们的控制器递归神经网络如何采样一个简单的卷积网络。它预测filter高度、filter宽度、stride高度、stride宽度、一层的filters数量并重复。每个预测都由softmax分类器进行，然后作为输入输入到下一个时间步。</div>            </figure><p>在我们的实验中，如果层的数量超过某个值，生成架构的过程就会停止。这个值遵循一个时间表，我们在训练过程中增加它。一旦控制器RNN生成了一个架构，就用这个架构建立并训练一个神经网络。收敛时，将记录持有的验证集上网络的准确性。控制器RNN的参数，$\theta_c$，接着被优化以最大化提出的架构在验证集上预测精度的期望。在下一节中，我们将描述一种策略梯度方法，用于更新参数$\theta_c$，以便控制器RNN随着时间的推移生成更好的架构。</p><h4 id="Training-with-REINFORCE"><a href="#Training-with-REINFORCE" class="headerlink" title="Training with REINFORCE"></a>Training with REINFORCE</h4><p>控制器预测的标记列表可以看作是设计子网络架构的action的列表$a_{1:T}$。在收敛时，该子网络将在持有的数据集上达到精度$R$。我们可以用这个精度$R$作为奖励信号，用强化学习来训练控制器。更具体地说，为了找到最佳的架构，我们要求我们的控制器最大化其预期报酬，以$J(\theta_c)$表示：</p><script type="math/tex; mode=display">J\left(\theta_{c}\right)=E_{P\left(a_{1: T} ; \theta_{c}\right)}[R]</script><p>由于奖励信号$R$是不可微的，我们需要使用策略梯度方法迭代更新$\theta_c$。在这项工作中，我们使用<a href="https://link.springer.com/article/10.1007/BF00992696" target="_blank" rel="noopener">Williams（1992）</a>的REINFORCE规则：</p><script type="math/tex; mode=display">\nabla \theta_{c} J\left(\theta_{c}\right)=\sum_{t=1}^{T} E_{P\left(a_{1: T} ; \theta_{c}\right)}\left[\nabla \theta_{c} \log P\left(a_{t} | a_{(t-1): 1} ; \theta_{c}\right) R\right]</script><p><strong>Accelerate Training with Parallelism and Asynchronous Updates:</strong></p><h4 id="Increase-Architecture-Complexity-with-Skip-Connections-and-Other-Layer-Types"><a href="#Increase-Architecture-Complexity-with-Skip-Connections-and-Other-Layer-Types" class="headerlink" title="Increase Architecture Complexity with Skip Connections and Other Layer Types"></a>Increase Architecture Complexity with Skip Connections and Other Layer Types</h4><h4 id="Generate-Recurrent-Cell-Architectures"><a href="#Generate-Recurrent-Cell-Architectures" class="headerlink" title="Generate Recurrent Cell Architectures"></a>Generate Recurrent Cell Architectures</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;神经网络是一种功能强大、灵活的模型，在图像、语音和自然语言理解等许多困难的学习任务中起着很好的作用。尽
      
    
    </summary>
    
    
      <category term="论文翻译" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="NAS" scheme="http://yoursite.com/tags/NAS/"/>
    
      <category term="Google Brain" scheme="http://yoursite.com/tags/Google-Brain/"/>
    
      <category term="Reinforcement Learning" scheme="http://yoursite.com/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning</title>
    <link href="http://yoursite.com/2019/11/01/Meta-World-A-Benchmark-and-Evaluation-for-Multi-Task-and-Meta-Reinforcement-Learning/"/>
    <id>http://yoursite.com/2019/11/01/Meta-World-A-Benchmark-and-Evaluation-for-Multi-Task-and-Meta-Reinforcement-Learning/</id>
    <published>2019-11-01T07:24:27.000Z</published>
    <updated>2019-12-03T11:27:23.450Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>元强化学习算法可以通过利用先前的经验来学习如何学习，从而使机器人更快地掌握新技能。但是，当前有关元强化学习的许多研究都集中在非常狭窄的任务分布上。例如，一个常用的元强化学习基准将模拟机器人的不同的运行速度作为不同的任务。当在这样狭窄的任务分布上进行策略的元训练时，它们可能无法泛化到更快地获取全新的任务。因此，如果这些方法的目的是能够更快地获取全新的行为，则我们必须在足够广泛的任务分布上评估它们，以使其能够推广到新的行为。在本文中，我们提出了一种用于元强化学习和多任务学习的开源模拟benchmark，该benchmark包含50个不同的机器人操纵任务。我们的目标是使开发用于加速获取全新的、可执行的任务的算法成为可能。我们针对这些任务评估了6种最新的元强化学习和多任务学习算法。令人惊讶的是，尽管每项任务及其变体（例如，不同的对象位置）都可以合理地成功学习，但是这些算法难以同时学习多个任务，即使只有十个不同的训练任务也是如此。我们的分析和开源环境为将来的多任务学习和元学习研究铺平了道路，这些研究可以实现有意义的泛化，从而释放这些方法的全部潜力。</p><p>benchmark任务的视频在项目页面上：<a href="https://meta-world.github.io" target="_blank" rel="noopener">meta-world.github.io</a>。我们的开源代码可在以下网址获得：<a href="https://github.com/rlworkgroup/metaworld" target="_blank" rel="noopener">https://github.com/rlworkgroup/metaworld</a></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;元强化学习算法可以通过利用先前的经验来学习如何学习，从而使机器人更快地掌握新技能。但是，当前有关元强化学习的许多研究都集中在非常狭窄的任务分布上。例如，一个常用的元强化学习基准将模拟机器人的不同的运行速度作为不同的任务。当在这样狭窄的任务分布上进行策略的元训练时，它们可能无法泛化到更快地获取全新的任务。因此，如果这些方法的目的是能够更快地获取全新的行为，则我们必须在足够广泛的任务分布上评估它们，以使其能够推广到新的行为。在本文中，我们提出了一种用于元强化学习和多任务学习的开源模拟benchmark，该benchmark包含50个不同的机器人操纵任务。我们的目标是使开发用于加速获取全新的、可执行的任务的算法成为可能。我们针对这些任务评估了6种最新的元强化学习和多任务学习算法。令人惊讶的是，尽管每项任务及其变体（例如，不同的对象位置）都可以合理地成功学习，但是这些算法难以同时学习多个任务，即使只有十个不同的训练任务也是如此。我们的分析和开源环境为将来的多任务学习和元学习研究铺平了道路，这些研究可以实现有意义的泛化，从而释放这些方法的全部潜力。&lt;/p&gt;
&lt;p&gt;benchmark任务的视频在项目页面上：&lt;a href=&quot;https://meta-world.github.io&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;meta-world.github.io&lt;/a&gt;。我们的开源代码可在以下网址获得：&lt;a href=&quot;https://github.com/rlworkgroup/metaworld&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/rlworkgroup/metaworld&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文翻译" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="meta-learning" scheme="http://yoursite.com/tags/meta-learning/"/>
    
      <category term="benchmark" scheme="http://yoursite.com/tags/benchmark/"/>
    
  </entry>
  
  <entry>
    <title>iMAML笔记（翻译）[更新中]</title>
    <link href="http://yoursite.com/2019/10/18/iMAML%E7%AC%94%E8%AE%B0%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89/"/>
    <id>http://yoursite.com/2019/10/18/iMAML笔记（翻译）/</id>
    <published>2019-10-18T07:45:27.000Z</published>
    <updated>2019-12-03T02:29:50.554Z</updated>
    
    <content type="html"><![CDATA[<p><strong>作者</strong>： <a href="https://www.inference.vc/" target="_blank" rel="noopener">inFERENCe</a></p><p>本周，我阅读了这份关于元学习的新文章：基于一些关于正则化优化最优结果的微分的观察，与前一版本相比，它的方法略有不同。</p><ul><li>Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine (2019) <a href="https://arxiv.org/abs/1909.04630" target="_blank" rel="noopener">Meta-Learning with Implicit Gradients</a></li></ul><p>同时发表的另一篇论文也发现了类似的技术，所以我认为我会更新该帖子并提及它，尽管我不会详细介绍它，并且该帖子主要是关于 Rajeswaran et al (2019) 的</p><ul><li>Yutian Chen, Abram L. Friesen, Feryal Behbahani, David Budden, Matthew W. Hoffman, Arnaud Doucet, Nando de Freitas (2019) <a href="https://arxiv.org/abs/1909.05557" target="_blank" rel="noopener">Modular Meta-Learning with Shrinkage</a></li></ul><h4 id="大纲："><a href="#大纲：" class="headerlink" title="大纲："></a>大纲：</h4><ul><li>我将对元学习设置进行高层概述，我们的目标是学习一种良好的SGD初始化或正则化策略，从而使SGD收敛到更好地完成一系列任务的最小值。</li><li>我将说明iMAML如何在1D玩具示例中工作，并讨论元目标的行为和属性。</li><li>然后，我将讨论iMAML的局限性：它仅考虑最小值的位置，而不考虑随机算法最终达到特定最小值的可能性。</li><li>最后，我将把iMAML与元学习的一种变体方法联系起来。</li></ul><h3 id="元学习与MAML"><a href="#元学习与MAML" class="headerlink" title="元学习与MAML"></a>元学习与MAML</h3><p>元学习有几种可能的表述方式，我将尝试按照我自己的解释和表示来解释这篇文章的设置，这与这篇文章有所不同，但使我的解释更加清楚（希望会）。</p><p>在元学习中，我们有一系列独立的任务，分别具有关联的训练和验证损失函数$f_i$和$g_i$。我们有一组在任务之间共享的模型参数$\theta$，损失函数$f_i(\theta)$和$g_i(\theta)$评估具有参数$\theta$的模型在任务$i$的训练和测试案例中的表现如何。我们有一个算法可以访问训练损失$f_i$和一些元参数$\theta_0$，并输出一些最优或学习的参数$\theta^\ast_i = Alg(f_i,\theta_0)$。元学习算法的目标是关于元参数$\theta_0$优化元目标</p><script type="math/tex; mode=display">\mathcal{M}(\theta_0)=\sum_i g_i(Alg(f_{i}, \theta_{0}))</script><p>在这项工作的早期版本MAML中，该算法被选择为随机梯度下降算法，$f_i$和$g_i$是神经网络的训练和测试损失。元参数$\theta_0$是SGD算法的初始化点，在所有任务之间共享。由于SGD更新是可微的，因此可以通过简单地穿过SGD更新步反向传播来计算相对于初始值$\theta_0$的元目标的梯度。这基本上就是MAML所做的。</p><p>但是，初始化对$\theta$最终值的影响非常微弱，并且很难进行分析表征（如果有可能的话）。如果我们允许SGD继续执行许多更新步，则可能会收敛到一个更好的参数，但是轨迹将非常长，并且相对于初始值的梯度将消失。如果我们使轨迹足够短，则关于$\theta_{0}$的梯度是有用的，但我们可能无法达到很好的最终值。</p><h3 id="iMAML"><a href="#iMAML" class="headerlink" title="iMAML"></a>iMAML</h3><p>这就是为什么Rajeswaran等人选择使轨迹的终点对元参数$\theta_0$的依赖性更强的原因：除了简单地从$\theta_0$初始化SGD之外，他们还通过在loss中添加二次正则项$|\theta−\theta_0|$来固定参数以使其停留在$\theta_0$附近。因此，发生了两件事：</p><ul><li>现在，SGD的所有步骤都取决于$\theta$，而不仅仅是起始点</li><li>现在最小化SGD最终收敛到的位置也取决于$\theta_0$</li></ul><p>iMAML正是利用了这第二个属性。让我说明一下这种依赖性是什么样的：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/20191018165008.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>在上图中，假设我们要最小化目标函数$f(\theta)$。这将是元学习算法必须解决的任务之一的训练损失。我们当前的元参数$\theta_0$标记在x轴上，橙色曲线显示了相关的二次惩罚。蓝绿色曲线显示了加上惩罚项的目标。红星表示最小值的位置，这是学习算法发现的位置。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/Notes%20on%20iMAML_%20Meta-Learning%20with%20Implicit%20Gradients.gif" alt title>                </div>                <div class="image-caption"></div>            </figure><p>现在，让此动画动起来。我将移动锚点$\theta_0$，并重现相同的图。您会看到，随着我们移动$\theta_0$和相应的惩罚项，正则化目标移动的局部（同时也是全局）最小值发生了变化：</p><p>因此很明显，锚点$\theta_0$与局部最小值$\theta^\ast$的位置之间存在非平凡非线性的关系。让我们根据锚点绘制此关系：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/20191018165057.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>我们可以看到该函数一点也不好处理，当最接近$\theta_0$的局部最小值发生变化时，它具有急剧的跳变，并且在这些跳变之间相对平坦。实际上，你可以观察到最接近$\theta_0$的局部最小值越锐利，则$\theta_0$和$\theta$之间的关系越平坦。这是因为，如果$f$在$\theta_0$附近具有尖锐的局部最小值，则正则化最小值的位置将主要由$f$确定，并且锚点$\theta_0$的位置无关紧要。如果围绕$f$的局部最小值很宽，则最优值会有很大的摆动空间，并且正则化的效果会更大。</p><h3 id="Implicit-Gradients"><a href="#Implicit-Gradients" class="headerlink" title="Implicit Gradients"></a>Implicit Gradients</h3><p>现在，我们讨论iMAML程序的全部内容。实际上，该函数$\theta^\ast(\theta_0)$的梯度可以以封闭形式计算。实际上，它与$f$的曲率或二阶导数有关，在我们找到的最小值附近：</p><script type="math/tex; mode=display">\frac{d \theta^{\ast}}{d \theta_{0}}=\frac{1}{1+f^{\prime \prime}\left(\theta^{\ast}\right)}</script><p>为了检查此公式是否有效，我对导数进行了数值计算，并将其与理论预测的结果进行了比较，它们完全匹配：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/20191018165818.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>当参数空间是高维时，我们有一个类似的公式，其中包含Hessian的逆加单位阵。在高维中，Hessian的求逆甚至计算和存储都不太实际。iMAML论文的主要贡献之一是使用共轭梯度内部优化循环来逼近梯度的实用方法。有关详细信息，请阅读论文。</p><h4 id="Optimizing-the-meta-objective"><a href="#Optimizing-the-meta-objective" class="headerlink" title="Optimizing the meta-objective"></a>Optimizing the meta-objective</h4><p>在元学习设置中优化锚点时，我们感兴趣的不是位置$\theta^\ast$，而是函数$f$在此位置处取的值。（实际上，我们现在将使用验证损失来代替用于梯度下降的训练损失，但为简单起见，我假设这两个损失是重叠的）。$f$在其局部最优处的值绘制如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/20191018165958.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>噢，这个函数不是很漂亮。元目标$f(\theta^\ast(\theta_0))$变成分段连续函数，是相邻盆地之间的连接，边界不光滑。该函数的局部梯度包含很少的有关损失函数的整体结构的信息，它仅告诉你如何到达最接近的局部最小值的位置。所以我不会说这是最好的优化函数。</p><p>值得庆幸的是，该函数不是我们必须优化的。在元学习中，我们有优化的函数上的分布，因此实际的元目标类似于$\sum_i f_i(\theta^\ast_i(\theta_0))$。一堆丑陋的函数的总和很可能会变成平滑而优美的东西。另外，我在此博客文章中使用的一维函数不能代表我们要应用iMAML的神经网络的高维损失函数。以模式连通性的概念为例（参见例如<a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://arxiv.org/pdf/1802.10026.pdf" target="_blank" rel="noopener">Garipov et al, 2018</a>）：似乎SGD使用不同的随机种子发现的模式不仅仅只是孤立的盆地，而是通过训练和测试误差低的光滑的山谷相连。反过来，这可能会使元目标在最小值之间表现得更加平稳。</p><h3 id="What-is-missing-Stochasticity"><a href="#What-is-missing-Stochasticity" class="headerlink" title="What is missing? Stochasticity"></a>What is missing? Stochasticity</h3><p>MAML或iMAML不考虑的重要方面是我们通常使用随机优化算法的事实。SGD不会确定性地找到特定的最小值，而是采样不同的最小值：当使用不同的随机种子运行时，它将发现不同的最小值。</p><p>对元目标的更为慷慨的表述将允许使用随机算法。如果我们用$Alg(f_i,\theta_0)$表示算法发现的解的分布，则元目标将是</p><script type="math/tex; mode=display">\mathcal{M}_{s t o c h a s t i c}(\theta)=\sum_{i} \mathbb{E}_{\theta \sim \mathcal{Alg}\left(f_{i}, \theta_{0}\right)} g_{i}(\theta)</script><p>允许随机行为实际上可能对元学习而言是个很好的特性。正则化目标的全局最小值的位置会随$\theta_{0}$突然变化（如上面第三图所示），允许随机行为可能会使我们的元学习目标变得平滑。</p><p>现在假设锚定到$\theta_{0}$的SGD收敛到局部极小值的有限集合之一。那么元学习目标以两种不同方式受到$\theta_{0}$的影响：</p><ul><li>当我们更改锚点$\theta_{0}$时，最小值的位置也会发生变化，如上所述。这种变化是可微的，我们知道其导数。</li><li>当我们改变锚点$\theta_{0}$时，找到不同解的概率就会改变。有些解的发现频率更高，而有些则更少。</li></ul><p>iMAML考虑第一种影响，但它忽略了第二种机制的影响。这并不是说iMAML是错误的，而是它忽略了MAML或通过算法显式微分没有忽略的随机行为可能做出的关键贡献。</p><h3 id="Compare-with-a-Variational-Approach"><a href="#Compare-with-a-Variational-Approach" class="headerlink" title="Compare with a Variational Approach"></a>Compare with a Variational Approach</h3><p>当然，这项工作使我想起了贝叶斯方法。每当有人描述二次惩罚时，我所看到的就是高斯分布。</p><p>在iMAML的贝叶斯解释中，可以将锚点$\theta_{0}$视为神经网络权重上先验分布的均值。然后，在给定相关数据集的情况下，算法的内部循环或$Alg(f_i,\theta_{0})$会找到$\theta$上后验的最大后验（MAP）近似值。假设损失是某种形式的对数似然。问题是，如何更新元参数$\theta_{0}$？</p><p>在贝叶斯世界中，我们将寻求通过最大化边缘似然来优化$\theta_{0}$。由于这通常很棘手，因此通常需要变分近似，在这种情况下，它看起来像这样：</p><script type="math/tex; mode=display">\mathcal{M}_{\text {variational }}\left(\theta_{0}, Q_{i}\right)=\sum_{i}\left(K L\left[Q_{i} | \mathcal{N}_{\theta_{0}}\right]+\mathbb{E}_{\theta \sim Q_{i}} f_{i}(\theta)\right)</script><p>其中$Q_i$逼近任务$i$的模型参数的后验。$Q_i$的特定选择是狄拉克三角洲分布，其中心位于特定点$ Q_i(\theta)= \delta(\theta-\theta^{\ast}_i)$。如果我们慷慨地忽略某些常数会无限大地爆炸，那么高斯先验和简并点后验之间的KL散度就是一个简单的欧几里得距离，而我们的变分目标可简化为：</p><script type="math/tex; mode=display">\mathcal{M}_{\mathrm{variational}}\left(\theta_{0}, \theta_{i}\right)=\sum_{i}\left(\left\|\theta_{i}-\theta_{0}\right\|^{2}+f_{i}\left(\theta_{i}\right)\right)</script><p>现在，该目标函数非常类似于iMAML内循环试图解决的优化问题。如果我们在纯变分框架中工作，则可能是我们留下的东西，我们可以共同优化所有$\theta_i$和$\theta_{0}$。知道的人，请在下面发表评论，为我提供进行元学习的最佳参考。</p><p>使用无内环优化或黑魔法，该目标明显易于优化。它只是简单地将$\theta_{0}$拉到更接近为每个任务$i$找到的各种最优值的重心。不确定对于元学习来说这是否是一个好主意，因为通过从头开始从$\theta_{0}$进行SGD，我们可能无法通过共同优化所有目标而获得的$\theta_i$最终值。但是谁知道。鉴于上述观察，一个好主意可能是使$\theta_{0}$和$\theta_i$的变化目标共同最小化，但不时地将$\theta_{i}$重新初始化为$\theta_{0}$。但是在这一点上，我真的只是在编造东西…</p><p>无论如何，回到iMAML，它对这个变化目标做了一些有趣的事情，我认为可以将其理解为一种摊销计算：不是将$\theta_i$视为单独的辅助参数，而是指定了$\theta_i$实际上是$\theta_{0}$的确定性函数。。由于变分目标是任何$\theta_i$值的有效上限，因此，如果我们明确地使$\theta_i$取决于$\theta_{0}$，它也是有效的上限。因此，变分目标仅成为$\theta_{0}$的函数（以及算法$Alg$的超参数（如果有的话））：</p><script type="math/tex; mode=display">\mathcal{M}_{\text {variational }}\left(\theta_{0}\right)=\sum_{i}\left(\left\|Alg \left(f_{i}, \theta_{0}\right)-\theta_{0}\right\|^{2}+f_{i}\left(Alg\left(f_{i}, \theta_{0}\right)\right)\right)</script><p>我们终于得到它了。元学习$\theta_{0}$的变分目标与MAML / iMAML元目标非常相似，不同之处在于它还有$|Alg(f_i,\theta_{0})-\theta_{0}|^2$项，这是我们以前没有更新过的$\theta_{0}$的因素。还要注意，我没有使用单独的训练和验证损失$f_i$和$g_i$，但这也是一个非常合理的选择。</p><p>这样做的妙处在于，它为iMAML正在尝试做的事情提供了额外的理由和解释，并提出了可能改进iMAML的方向。另一方面，iMAML中的隐式区分技巧可能在其他情况下同样有用，即我们希望摊销后验后验。</p><p>我敢肯定我错过了很多参考资料，如果您认为我应该添加任何内容，特别是在变体位上，请在下面评论。</p><p><strong>原文链接</strong>：<a href="https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/" target="_blank" rel="noopener">Notes on iMAML: Meta-Learning with Implicit Gradients</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;： &lt;a href=&quot;https://www.inference.vc/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;inFERENCe&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本周，我阅读了这份关于元学习的新文章：基于一些关于正则化
      
    
    </summary>
    
    
      <category term="笔记翻译" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="meta-learning" scheme="http://yoursite.com/tags/meta-learning/"/>
    
      <category term="MAML" scheme="http://yoursite.com/tags/MAML/"/>
    
  </entry>
  
  <entry>
    <title>On First-Order Meta-Learning Algorithms</title>
    <link href="http://yoursite.com/2019/10/17/On-First-Order-Meta-Learning-Algorithms/"/>
    <id>http://yoursite.com/2019/10/17/On-First-Order-Meta-Learning-Algorithms/</id>
    <published>2019-10-17T02:58:41.000Z</published>
    <updated>2019-12-03T11:21:29.393Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>本文考虑了存在任务分布的元学习问题，并且我们希望获得一个从该分布中采样到以前没有见过的任务时表现良好（即快速学习）的agent。我们分析了一族用于学习参数初始化的算法，可以在新任务上进行快速微调，仅使用一阶导数进行元学习更新。该族包括并推广了一阶MAML，它是通过忽略二阶导数获得的MAML的近似值。它还包括Reptile，这是我们在此处引入的新算法，该算法通过重复采样任务，对其进行训练并将初始化朝着该任务的训练权重进行工作。我们扩展了Finn等人的结果。说明一阶元学习算法在一些公认的针对少数镜头分类的基准上表现良好，并且我们提供了旨在理解这些算法为何起作用的理论分析。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;本文考虑了存在任务分布的元学习问题，并且我们希望获得一个从该分布中采样到以前没有见过的任务时表现良好（
      
    
    </summary>
    
    
      <category term="论文翻译" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="meta-learning" scheme="http://yoursite.com/tags/meta-learning/"/>
    
      <category term="MAML" scheme="http://yoursite.com/tags/MAML/"/>
    
  </entry>
  
  <entry>
    <title>Meta-Learning: A Survey[更新中]</title>
    <link href="http://yoursite.com/2019/10/09/Meta-Learning-A-Survey/"/>
    <id>http://yoursite.com/2019/10/09/Meta-Learning-A-Survey/</id>
    <published>2019-10-09T12:37:57.000Z</published>
    <updated>2020-03-10T06:16:12.800Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>元学习或学会学习是系统地观察不同机器学习方法如何在广泛的学习任务中执行的科学，然后从这种经验或元数据中学习，以比其他方式更快地学习新任务 。<br>这不仅极大地加速和改进了机器学习流程或神经网络架构的设计，还使我们能够用数据驱动方式学习的新方法取代手工设计算法。<br>在本章中，我们将概述这个迷人且不断发展的领域的最新技术。</p><a id="more"></a><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><hr><p>当我们学习新技能时，我们很少 - 如果有的话 - 从头开始。我们从之前在相关任务中学到的技能开始，重用以前运作良好的方法，并根据经验关注可能值得尝试的内容（Lake et al。，2017）。通过学到的所有技能，学习新技能变得更容易，需要更少的示例和更少的试错。简而言之，我们跨任务学习如何学习。同样，在为特定任务构建机器学习模型时，我们通常会建立相关任务的经验，或者使用我们（通常是隐含的）对机器学习技术行为的理解来帮助做出正确的选择。</p><p>元学习的挑战是以系统的，数据驱动的方式从先前的经验中学习。首先，我们需要收集描述先前学习任务和先前学习模型的<strong>元数据</strong>。它们包括用于训练模型的精确<strong>算法配置</strong>，包括超参数设置，流程组合和/或网络架构，所得到的<strong>模型评估</strong>，例如准确性和训练时间，学到的模型参数，例如训练到的神经网络的权重，以及任务本身的可测量的适当关系，也称为<strong>元特征</strong>。接着，我们需要从这个先前的元数据开始<strong>学习</strong>，以提取和传递知识用于指导搜索新任务的最佳模型。本章简要概述了有效实现这一目标的不同元学习方法。</p><p><strong>元学习</strong>一词涵盖基于其他任务的先前经验的任何类型的学习。这些先前的任务越<strong>相似</strong>，我们可以利用的元数据类型就越多，并且定义任务相似性将是一个关键的总体挑战。不用多说，天下没有免费的午餐（Wolpert和Macready，1996； Giraud-Carrier和Provost，2005）。当一项新任务代表完全不相关的现象或随机噪音时，利用先前的经验将无效。幸运的是，在现实世界中的任务中，有很多机会可以学习以前的经验。</p><p>在本章的其余部分，我们根据元数据学习所利用的元数据类型对元学习技术进行分类，从最通用的到最特定于任务的。首先，在第2节中，我们讨论如何<strong>纯粹从模型评估中学习</strong>。这些技术可用于推荐通常有用的配置和配置搜索空间，以及从<strong>经验相似的</strong>任务中转移知识。在第3节中，我们讨论如何<strong>表征</strong>任务以更明确地表达任务相似性并建立元模型来学习数据特征与学习性能之间的关系。最后，第4节介绍了如何在固有相似的任务之间<strong>传递训练后的模型参数</strong>，例如共享相同的输入功能，从而可以进行迁移学习（Pan和Yang，2010）和少样本学习（Ravi和Larochelle，2017）。</p><p>请注意，尽管<strong>多任务学习</strong>（Caruana，1997）（同时学习多个相关任务）和<strong>集成学习</strong>（Dietterich，2000）（在同一任务上构建多个模型）通常可以与元学习系统有意义地结合，本身并不涉及在其他任务上的先前经验中学习。</p><h3 id="Learning-from-Model-Evaluations"><a href="#Learning-from-Model-Evaluations" class="headerlink" title="Learning from Model Evaluations"></a>Learning from Model Evaluations</h3><hr><p>考虑到我们可以访问先前任务$t_j \in T$，所有已知任务的集合以及一组学习算法，这些算法完全由其<strong>配置</strong>$\theta_i\in \Theta$定义； 在此，$\Theta$表示离散的，连续的或混合的配置空间，其可以包括超参数设置，流程组件和/或网络架构组件。$P$是任务$t_j$上所有配置$\theta_i$的所有先前标量评估$P_{i, j}=P(\theta_i,t_j)$的集合，根据预先定义的评估方法，例如 准确性和模型评估技术，例如 交叉验证。$P_{new}$是新任务$t_{new}$上一组已知评估$P_{i,new}$的集合。现在，我们想训练一个元学习器$L$，它预测针对新任务$t_{new}$的推荐配置$\Theta^\ast _{new}$。元学习器接受元数据$P\cup P_{new}$的训练。$P$通常是事先收集的，或者是从元数据存储库中提取的（Vanschoren等，2014，2012）。通过元学习技术本身以迭代方式学习$P_{new}$，有时以另一种方法生成的初始$P’_ {new}$进行<strong>热启动</strong>。</p><h4 id="Task-Independent-Recommendations"><a href="#Task-Independent-Recommendations" class="headerlink" title="Task-Independent Recommendations"></a>Task-Independent Recommendations</h4><p>首先，假设无法获得有关$t_{new}$的任何评估，因此$P_{new}=\emptyset$。然后，我们仍然可以学习函数$f: \Theta \times T \to \{\theta^\ast _ k\},k = 1..K$，产生了一组<strong>独立</strong>于$t_{new}$的推荐配置。然后可以重新评估这些$\theta ^ \ast _ k$，以选择最佳的$\theta  _ k$，或热启动进一步的优化方法，例如第2.3节中讨论的方法。</p><p>这种方法通常会产生排序，即<strong>有序</strong>集合$\theta ^ \ast _ k$。这通常是通过将$\theta$离散化为一组候选配置$\theta_ i$，该候选配置也称为<strong>portfolio</strong>，在大量任务$t_j$上进行评估来完成的。然后，我们可以根据<strong>成功率</strong>，<strong>AUC</strong>或<strong>significant wins</strong>来建立每个任务上的排名（Brazdil等，2003a； Demˇsar，2006； Leite等，2012）。但是，通常希望将同样好的但速度更快的算法排在更高的位置，并且已经提出了多种方法来权衡准确性和训练时间（Brazdil等人，2003a; van Rijn等人，2015）。接下来，我们可以将这些单任务排名汇总为<strong>全局排名</strong>，例如通过计算在所有任务上的平均排名（Lin，2010; Abdulrahman et al。，2018）。当没有足够的数据来建立全局排名时，可以根据每个先验任务的最佳已知配置来推荐<strong>配置子集</strong>（Todorovski和Dzeroski，1999; Kalousis，2002），或者返回<strong>准线性排名</strong>（Cook等。（1996）。</p><p>为了找到任务$t_{new}$的最佳$\theta^\ast$，这是从未见过的任务，一种随时随地的简单方法是选择前$K$个配置（Brazdil等人，2003a），从列表中查找并依次评估每个配置在$t_{new}$上的表现。在固定值$K$个，时间预算约束或找到足够准确的模型之后，可以停止此评估。在时间受限的环境中，已表明多目标排名（包括训练时间）更快地收敛到接近最优的模型（Abdulrahman等，2018； van Rijn等，2015），并提供了强有力的基线用于算法比较（Abdulrahman等，2018; Leite等，2012）。</p><p>与上述方法非常不同的方法是，首先对特定任务$t_j$的所有先前评估拟合微分函数$f_j(\theta_i)= P_{i,j}$，然后使用梯度下降找到每个先前任务的优化配置$\theta^\ast _ j$（Wistuba等人，2015a）。假设某些任务$t_j$与$t_{new}$相似，则这些$\theta ^ \ast _ j$对于热启动贝叶斯优化方法很有用。</p><h4 id="Confifiguration-Space-Design"><a href="#Confifiguration-Space-Design" class="headerlink" title="Confifiguration Space Design"></a>Confifiguration Space Design</h4><h4 id="Confifiguration-Transfer"><a href="#Confifiguration-Transfer" class="headerlink" title="Confifiguration Transfer"></a>Confifiguration Transfer</h4><h5 id="Relative-Landmarks"><a href="#Relative-Landmarks" class="headerlink" title="Relative Landmarks"></a>Relative Landmarks</h5><h5 id="Surrogate-Models"><a href="#Surrogate-Models" class="headerlink" title="Surrogate Models"></a>Surrogate Models</h5><h5 id="Warm-Started-Multi-task-Learning"><a href="#Warm-Started-Multi-task-Learning" class="headerlink" title="Warm-Started Multi-task Learning"></a>Warm-Started Multi-task Learning</h5><h5 id="Other-Techniques"><a href="#Other-Techniques" class="headerlink" title="Other Techniques"></a>Other Techniques</h5><h4 id="Learning-Curves"><a href="#Learning-Curves" class="headerlink" title="Learning Curves"></a>Learning Curves</h4><h3 id="Learning-from-Task-Properties"><a href="#Learning-from-Task-Properties" class="headerlink" title="Learning from Task Properties"></a>Learning from Task Properties</h3><hr><h4 id="Task-Independent-Recommendations-1"><a href="#Task-Independent-Recommendations-1" class="headerlink" title="Task-Independent Recommendations"></a>Task-Independent Recommendations</h4><h4 id="Confifiguration-Space-Design-1"><a href="#Confifiguration-Space-Design-1" class="headerlink" title="Confifiguration Space Design"></a>Confifiguration Space Design</h4><h4 id="Confifiguration-Transfer-1"><a href="#Confifiguration-Transfer-1" class="headerlink" title="Confifiguration Transfer"></a>Confifiguration Transfer</h4><h5 id="Relative-Landmarks-1"><a href="#Relative-Landmarks-1" class="headerlink" title="Relative Landmarks"></a>Relative Landmarks</h5><h5 id="Surrogate-Models-1"><a href="#Surrogate-Models-1" class="headerlink" title="Surrogate Models"></a>Surrogate Models</h5><h5 id="Warm-Started-Multi-task-Learning-1"><a href="#Warm-Started-Multi-task-Learning-1" class="headerlink" title="Warm-Started Multi-task Learning"></a>Warm-Started Multi-task Learning</h5><h5 id="Other-Techniques-1"><a href="#Other-Techniques-1" class="headerlink" title="Other Techniques"></a>Other Techniques</h5><h4 id="Learning-Curves-1"><a href="#Learning-Curves-1" class="headerlink" title="Learning Curves"></a>Learning Curves</h4><h3 id="Learning-from-Task-Properties-1"><a href="#Learning-from-Task-Properties-1" class="headerlink" title="Learning from Task Properties"></a>Learning from Task Properties</h3><hr><h4 id="Meta-Features"><a href="#Meta-Features" class="headerlink" title="Meta-Features"></a>Meta-Features</h4><h4 id="Learning-Meta-Features"><a href="#Learning-Meta-Features" class="headerlink" title="Learning Meta-Features"></a>Learning Meta-Features</h4><h4 id="Warm-Starting-Optimization-from-Similar-Tasks"><a href="#Warm-Starting-Optimization-from-Similar-Tasks" class="headerlink" title="Warm-Starting Optimization from Similar Tasks"></a>Warm-Starting Optimization from Similar Tasks</h4><h4 id="Meta-Models"><a href="#Meta-Models" class="headerlink" title="Meta-Models"></a>Meta-Models</h4><h5 id="Ranking"><a href="#Ranking" class="headerlink" title="Ranking"></a>Ranking</h5><h5 id="Performance-Prediction"><a href="#Performance-Prediction" class="headerlink" title="Performance Prediction"></a>Performance Prediction</h5><h4 id="Pipeline-Synthesis"><a href="#Pipeline-Synthesis" class="headerlink" title="Pipeline Synthesis"></a>Pipeline Synthesis</h4><h4 id="To-Tune-or-Not-to-Tune"><a href="#To-Tune-or-Not-to-Tune" class="headerlink" title="To Tune or Not to Tune?"></a>To Tune or Not to Tune?</h4><h3 id="Learning-from-Prior-Models"><a href="#Learning-from-Prior-Models" class="headerlink" title="Learning from Prior Models"></a>Learning from Prior Models</h3><hr><h4 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h4><h4 id="Meta-Learning-in-Neural-Networks"><a href="#Meta-Learning-in-Neural-Networks" class="headerlink" title="Meta-Learning in Neural Networks"></a>Meta-Learning in Neural Networks</h4><h4 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h4><p>一项特别具有挑战性的元学习问题是，鉴于有大量可用训练集的非常相似任务的先前经验，我们仅使用几个训练示例就可以训练出准确的深度学习模型。这称为少样本学习。人类对此具有与生俱来的能力，我们希望构建可以做到这一点的机器学习agent（Lake等人，2017）。一个具体的例子是“ K-shot N-way”分类，其中给了我们某些类别（例如物体）的许多例子（例如图像），并且想要学习一个能够仅使用$K$个示例对$N$个新类别进行分类的分类器$l_{new}$。</p><p>例如，利用先前的经验，我们可以学习所有任务的通用特征表示，通过更好的模型参数初始化$W_{init}$开始训练$l_{new}$，以及获得归纳偏差以帮助指导模型参数的优化，从而使$l_{new}$能比其他方法训练快得多。</p><p>单样本学习的早期工作主要基于手工设计的特征 (Fei-Fei et al., 2006; Fei-Fei, 2006; Fink, 2005; Bart and Ullman, 2005)。但是，通过元学习，我们希望以端到端的方式学习所有任务的通用特征表示。</p><p>Vinyals et al. (2016) 指出，要从很少的数据中学习，就应该关注非参数模型（例如k近邻），该模型使用记忆组件而不是学习许多模型参数。他们的元学习器是一个匹配网络，它应用了神经网络中记忆组件的概念。它为标记的样本学习通用表示，并使用余弦相似度将每个新的测试样本与存储的样本进行匹配。该网络在小批次上进行了训练，每个批次仅包含几个特定任务的样本。</p><p>Snell et al. (2017) 提出了原型网络，将样本映射到p维向量空间，以使给定输出类的样本彼此接近。然后，它计算每个类的原型（均值向量）。新的测试样本被映射到相同的向量空间，并且距离度量用于在所有可能的类上创建softmax。Ren et al. (2018) 将这种方法扩展到半监督学习。</p><p>Ravi and Larochelle (2017) 使用基于LSTM的元学习器来学习用于训练神经网络学习器的更新规则。对于每个新样本，学习器将当前的梯度和损失返回给LSTM元学习器，然后LSTM元学习器更新学习器的模型参数$\{w_k\}$。元学习器在所有先前任务上训练。</p><p>另一方面，模型无关的元学习（MAML） (Finn et al., 2017)不尝试学习更新规则，而是学习模型参数初始化$W_{init}$，该模型可以更好地推广到类似任务。从随机$\{w_k\}$开始，迭代选择一批先前的任务，并针对每个任务对学习器进行$K$个样本的训练，以计算梯度和损失（在测试集上）。然后，它会将元梯度反向传播，以沿权重$\{w_k\}$更容易更新的方向进行更新。换句话说，在每次迭代之后，权重$\{w_k\}$成为更好的$W_{init}$，可以开始对任何任务进行微调。Finn and Levine (2017) 表明，在使用足够深的ReLU网络和某些损失的情况下，MAML能够逼近任何学习算法。他们还得出结论，与基于LSTM的元学习方法相比，MAML初始化对于小样本的过拟合更具弹性，并且泛化得更广泛。 Grant et al. (2018) 提出了MAML的新派生和扩展，说明了该算法可以理解为推理贝叶斯模型中先验分布的参数。</p><p>REPTILE (Nichol et al., 2018) 是MAML的近似值，它对给定任务执行$K$次迭代的随机梯度下降，然后朝$K$次迭代后获得的权重方向逐渐移动初始化权重。Intuition在于每个任务可能具有一组以上的最佳权重$\{w_i^\ast\}$，目标是找到一个与每个任务至少接近$\{w_i^\ast\}$的$W_{init}$。</p><p>最后，我们还可以从黑盒神经网络派生一个元学习器。 Santoro et al. (2016a)提出了记忆增强神经网络（MANNs），它可以训练神经图灵机（NTM） (Graves et al., 2014)，这是一种具有增强记忆功能的神经网络，是一种元学习器。然后，该元学习者可以记住有关先前任务的信息，并利用这些信息学习学习器$l_{new}$。 SNAIL (Mishra et al., 2018)是一种通用的元学习器架构，由交错的时间卷积和因果attention层组成。卷积网络为训练样本（图像）学习一个公共特征向量，以汇总过去的经验信息。因果关注层可从收集的经验中学习选择哪些信息，以适应新的任务。</p><p>总体而言，深度学习和元学习的交集被证明是开创性新思想的特别沃土，我们希望随着时间的推移，这一领域将变得越来越重要。</p><h4 id="Beyond-Supervised-Learning"><a href="#Beyond-Supervised-Learning" class="headerlink" title="Beyond Supervised Learning"></a>Beyond Supervised Learning</h4><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><hr><p>元学习机会以许多不同的方式展现出来，并且可以使用多种学习技术加以体现。每当我们尝试学习某个任务时，无论成功与否，我们都会获得有益的经验，可以利用这些经验来学习新任务。我们永远不必完全从头开始。相反，我们应该系统地收集我们的“学习资源”，并从中学习以构建随着时间的推移不断改进的AutoML系统，从而帮助我们更加有效地解决新的学习问题。我们遇到的新任务越多，这些新任务越相似，我们就越可以利用先前的经验，以至于大多数必需的学习已经事先完成。计算机系统能够存储几乎无限量的以前的学习经验（以元数据的形式）的能力为以全新的方式使用该经验提供了广泛的机会，而我们才刚刚开始学习如何从中学习事先有效的经验。然而，这是一个值得实现的目标：学习如何学习任何任务，不仅使我们了解如何学习特定的任务，还使我们拥有了更多的能力。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;元学习或学会学习是系统地观察不同机器学习方法如何在广泛的学习任务中执行的科学，然后从这种经验或元数据中学习，以比其他方式更快地学习新任务 。&lt;br&gt;这不仅极大地加速和改进了机器学习流程或神经网络架构的设计，还使我们能够用数据驱动方式学习的新方法取代手工设计算法。&lt;br&gt;在本章中，我们将概述这个迷人且不断发展的领域的最新技术。&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文翻译" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="meta-learning" scheme="http://yoursite.com/tags/meta-learning/"/>
    
      <category term="Survey" scheme="http://yoursite.com/tags/Survey/"/>
    
  </entry>
  
  <entry>
    <title>Learning to Learn via Self-Critique</title>
    <link href="http://yoursite.com/2019/10/09/Learning-to-Learn-via-Self-Critique/"/>
    <id>http://yoursite.com/2019/10/09/Learning-to-Learn-via-Self-Critique/</id>
    <published>2019-10-09T12:33:12.000Z</published>
    <updated>2019-12-03T11:22:59.224Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>在少样本学习中，机器学习系统从一小组与特定任务有关的有标签样本中学习，从而可以推广到同一任务的新示例。鉴于此类任务中有标签样本的数量有限，我们希望充分利用所有可能的信息。通常，模型从小型训练集（support-set）中学习任务特定的信息，以对无标签验证集（target-set也叫query-set）进行预测。target-set包含其他特定于任务的信息，而现有的少样本学习方法并未利用这些信息。通过transductive learning来使用target-set样本需要更先进的方法；at inference time, the target-set contains only unlabelled input data-points, and so discriminative learning cannot be used。在本文中，我们提出了一个名为“Self-Critique and Adapt”或SCA的框架，该框架可以学习无标签损失函数，该函数被参数化为神经网络。基本模型使用现有方法（例如，随机梯度下降与交叉熵损失相结合）在支持集上学习，然后使用学习到的损失函数针对传入的target-task进行更新。学习无标签损失函数，以便target-set-updated模型实现更高的泛化性能。实验表明，与仅适用于支持集的基准相比，SCA可以显着降低错误率，并可以在Mini-ImageNet和Caltech-UCSD Birds 200上获得最先进的基准性能。</p><h3 id="Self-Critique-and-Adapt"><a href="#Self-Critique-and-Adapt" class="headerlink" title="Self-Critique and Adapt"></a>Self-Critique and Adapt</h3><p>为了让模型学习和适应仅输入数据点可用的设置（例如，在给定任务的少样本target-set上），就需要一种无标签损失函数。例如，许多无监督的学习方法试图使生成概率最大化，因此使用负对数似然度（或其bound）作为损失函数。通常，大多数生成模型都与任务无关。在一组特定的任务中，针对损失函数可能会有更恰当和专业的选择。</p><p>手动设计这样的损失函数具有挑战性，通常只能产生可能在一种设置下起作用而在另一种情况下不起作用的损失函数。了解损失函数选择的全部影响并不容易。相反，我们提出了一种Self-Critique and Adapt方法，该方法元学习特定任务集的损失函数。它是通过使用set-to-set少样本学习框架并使用端到端基于梯度的可微元学习作为我们的学习框架来解决问题的。</p><p>SCA与模型无关，可以应用在<strong>任何</strong>使用内环优化过程来获取特定于任务的信息的端到端可微且基于梯度的元学习方法之上。许多这样的方法（Ravi and Larochelle, 2016; Finn et al., 2017; Li et al., 2017; Antoniou et al., 2019; Finn et al., 2018; Qiao et al., 2018; Rusu et al., 2018; Grant et al., 2018）目前正在争夺少样本学习领域中的SOTA。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://res.cloudinary.com/dyxexppyu/image/upload/v1570623088/wps_2019-09-28_22-57-12_eioif3.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>在上图中总结的Self-Critique and Adapt，采用一个基本模型并使用现有的基于梯度的元学习方法(e.g. MAML (Finn et al., 2017), MAML++ (Antoniou et al., 2019) or LEO (Rusu et al., 2018))，根据support-set更新基本模型，然后推断出对target-set的预测。推断出预测后，将它们与其他基本模型相关的信息（例如模型参数，任务嵌入等）串联在一起，然后传递到可学习的critic loss network，其输出应解释为给定输入的loss值。该critic network计算并返回关于target-set的损失。然后针对该critic loss，使用任何随机梯度优化方法(如SGD)更新基本模型；如有必要，可以多次进行更新。这种inner-loop优化可生成特定于support/target set信息的预测模型。</p><p>内部循环过程在推理时直接用于手头的任务。但是，与其他元学习设置一样，我们使用一系列训练任务来优化inner-loop（这些任务与第7节中所述的测试任务不同）。使用训练任务中的ground truth标签评估inner-loop学习的预测模型的质量。然后，outer loop优化初始参数和critic loss，以最大化inner loop预测的质量。与其他元学习方法一样，整个inner loop的可微性确保可以使用基于梯度的方法来学习此outer loop。</p><p>在本文中，我们使用MAML ++作为基本方法。我们用$f(\cdot,\theta)$表示参数化为神经网络的模型，参数为$\theta$，critic loss $C(\cdot,W)$也是参数为$W$的神经网络。我们想学习好的参数$\theta$和$W$，当优化模型$f$时，针对support set $S_b=\{x_S, y_S\}$上的loss $L$，需要执行$N$步优化，然后再针对critic loss $C$ 另外向target-set $T_b = \{x_T\}$优化$I$步，可以在target-set上实现良好的泛化性能。这里，$b$是一个具体任务在一批任务中的索引。完整算法在下面进行了描述。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://res.cloudinary.com/dyxexppyu/image/upload/v1570624552/wps_2019-10-09_20-35-27_twt5kl.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>算法框图中的等式2定义了潜在的条件特征集合$F$，这些特征概括了基本模型及其行为。这些特征可以被无监督的critic loss $C$ 用来调整target set 更新。在这些可能的特征中，$f (\theta_N,x_T)$是基本模型$f$的预测，使用参数$\theta_N$（即针对support-set loss的N步更新后的参数），而$g(x_S,x_n)$是任务嵌入，参数化为神经网络，该神经网络以support和target输入数据点为条件。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在少样本学习中，机器学习系统从一小组与特定任务有关的有标签样本中学习，从而可以推广到同一任务的新示例。
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>HOW TO TRAIN YOUR MAML</title>
    <link href="http://yoursite.com/2019/10/09/HOW-TO-TRAIN-YOUR-MAML/"/>
    <id>http://yoursite.com/2019/10/09/HOW-TO-TRAIN-YOUR-MAML/</id>
    <published>2019-10-09T12:31:44.000Z</published>
    <updated>2019-12-03T11:23:23.223Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>少样本学习领域最近有了长足的进步。这些进步中的大多数来自将少样本学习构建为元学习问题。目前，Model Agnostic Meta Learning或MAML是通过元学习进行少样本学习的最佳方法之一。MAML简单，优雅且功能强大，但是它具有许多问题，例如对神经网络结构非常敏感，通常会导致训练过程中的不稳定，需要艰巨的超参数搜索来稳定训练并实现高泛化，在训练和推理时都非常耗费算力。在本文中，我们提出了对MAML的各种修改，这些修改不仅可以稳定系统，而且可以大大提高MAML的泛化性能，收敛速度和计算开销，我们称之为MAML++。</p><h3 id="MAML"><a href="#MAML" class="headerlink" title="MAML"></a>MAML</h3><script type="math/tex; mode=display">\theta_{0}=\theta_{0}-\beta \nabla_{\theta} \sum_{b=1}^{B} \mathcal{L}_{T_{b}}\left(f_{\theta_{N}^{b}\left(\theta_{0}\right)}\right)</script><h4 id="3-1-MAML的问题"><a href="#3-1-MAML的问题" class="headerlink" title="3.1 MAML的问题"></a>3.1 MAML的问题</h4><p>MAML的简单，优雅和高性能使其成为元学习的非常强大的框架。但是，MAML也有许多问题，使其难以使用。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://res.cloudinary.com/dyxexppyu/image/upload/v1570623097/wps_2019-09-27_16-36-53_jwqoom.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p><strong>梯度不稳定性</strong>：<br>如上图所示，受到神经网络结构和全局超参数设置的影响，MAML在训练过程中可能非常不稳定。优化outer loop涉及多次穿过由同一网络组成的未展开的inner loop进行导数的反向传播。仅此一项就可能导致梯度问题。但是，模型架构进一步加剧了梯度问题，标准4层卷积网络但没有skip-connections。缺少任何skip-connections意味着每个梯度必须多次通过每个卷积层。实际上，梯度将被多次乘以相同的参数集。经过多次反向传播后，展开网络的深度结构和skip-connections的缺失会分别引起梯度爆炸和梯度消失问题。</p><p><strong>二阶导数成本</strong>：<br>通过梯度更新步骤进行优化需要计算二阶梯度，而二阶梯度的计算成本非常高昂。MAML的作者建议使用一阶近似将处理速度提高三倍，但是使用这些近似可能会对最终的泛化误差产生负面影响。已经在Reptile(Nichol et al., 2018)中尝试了进一步使用一阶方法的尝试，作者在基本模型上应用标准SGD，然后更新其初始化参数向N步更新后的参数方向迈出一步。Reptile的结果变化较大，在某些情况下超过MAML，而在另一些情况下则不如MAML。尚未提出减少计算时间而不牺牲泛化性能的方法。</p><p><strong>缺少Batch Normalization统计量累积</strong>：<br>影响生成性能的另一个问题是原始MAML论文中在实验中使用Batch Normalization的方式。不是累积运行统计信息，而是将当前batch的统计信息用于Batch Normalization。这导致Batch Normalization的效果较差，因为学习的偏差必须适应各种不同的均值和标准差，而不是单个均值和标准差。另一方面，如果Batch Normalization使用累积的运行统计信息，则最终将收敛到某些全局平均值和标准偏差。这样就只剩下一个均值和标准偏差来学习偏差了。使用running统计信息而不是batch统计信息，可以极大地提高收敛速度，稳定性和泛化性能，因为归一化的特征将导致更平滑的优化环境（Santurkar et al.，2018）。</p><p><strong>共享（跨step）Batch Normalization偏差</strong>：<br>MAML中的批处理规范化的另一个问题源于以下事实：Batch Normalization偏差未在inner loop中更新；相反，在基础模型的所有迭代中都使用相同的偏差。隐式地执行此操作将假定所有基本模型在整个inner loop更新中都是相同的，因此通过它们传递的特征具有相同的分布。这是一个错误的假设，因为在每次inner loop更新时，都会实例化一个新的基础模型，该基础模型与前一个基础模型的差异足以从偏差估计的角度将其视为新模型。因此，为基本模型的所有迭代学习单个偏差集会限制性能。</p><p><strong>共享的inner loop（跨step和跨参数）学习率</strong>：<br>影响泛化和收敛速度（就训练迭代而言）的一个问题是对所有参数和所有更新步骤使用共享学习率的问题。这样做会带来两个主要问题。具有固定的学习率要求进行多次超参数搜索，以找到特定数据集的正确学习率； 根据搜索的完成方式，此过程可能在计算上非常昂贵。<br>（Li et al。，2017）中的作者建议为网络的每个参数学习学习率并更新方向。这样做解决了手动搜索正确学习率的问题，并且还允许各个参数具有较小或较大的学习率。然而，这种方法带来了自己的问题。由于网络包含40K到50K的参数（取决于数据点的维数），因此学习每个网络参数的学习率意味着要增加计算量并增加内存使用量。</p><p><strong>固定outer loop学习率</strong>：<br>在MAML中，作者使用具有固定学习率的Adam来优化元目标。事实证明，使用阶跃或余弦函数对学习率进行退火对于在多种情况下实现最新的泛化性能至关重要(Loshchilov &amp; Hutter, 2016; He et al., 2016; Larsson et al., 2016; Huang et al., 2017)。因此，我们认为使用静态学习率会降低MAML的泛化性能，这也可能是优化速度较慢的原因。此外，具有固定的学习速率可能意味着必须花费更多（计算）时间来调整学习速率。</p><h4 id="稳定，自动和改进的MAML"><a href="#稳定，自动和改进的MAML" class="headerlink" title="稳定，自动和改进的MAML"></a>稳定，自动和改进的MAML</h4><p>在本节中，我们提出了解决MAML框架问题的方法，如第3.1节所述。每个解决方案都有一个与要解决的问题相同的参考。</p><p><strong>梯度不稳定性→多步损失优化（MSL）</strong>：<br>MAML最小化完成对support set任务的<strong>所有</strong>inner-loop更新后的基础网络所计算出的在target set的loss。相反，我们建议最小化完成对support set任务的<strong>每一步</strong>更新的基础网络所计算出的在target set的loss。更具体地说，我们建议最小化的loss是每步support set loss更新后target set loss的加权总和。更正式地：</p><script type="math/tex; mode=display">\theta=\theta-\beta \nabla_{\theta} \sum_{b=1}^{B} \sum_{i=0}^{N} v_{i} \mathcal{L}_{T_{b}}\left(f_{\theta_{i}^{b}}\right)</script><p>其中$\beta$是学习率，$L_{T_b}(f_{\theta^b_i})$表示在$i$向最小化support set任务loss更新后的基本网络权重在任务$b$的target set loss，$v_i$表示步骤$i$中target set loss的重要性权重， 用于计算加权和。</p><p>通过使用上面提出的multi-step loss，我们改善了梯度传播，因为现在每一步的基础网络权重都直接（对于当前步loss）和间接（来自后续步的loss）接收梯度。使用第3节中描述的原始方法，由于反向传播，除最后一步外，每个步骤的基础网络权重都被隐式优化，这导致了MAML的许多不稳定问题。但是，如图1所示，使用multi-step loss可以缓解此问题。此外，我们对每步损耗采用了退火加权。最初，所有损失都对损失具有相同的贡献，但是随着迭代次数的增加，我们会减少早期步骤的权重，并逐渐增加后续步骤的权重。这样做是为了确保随着训练的进行，最终步数loss会受到优化器的更多关注，从而确保其达到可能的最低损失。如果不使用退火，我们发现最终损失可能会高于原始方法。</p><p><strong>二阶导数成本→导数退火（DA）</strong>：<br>使MAML具有更高的计算效率的一种方法是减少所需的inner-loop更新次数，这可以通过本报告后续部分中介绍的某些方法来实现。但是，在本段中，我们提出了一种直接减少per-step计算开销的方法。MAML的作者提出了梯度导数的一阶近似的用法。但是，他们在整个训练阶段都采用了一阶近似。相反，我们建议随着训练的进行对微分阶数进行退火。更具体地说，我们建议在训练阶段的前50个epochs使用一阶梯度，然后在训练阶段的其余时间使用二阶梯度。我们凭经验证明，这样做可以大大加快前50个epochs的速度，同时允许进行二阶训练，以实现二阶梯度提供给模型的强大泛化性能。另一个有趣的观察结果是，与更不稳定的仅二阶实验相反，微分阶数退火实验没有出现梯度爆炸或消失的事件。在开始使用二阶导数之前使用一阶可以用作一种强大的预训练方法，该方法可以学习不太可能产生梯度爆炸/减小问题的参数。</p><p><strong>缺少Batch Normalization统计信息累积→Per-Step Batch Normalization运行统计信息（BNRS）</strong>：<br>在MAML Finn et al. (2017)的原始实现中，作者仅使用当前batch统计信息作为Batch Normalization统计信息。我们认为，这导致了3.1节中描述的各种不良影响。为了缓解这些问题，我们建议使用running batch统计信息进行Batch Normalization。要在MAML上下文中简单地实现Batch Normalization，就需要在inner-loop fast-knowledge获取过程的所有更新步骤之间共享running batch统计信息。然而，这样做将导致不希望的结果，即所存储的统计信息将在网络的所有inner loop更新之间共享。由于能在跨网络参数的多个更新上工作的学习参数的复杂性不断提高，因此这将导致优化问题，并有可能减慢或完全停止优化。更好的替代方法是按步骤收集统计信息。要按步骤收集running统计信息，需要实例化网络中每个Batch Normalization层的N组running平均值和running标准偏差集（其中N是inner loop更新步骤的总数），并使用优化过程中采取的步骤分别更新running统计信息。per-step batch normalization方法应加快MAML的优化速度，同时潜在地提高泛化性能。</p><p><strong>共享（跨步骤）Batch Normalization偏差→Per-Step Batch Normalization权重和偏差（BNWB）</strong>：<br>在MAML论文中，作者训练他们的模型去学到对每一层的<strong>一组</strong>偏差。这样做是假设通过网络传递的特征的分布是相似的。但是，这是一个错误的假设，因为基本模型已更新了许多次，从而使特征分布彼此之间越来越不相似。为了解决这个问题，我们建议在inner-loop更新过程中<strong>每步</strong>学习一组偏差。这样做意味着Batch Normalization将学习特定于在每个集合处看到的特征分布的偏差，这将提高收敛速度，稳定性和泛化性能。</p><p><strong>共享的inner loop学习率（跨步和跨参数）→学习每层每步学习率和梯度方向（LSLR）</strong>：<br>Li et al. (2017)的先前工作，证明了学习基础网络中<strong>每个</strong>参数的学习率和梯度方向可以提高系统的泛化性能。然而，这导致参数数量增加和计算开销增加的结果。因此，我们建议改为学习网络中每一层的学习率和方向，以及随着基础网络的逐步适应而学习不同的学习率。学习每层而不是每个参数的学习率和方向应该减少所需的内存和计算，同时在更新步骤中提供更多的灵活性。此外，对于学习到的每个学习率，将有N个实例的学习率，每个步骤要采用一个实例。通过这样做，参数可以自由学习在每步降低学习率，这可以帮助减轻过拟合的情况。</p><p><strong>固定outer loop学习率→元优化器学习率的余弦退火（CA）</strong>：<br>在MAML中，作者在元模型的优化器上使用静态学习率。通过使用阶跃函数(He et al., 2016)或余弦函数(Loshchilov＆Hutter，2016)对学习率进行退火已被证明对于具有更高泛化能力的学习模型至关重要。余弦退火调度在产生最新技术结果方面特别有效，同时消除了对学习速率空间进行任何超参数搜索的需求。因此，我们建议将余弦退火调度应用于元模型的优化器（即元优化器）。退火学习率可使模型更有效地拟合训练集，结果可能会产生更高的泛化性能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;少样本学习领域最近有了长足的进步。这些进步中的大多数来自将少样本学习构建为元学习问题。目前，Model
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION</title>
    <link href="http://yoursite.com/2019/10/09/META-LEARNING-WITH-LATENT-EMBEDDING-OPTIMIZATION/"/>
    <id>http://yoursite.com/2019/10/09/META-LEARNING-WITH-LATENT-EMBEDDING-OPTIMIZATION/</id>
    <published>2019-10-09T12:27:13.000Z</published>
    <updated>2019-12-03T11:21:42.068Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>基于梯度的元学习技术在解决具有挑战性的少样本学习和快速适应问题方面有着广泛的应用和实用价值。然而，它们在极端低数据状态下在高维参数空间上操作时存在实际困难。我们表明，有可能通过学习到一个模型参数的依赖数据的潜在生成表示，并在此低维潜在空间中执行基于梯度的元学习，从而绕过这些限制。最终的方法，latent embedding optimization（LEO），将基于梯度的自适应过程与模型参数的底层高维空间解耦。我们的评估表明，LEO可以在竞争激烈的miniImageNet和tieredImageNet少样本分类任务中达到最先进的性能。进一步的分析表明，LEO能够捕获数据中的不确定性，并能通过在潜在空间中进行优化，更有效地进行适应。</p><h4 id="2-3-LATENT-EMBEDDING-OPTIMIZATION-FOR-META-LEARNING"><a href="#2-3-LATENT-EMBEDDING-OPTIMIZATION-FOR-META-LEARNING" class="headerlink" title="2.3 LATENT EMBEDDING OPTIMIZATION FOR META-LEARNING"></a>2.3 LATENT EMBEDDING OPTIMIZATION FOR META-LEARNING</h4><p>本文的主要贡献是表明，有可能并且确实有益的是，将基于优化的元学习技术与模型参数的高维空间解耦。我们通过学习具有信息瓶颈的随机潜在空间来实现这一目标，该瓶颈取决于输入数据，并从中生成高维参数。</p><p>我们没有像在MAML中那样显式实例化并维护一组唯一的模型参数θ，而是学习了具有相同目的的模型参数的生成分布。这是一个自然扩展：我们将找到单个最佳$\theta^* \in\Theta$的要求放宽到近似于$\Theta$的数据相关条件概率分布的要求，这可能更具表达性。由编码过程和解码（或参数生成）过程组成的结构的选择，使我们能够在学习到的参数生成模型的低维嵌入空间中执行基于MAML梯度的适应步骤（或”inner loop”）（图1）。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://res.cloudinary.com/dyxexppyu/image/upload/v1570623091/wps_2019-09-28_22-55-47_lrbegy.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h5 id="2-3-1-模型概述"><a href="#2-3-1-模型概述" class="headerlink" title="2.3.1 模型概述"></a>2.3.1 模型概述</h5><p>大致的操作如（算法1）所示。首先，给定任务实例$T_i$，将输入$\{x^k_n\}$通过随机编码器以产生潜在边码$z$，然后使用参数生成器将其解码为参数$\theta_i$。给定这些实例化的模型参数，在潜在空间中应用一个或多个适应步骤，通过计算相对于$z$的loss的微分，梯度更新几步获得$z\prime$，解码新的模型参数并获得新的loss。最后，对优化后的编码进行解码以生成最终的适应参数$\theta\prime_i$，该参数可用于执行任务或计算任务特定的meta-loss。通过这种方式，LEO结合了<strong>基于模型</strong>和<strong>基于优化</strong>的元学习的各个方面，产生的参数首先取决于输入数据，然后通过梯度下降进行调整。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://res.cloudinary.com/dyxexppyu/image/upload/v1570623095/wps_2019-09-28_22-54-48_qbjlvg.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>图2显示了生成的网络的结构。直觉上，解码器类似于生成模型，从低维潜在编码映射到模型参数的分布。编码过程可确保基于梯度的适应之前的初始潜在编码和参数已经与数据相关。该编码过程还利用了一个关系网络，该关系网络允许潜在编码依赖于上下文，考虑到问题实例中所有类之间的成对关系。在以下各节中，我们将更正式地解释LEO的各个步骤。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://res.cloudinary.com/dyxexppyu/image/upload/v1570623092/wps_2019-09-28_22-55-25_jksoke.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h5 id="2-3-2-INITIALIZATION-GENERATING-PARAMETERS-CONDITIONED-ON-A-FEW-EXAMPLES"><a href="#2-3-2-INITIALIZATION-GENERATING-PARAMETERS-CONDITIONED-ON-A-FEW-EXAMPLES" class="headerlink" title="2.3.2 INITIALIZATION: GENERATING PARAMETERS CONDITIONED ON A FEW EXAMPLES"></a>2.3.2 INITIALIZATION: GENERATING PARAMETERS CONDITIONED ON A FEW EXAMPLES</h5><ul><li><strong>Encoding</strong></li><li><strong>Decoding</strong></li></ul><h5 id="2-3-3-ADAPTATION-BY-LATENT-EMBEDDING-OPTIMIZATION-LEO-THE-“INNER-LOOP”"><a href="#2-3-3-ADAPTATION-BY-LATENT-EMBEDDING-OPTIMIZATION-LEO-THE-“INNER-LOOP”" class="headerlink" title="2.3.3 ADAPTATION BY LATENT EMBEDDING OPTIMIZATION (LEO) (THE “INNER LOOP”)"></a>2.3.3 ADAPTATION BY LATENT EMBEDDING OPTIMIZATION (LEO) (THE “INNER LOOP”)</h5><h5 id="2-3-4-META-TRAINING-STRATEGY-THE-“OUTER-LOOP”"><a href="#2-3-4-META-TRAINING-STRATEGY-THE-“OUTER-LOOP”" class="headerlink" title="2.3.4 META-TRAINING STRATEGY (THE “OUTER LOOP”)"></a>2.3.4 META-TRAINING STRATEGY (THE “OUTER LOOP”)</h5><h5 id="2-3-5-BEYOND-CLASSIFICATION-AND-LINEAR-OUTPUT-LAYERS"><a href="#2-3-5-BEYOND-CLASSIFICATION-AND-LINEAR-OUTPUT-LAYERS" class="headerlink" title="2.3.5 BEYOND CLASSIFICATION AND LINEAR OUTPUT LAYERS"></a>2.3.5 BEYOND CLASSIFICATION AND LINEAR OUTPUT LAYERS</h5>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;基于梯度的元学习技术在解决具有挑战性的少样本学习和快速适应问题方面有着广泛的应用和实用价值。然而，它们
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Meta-Learning with Implicit Gradients</title>
    <link href="http://yoursite.com/2019/10/09/Meta-Learning-with-Implicit-Gradients/"/>
    <id>http://yoursite.com/2019/10/09/Meta-Learning-with-Implicit-Gradients/</id>
    <published>2019-10-09T12:26:02.000Z</published>
    <updated>2019-12-03T11:22:07.201Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>智能系统的一个核心功能是能够通过借鉴先前的经验来快速学习新任务的能力。最近，基于梯度（或优化）的元学习已成为一种有效的少样本学习方法。在此形式中，仅使用当前任务中的少量数据，即可在outer loop中学习元参数，而在inner-loop中学习特定于任务的模型。扩展这些方法的关键挑战是需要通过inner-loop学习过程计算微分，这可能会带来相当大的计算和内存负担。借助隐式微分，我们开发了隐式MAML算法，该算法仅取决于inner level优化的解，而不取决于inner loop优化器采用的路径。这有效地将元梯度计算与inner loop优化器的选择解耦。因此，我们的方法与inner loop优化器的选择无关，并且可以优雅地处理许多梯度步骤而不会梯度消失或内存限制。从理论上讲，我们证明隐式MAML可以使用不超过计算单个内循环梯度所需的内存占用量来计算准确的元梯度，而不会增加总的计算成本。从实验上，我们证明了隐式MAML的这些好处可转化为在少样本的图像识别benchmarks上的经验收益。</p><p>其他来源：<a href="https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/" target="_blank" rel="noopener">Notes on iMAML: Meta-Learning with Implicit Gradients</a></p><a id="more"></a><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/wps_2019-09-29_21-00-20.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="Problem-Formulation-and-Notations"><a href="#Problem-Formulation-and-Notations" class="headerlink" title="Problem Formulation and Notations"></a>Problem Formulation and Notations</h3><p>我们首先在少样本监督学习的背景下提出元学习问题，然后概括该概念以帮助本文的其余论述。</p><h4 id="Review-of-Few-Shot-Supervised-Learning-and-MAML"><a href="#Review-of-Few-Shot-Supervised-Learning-and-MAML" class="headerlink" title="Review of Few-Shot Supervised Learning and MAML"></a>Review of Few-Shot Supervised Learning and MAML</h4><p>在这种设置下，我们从$P(T)$中提取了一系列元训练任务$\{T_i\} ^M_{i = 1}$。每个任务$T_i$与一个数据集$D_i$相关联，我们可以从中采样两个不相交的集合：$D^{tr}_i$和$D^{test}_i$。这些数据集都由$K$个输入输出对组成。令$x\in X$和$y\in Y$分别表示输入和输出。数据集采用$D^{tr}_i = \{(x^k_i，y^k_i)\}^K_{k = 1}$的形式，对于$D^{test}_i$同样。我们对学习 $h_\phi(x): X \to Y $形式的模型感兴趣，由$\phi\in\Phi\equiv\mathbb {R}^{d}$参数化。任务的性能由损失函数指定，例如交叉熵或平方误差损失。我们将损失函数以$L(\phi，D)$的形式写成参数向量和数据集的函数。任务$T_i$的目标是使用$D^{tr}_ i$学习特定于任务的参数$\phi_i$，以便我们可以使任务的总体或测试损失$L(\phi_i，D^{test}_ i)$最小化。</p><p>在一般的双层元学习设置中，我们考虑使用一组元参数$\theta\in\Theta\equiv\mathbb{R}^{d}$和来自任务任务的训练数据集用于计算任务特定参数的算法空间，形式化如$\phi_i= Alg(\theta,D^{tr}_ i)$针对任务$T_i$。元学习的目标是学习适应后产生良好任务特定参数的元参数，如下所示：</p><script type="math/tex; mode=display">\overbrace{\boldsymbol{\theta}_{\mathrm{ML}}^{*}:=\underset{\boldsymbol{\theta} \in \Theta}{\operatorname{argmin}} F(\boldsymbol{\theta})}^{\text{outer-level}}, \text { where } F(\boldsymbol{\theta})=\frac{1}{M} \sum_{i=1}^{M} \mathcal{L}\left(\overbrace{Alg\left(\boldsymbol{\theta}, \mathcal{D}_{i}^{\mathrm{tr}}\right)}^{\text{inner-level}}, \mathcal{D}_{i}^{\mathrm{test}}\right)</script><p>由于我们通常将$Alg(\theta,D^{tr}_i)$解释为显式或隐式解决潜在优化问题，因此我们将其视为双层优化问题。在元测试（部署）时，当提供与新任务$T_j\sim P(T)$相对应的数据集$D^{tr}_j$时，我们可以通过对学到的元参数使用适应过程来获得良好的泛化性能（即低test error），即得到$\phi_j= Alg(\theta^\ast_{ML},D^{tr}_j)$。</p><p>在MAML中，$Alg(\theta,D)$对应于以$\theta$初始化的一个或多个梯度下降步骤。例如，如果使用一个梯度下降步骤，我们得到：</p><script type="math/tex; mode=display">\phi_{i} \equiv \mathcal{A} l g\left(\boldsymbol{\theta}, \mathcal{D}_{i}^{\mathrm{tr}}\right)=\boldsymbol{\theta}-\alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\boldsymbol{\theta}, \mathcal{D}_{i}^{\mathrm{tr}}\right) . \quad \text { (inner-level of MAML) }</script><p>通常，$\alpha$是标量超参数，但也可以是可学习的向量。因此，对于MAML，元学习参数（$\theta^\ast_{ML}$）具有学习归纳偏差，该偏差特别适合于使用$K$个样本对$P(T)$中的任务进行微调。为了使用基于梯度的方法解决外层问题，我们需要一种贯穿$Alg$计算微分的方法。在MAML中，这对应于通过梯度下降的动力学进行反向传播。</p><h4 id="Proximal-Regularization-in-the-Inner-Level"><a href="#Proximal-Regularization-in-the-Inner-Level" class="headerlink" title="Proximal Regularization in the Inner Level"></a>Proximal Regularization in the Inner Level</h4><p>为了在内层水平上获得足够的学习，同时又避免过拟合，$Alg$需要纳入某种形式的正则化。由于MAML使用少量的梯度步骤，这对应于早停，可以解释为正则化和贝叶斯先验的一种形式。在病态条件数的优化情形和中等样本量学习的情况下，我们可能需要采用多个梯度下降步，这给MAML带来了两个挑战。首先，我们需要通过漫长的$Alg$优化路径进行存储和计算微分，这会带来相当大的计算和内存负担。其次，随着$Alg$中梯度步数的增加，模型参数$\{\phi_i\}$对元参数（$\theta $）的依赖关系会缩小和消失，从而使得元学习变得困难。为了克服这些限制，我们考虑使用更明确的正则化算法： </p><script type="math/tex; mode=display">\mathcal{Alg}^{\star}\left(\boldsymbol{\theta}, \mathcal{D}_{i}^{\mathrm{tr}}\right)=\underset{\boldsymbol{\phi}^{\prime} \in \Phi}{\operatorname{argmin}} \mathcal{L}\left(\boldsymbol{\phi}^{\prime}, \mathcal{D}_{i}^{\mathrm{tr}}\right)+\frac{\lambda}{2}\left\|\boldsymbol{\phi}^{\prime}-\boldsymbol{\theta}\right\|^{2}</script><p>等式中的正则项促使$\phi_i$保持接近$\theta$，从而始终保持很强的依赖性。正则化强度（$\lambda$）的作用类似于MAML中的学习率（$\alpha$），控制先验（$\theta$）相对于数据（$D^{tr}_T$）的强度。像$\alpha$一样，正则强度$\lambda$也可以被学习。此外，$\alpha$和$\lambda$都可以是标量，向量或完整矩阵。为简单起见，我们将$\lambda$作为标量超参数。在等式中，我们用$\star$表示优化问题已完全解决。在实践中，我们使用迭代算法（由$Alg$表示）进行有限迭代，其返回近似最小化解。我们在分析中明确考虑了近似解与精确解之间的差异。</p><h4 id="The-Bi-Level-Optimization-Problem"><a href="#The-Bi-Level-Optimization-Problem" class="headerlink" title="The Bi-Level Optimization Problem"></a>The Bi-Level Optimization Problem</h4><p>为方便起见，有时我们会使用下标而不是参数来表达对任务$T_i$的依赖性，例如我们写：</p><script type="math/tex; mode=display">\mathcal{L}_{i}(\phi):=\mathcal{L}\left(\phi, \mathcal{D}_{i}^{\text {test }}\right), \quad \hat{\mathcal{L}}_{i}(\phi):=\mathcal{L}\left(\phi, \mathcal{D}_{i}^{\text {tr }}\right), \quad \mathcal{A} l g_{i}(\boldsymbol{\theta}):=\mathcal{A} \lg \left(\boldsymbol{\theta}, \mathcal{D}_{i}^{\text {tr }}\right)</script><p>使用这种表示法，可以将二层元学习问题更一般地写为：</p><script type="math/tex; mode=display">\begin{array}{l}{\boldsymbol{\theta}_{\mathrm{ML}}^{*}:=\underset{\boldsymbol{\theta} \in \Theta}{\operatorname{argmin}} F(\boldsymbol{\theta}), \text { where } F(\boldsymbol{\theta})=\frac{1}{M} \sum_{i=1}^{M} \mathcal{L}_{i}\left(\mathcal{A} l g_{i}^{\star}(\boldsymbol{\theta})\right), \text { and }} \\ {\mathcal{A} l g_{i}^{\star}(\boldsymbol{\theta}):=\underset{\boldsymbol{\phi}^{\prime} \in \Phi}{\operatorname{argmin}} G_{i}\left(\boldsymbol{\phi}^{\prime}, \boldsymbol{\theta}\right), \text { where } G_{i}\left(\boldsymbol{\phi}^{\prime}, \boldsymbol{\theta}\right)=\hat{\mathcal{L}}_{i}\left(\boldsymbol{\phi}^{\prime}\right)+\frac{\lambda}{2}\left\|\boldsymbol{\phi}^{\prime}-\boldsymbol{\theta}\right\|^{2}}\end{array} \tag{4}</script><h4 id="Total-and-Partial-Derivatives"><a href="#Total-and-Partial-Derivatives" class="headerlink" title="Total and Partial Derivatives"></a>Total and Partial Derivatives</h4><p>我们用$d$表示全导数，用$\nabla$表示偏导数。对于形式为$L_i(\phi_i)$的嵌套函数，其中$\phi_i= Alg_i(\theta)$，我们有链式法则</p><script type="math/tex; mode=display">\boldsymbol{d}_{\boldsymbol{\theta}} \mathcal{L}_{i}\left(\mathcal{A} l g_{i}(\boldsymbol{\theta})\right)=\frac{d \mathcal{A} l g_{i}(\boldsymbol{\theta})}{d \boldsymbol{\theta}} \nabla_{\boldsymbol{\phi}} \mathcal{L}_{i}(\boldsymbol{\phi})|_{\boldsymbol{\phi}=\mathcal{A} l g_{i}(\boldsymbol{\theta})}=\frac{d \mathcal{A} l g_{i}(\boldsymbol{\theta})}{d \boldsymbol{\theta}} \nabla_{\boldsymbol{\phi}} \mathcal{L}_{i}\left(\mathcal{A} l g_{i}(\boldsymbol{\theta})\right)</script><p>注意$d_{\theta} \mathcal{L}_{i}(Alg_{i}(\theta))$和$\nabla_{\phi} \mathcal{L}_{i}(Alg_{i}(\theta))$之间的重要区别。前者传递导数穿过$Alg_i(\theta)$，而后者则不穿过。$\nabla_{\phi} \mathcal{L}_{i}(Alg_{i}(\theta))$只是梯度函数，即$\nabla_{\phi} \mathcal{L}_{i}(\phi)$，以$\phi=Alg_{i}(\theta)$求值。还要注意，$d_{\theta} \mathcal{L}_{i}(Alg_{i}(\theta))$和$\nabla_{\phi} \mathcal{L}_{i}(Alg_{i}(\theta))$是d维向量，而$\frac{d Alg_{i}(\theta)}{d \theta}$是（d×d）大小的Jacobian矩阵。在本文中，我们还将无差别地使用$d_\theta$和$\frac{d}{d\theta}$。</p><h3 id="The-Implicit-MAML-Algorithm"><a href="#The-Implicit-MAML-Algorithm" class="headerlink" title="The Implicit MAML Algorithm"></a>The Implicit MAML Algorithm</h3><p>我们的目的是使用形如$\theta \gets \theta - \eta d_\theta F(\theta)$的基于迭代梯度的算法解决公式4中的双层元学习问题。尽管为简单起见，我们基于标准梯度下降法导出了我们的方法，但也可以使用任何其他优化方法，例如准牛顿法或牛顿法，Adam或带动量的梯度下降法，而无需进行任何修改。使用链式法则扩展梯度下降更新为</p><script type="math/tex; mode=display">\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\eta \frac{1}{M} \sum_{i=1}^{M} \frac{d \mathcal{Alg}_{i}^{\star}(\boldsymbol{\theta})}{d \boldsymbol{\theta}} \nabla_{\phi} \mathcal{L}_{i}\left(\mathcal{A} l g_{i}^{\star}(\boldsymbol{\theta})\right)</script><p>在此，$\nabla_{\phi} \mathcal{L}_{i}(\mathcal{Alg}_{i}^{\star}(\theta))$即$\nabla_{\phi} \mathcal{L}_{i}(\phi)|_{\phi=\mathcal{Alg}_{i}^{\star}(\theta)}$，操作上可以通过自动微分而容易地求出。对于此更新规则，我们必须计算$\frac{d Alg^\star_{i}(\theta)}{d \theta}$，其中$Alg^\star_{i}$被隐式定义为优化问题(公式4)，这带来了主要挑战。现在，我们提出一种有效的算法（在计算和内存中）以计算元梯度。</p><h4 id="Meta-Gradient-Computation"><a href="#Meta-Gradient-Computation" class="headerlink" title="Meta-Gradient Computation"></a>Meta-Gradient Computation</h4><p>如果将$Alg^\star_{i}(\theta)$实现为迭代算法（例如梯度下降），则计算$\frac{d Alg^\star_{i}(\theta)}{d \theta}$的一种方法是通过迭代过程传播导数，无论是正向还是反向。但是，这样做的缺点是必须明确依赖于优化路径，而优化路径必须完全存储在内存中，而当所需的梯度下降步数较多时，该路径很快变得难以处理。此外，对于诸如牛顿法的二阶优化方法，需要难以获得的三阶导数。此外，当使用不可微分的操作（如行搜索）时，此方法变得不可能。但是，通过认识到$Alg^\star_{i}$是隐式定义的优化问题的解决方案，我们可以采用一个不同的策略，该策略不需要考虑优化路径，而只需考虑最终结果。这源自以下引理。</p><p><strong>引理 1.</strong> （隐式雅克比）考虑公式4中对于任务$T_i$定义的$Alg^\star_{i}(\theta)$。另$\phi_i=\mathcal{Alg}_{i}^{\star}(\theta)$为$Alg^\star_{i}(\theta)$的解。如果$\left(\boldsymbol{I}+\frac{1}{\lambda} \nabla_{\phi}^{2} \hat{\mathcal{L}}_{i}\left(\boldsymbol{\phi}_{i}\right)\right)$是可逆的，则导数雅可比行列式为</p><script type="math/tex; mode=display">\frac{d \mathcal{Alg}_{i}^{\star}(\boldsymbol{\theta})}{d \boldsymbol{\theta}}=\left(\boldsymbol{I}+\frac{1}{\lambda} \nabla_{\phi}^{2} \hat{\mathcal{L}}_{i}\left(\boldsymbol{\phi}_{i}\right)\right)^{-1} \tag{6}</script><p>请注意，导数（Jacobian）仅取决于算法的最终结果，而不取决于算法采用的路径。因此，原则上，任何算法方法都可用于计算$Alg^\star_{i}(\theta)$，从而将元梯度计算与内部级优化器的选择解耦。</p><p><strong>具体算法</strong>：虽然引理1提供了一种理想的方法来计算$Alg^\star_{i}$的雅可比行列式，因此通过扩展元梯度，在实践中可能很难直接使用它。有两个问题特别相关。首先，元梯度要求计算$Alg^\star_{i}(\theta)$，这是内部优化问题的精确解。实际上，我们可能只能获得近似解。第二，对于大型深度神经网络而言，显式地构造和求逆等式6中的矩阵，用于计算雅可比行列式，可能是棘手的。为了解决这些困难，我们考虑接近理想方法的可行算法。</p><p>首先，我们考虑内部优化问题的近似解，可以通过迭代优化算法（例如梯度下降）获得。</p><p><strong>定义1.</strong> （$\delta$-近似算法）另$Alg_i(\theta)$为$Alg^\star_{i}(\theta)$的$\delta$-近似，如：</p><script type="math/tex; mode=display">\|Alg_{i}(\theta)-Alg_{i}^{\star}(\theta)\| \leq \delta</script><p>其次，我们将执行部分或近似矩阵求逆：</p><p><strong>定义2.</strong> （$\delta’$-近似雅可比向量积）另$g_i$为一个向量满足</p><script type="math/tex; mode=display">\|g_{i}-\left(I+\frac{1}{\lambda} \nabla_{\phi}^{2} \hat{\mathcal{L}}_{i}\left(\phi_{i}\right)\right)^{-1} \nabla_{\phi} \mathcal{L}_{i}\left(\phi_{i}\right)\| \leq \delta^{\prime}</script><p>这里$\phi_i=Alg_i(\theta)$且$Alg_i$基于定义1.</p><p>注意，定义2中的$g_i$是任务$T_i$的元梯度的近似值。观察到可以将$g_i$作为优化问题的近似解：</p><script type="math/tex; mode=display">\min _{\boldsymbol{w}} \boldsymbol{w}^{\top}\left(\boldsymbol{I}+\frac{1}{\lambda} \nabla_{\boldsymbol{\phi}}^{2} \hat{\mathcal{L}}_{i}\left(\boldsymbol{\phi}_{i}\right)\right) \boldsymbol{w}-\boldsymbol{w}^{\top} \nabla_{\phi} \mathcal{L}_{i}\left(\boldsymbol{\phi}_{i}\right)</script><p>共轭梯度（CG）算法由于其出色的迭代复杂性和仅要求$\nabla_\phi^2\hat{\mathcal{L}}_i(\phi_i)$形式的Hessian矢量积的要求而特别适合于此问题。无需显式构造或存储Hessian矩阵即可容易地获得这样的hessian矢量积（如我们在附录C中所述）。这种基于CG的求逆已成功地应用在用于深度学习的Hessian-free或Newton-CG方法[36，44]和强化学习[52，47]中的信任区域方法。算法1提出了完整的实用算法。注意，这些近似值用于开发实用的算法，会在元梯度计算中引入误差。我们将在3.2节中分析这些误差的影响，并证明它们是可控制的。有关iMAML如何推广了基于先验梯度优化的元学习算法，请参阅附录A。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2019-10-15_Unknown_-_Rajeswaran_et_al._-_Meta-Learning_with_I_1.png" alt title>                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/sherlockbear/pic-repo/master/img/2019-10-15_Unknown_-_Rajeswaran_et_al._-_Meta-Learning_with_I_2.png" alt title>                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;智能系统的一个核心功能是能够通过借鉴先前的经验来快速学习新任务的能力。最近，基于梯度（或优化）的元学习已成为一种有效的少样本学习方法。在此形式中，仅使用当前任务中的少量数据，即可在outer loop中学习元参数，而在inner-loop中学习特定于任务的模型。扩展这些方法的关键挑战是需要通过inner-loop学习过程计算微分，这可能会带来相当大的计算和内存负担。借助隐式微分，我们开发了隐式MAML算法，该算法仅取决于inner level优化的解，而不取决于inner loop优化器采用的路径。这有效地将元梯度计算与inner loop优化器的选择解耦。因此，我们的方法与inner loop优化器的选择无关，并且可以优雅地处理许多梯度步骤而不会梯度消失或内存限制。从理论上讲，我们证明隐式MAML可以使用不超过计算单个内循环梯度所需的内存占用量来计算准确的元梯度，而不会增加总的计算成本。从实验上，我们证明了隐式MAML的这些好处可转化为在少样本的图像识别benchmarks上的经验收益。&lt;/p&gt;
&lt;p&gt;其他来源：&lt;a href=&quot;https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Notes on iMAML: Meta-Learning with Implicit Gradients&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文翻译" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="meta-learning" scheme="http://yoursite.com/tags/meta-learning/"/>
    
      <category term="MAML" scheme="http://yoursite.com/tags/MAML/"/>
    
  </entry>
  
  <entry>
    <title>Meta-Learning with Differentiable Convex Optimization</title>
    <link href="http://yoursite.com/2019/10/09/Meta-Learning-with-Differentiable-Convex-Optimization/"/>
    <id>http://yoursite.com/2019/10/09/Meta-Learning-with-Differentiable-Convex-Optimization/</id>
    <published>2019-10-09T12:19:00.000Z</published>
    <updated>2019-12-03T11:22:12.951Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要:"></a><strong>摘要:</strong></h3><p>少样本学习的许多元学习方法都依赖于简单的基础学习器，例如最近邻分类器。但是，即使在少样本情况下，经过判别训练的线性判别器可以提供更好的泛化能力。我们建议使用这些判别器作为基础学习器，以学习少样本学习的表示形式，并表明它们在一系列少样本识别benchmarks中提供了特征尺寸和性能之间的更好权衡。我们的目标是学习在线性分类规则下对新类别很好地泛化的特征嵌入。为了有效地解决该目标，我们利用线性分类器的两个属性：凸问题的最优性条件的隐式微分和优化问题的对偶表示。这使我们可以在计算开销适度增加的情况下使用具有更高泛化性的高维嵌入。我们的方法名为MetaOptNet，可在miniImageNet，tieredImageNet，CIFAR-FS和FC100一次性学习基准上获得最先进的性能。代码可以在<a href="https://github.com/kjunelee/MetaOptNet" target="_blank" rel="noopener">这里</a>找到</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://res.cloudinary.com/dyxexppyu/image/upload/v1570623069/wps_2019-09-30_16-53-43_ri1avv.png" alt title>                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要:&quot;&gt;&lt;/a&gt;&lt;strong&gt;摘要:&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;少样本学习的许多元学习方法都依赖于简单的基础学习器，例如最近邻分类器。但是，即使在少样本情况下，经过判
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
